[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Data Science",
    "section": "",
    "text": "Preface\nThis is the website for the Introduction to Data Science.\nThe website for Advanced Data Science is here.\nWe make announcements related to the book on Twitter. For updates follow @rafalab.\nThis book started out as the class notes used in the HarvardX Data Science Series1.\nThe Quarto files used to generate the book is available on GitHub2. Note that, the graphical theme used for plots throughout the book can be recreated using the ds_theme_set() function from dslabs package.\nThis work is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International CC BY-NC-SA 4.0.\nA hardcopy version of the book is available from CRC Press3.\nA free PDF of the October 24, 2019 version of the book is available from Leanpub4.\nThis book is dedicated to all the people involved in building and maintaining R and the R packages we use in this book. A special thanks to the developers and maintainers of R base, the tidyverse, data.table, and the caret package.\nA special thanks to my tidyverse guru David Robinson and Amy Gill for dozens of comments, edits, and suggestions. Also, many thanks to Stephanie Hicks who twice served as a co-instructor in my data science classes and Yihui Xie who patiently put up with my many questions related to markdown. Thanks also to Karl Broman, from whom I borrowed ideas for the Data Visualization and Productivity Tools parts. Thanks to Peter Aldhous from whom I borrowed ideas for the principles of data visualization section and Jenny Bryan for writing Happy Git and GitHub for the useR, which influenced our Git chapters. Also, many thanks to Jeff Leek, Roger Peng, and Brian Caffo, whose class inspired the way this book is divided and to Garrett Grolemund and Hadley Wickham for making the markdown code for their R for Data Science book open. Finally, thanks to Alex Nones for proofreading the manuscript during its various stages.\nThis book was conceived during the teaching of several applied statistics courses, starting over fifteen years ago. The teaching assistants working with me throughout the years made important indirect contributions to this book. The latest iteration of this course is a HarvardX series coordinated by Heather Sternshein and Zofia Gajdos. We thank them for their contributions. We are also grateful to all the students whose questions and comments helped us improve the book. The courses were partially funded by NIH grant R25GM114818. We are very grateful to the National Institutes of Health for its support.\nA special thanks goes to all those who edited the book via GitHub pull requests or made suggestions by creating an issue or sending an email: nickyfoto (Huang Qiang), desautm (Marc-André Désautels), michaschwab (Michail Schwab), alvarolarreategui (Alvaro Larreategui), jakevc (Jake VanCampen), omerta (Guillermo Lengemann), espinielli (Enrico Spinielli), asimumba(Aaron Simumba), braunschweig (Maldewar), gwierzchowski (Grzegorz Wierzchowski), technocrat (Richard Careaga), atzakas, defeit (David Emerson Feit), shiraamitchell (Shira Mitchell), Nathalie-S, andreashandel (Andreas Handel), berkowitze (Elias Berkowitz), Dean-Webb (Dean Webber), mohayusuf, jimrothstein, mPloenzke (Matthew Ploenzke), NicholasDowand (Nicholas Dow), kant (Darío Hereñú), debbieyuster (Debbie Yuster), tuanchauict (Tuan Chau), phzeller, BTJ01 (BradJ), glsnow (Greg Snow), mberlanda (Mauro Berlanda), wfan9, larswestvang (Lars Westvang), jj999 (Jan Andrejkovic), Kriegslustig (Luca Nils Schmid), odahhani, aidanhorn (Aidan Horn), atraxler (Adrienne Traxler), alvegorova,wycheong (Won Young Cheong), med-hat (Medhat Khalil), kengustafson, Yowza63, ryan-heslin (Ryan Heslin), raffaem, tim8west, jooleer, pauluhn (Paul), David D. Kane, El Mustapha El Abbassi, Vadim Zipunnikov, Anna Quaglieri, Chris Dong, Bowen Gu, and Rick Schoenberg."
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Introduction to Data Science",
    "section": "",
    "text": "https://www.edx.org/professional-certificate/harvardx-data-science↩︎\nhttps://github.com/rafalab/dsbook-part-1↩︎\nhttps://www.routledge.com/Introduction-to-Data-Science-Data-Analysis-and-Prediction-Algorithms-with/Irizarry/p/book/9780367357986?utm_source=author&utm_medium=shared_link&utm_campaign=B043135_jm1_5ll_6rm_t081_1al_introductiontodatascienceauthorshare↩︎\nhttps://leanpub.com/datasciencebook↩︎"
  },
  {
    "objectID": "intro.html#case-studies",
    "href": "intro.html#case-studies",
    "title": "Introduction",
    "section": "Case studies",
    "text": "Case studies\nThroughout the book, we use motivating case studies. In each case study, we try to realistically mimic a data scientist’s experience. For each of the skills covered, we start by asking specific questions and answer these through data analysis. We learn the concepts as a means to answer the questions. Examples of the case studies included in the book are:\n\n\n\n\n\n\n\n\nCase Study\nConcept\n\n\n\n\n\nUS murder rates by state\nR Basics\n\n\n\nStudent heights\nStatistical Summaries\n\n\n\nTrends in world health and economics\nData Visualization\n\n\n\nThe impact of vaccines on infectious disease rates\nData Visualization\n\n\n\nReported student heights\nData Wrangling"
  },
  {
    "objectID": "intro.html#who-will-find-this-book-useful",
    "href": "intro.html#who-will-find-this-book-useful",
    "title": "Introduction",
    "section": "Who will find this book useful?",
    "text": "Who will find this book useful?\nThis book is meant to be a textbook for a first course in Data Science. No previous knowledge of R is necessary, although some experience with programming may be helpful. To be a successful data analyst implementing these skill requires understanding advanced statistical concepts, such as those covered in Advanced Data Science. If you read and understand all the chapters and complete all the exercises in this book, and understand statistical concepts, you will be well-positioned to perform basic data analysis tasks and you will be prepared to learn the more advanced concepts and skills needed to become an expert."
  },
  {
    "objectID": "intro.html#what-does-this-book-cover",
    "href": "intro.html#what-does-this-book-cover",
    "title": "Introduction",
    "section": "What does this book cover?",
    "text": "What does this book cover?\nWe start by going over the basics of R and the tidyverse. You learn R throughout the book, but in the first part we go over the building blocks needed to keep learning.\nThe growing availability of informative datasets and software tools has led to increased reliance on data visualizations in many fields. In the second part we demonstrate how to use ggplot2 to generate graphs and describe important data visualization principles.\nThe third part uses several examples to familiarize the reader with data wrangling. Among the specific skills we learn are web scraping, using regular expressions, and joining and reshaping data tables. We do this using tidyverse tools.\nIn the final part, we provide a brief introduction to the productivity tools we use on a day-to-day basis in data science projects. These are RStudio, UNIX/Linux shell, Git and GitHub, Quarto, and knitr."
  },
  {
    "objectID": "intro.html#what-is-not-covered-by-this-book",
    "href": "intro.html#what-is-not-covered-by-this-book",
    "title": "Introduction",
    "section": "What is not covered by this book?",
    "text": "What is not covered by this book?\nThis book focuses on the computing skills necessary for the data analysis aspects of data science. As mentioned, we do not cover statistical concepts. We also do not cover aspects related to data management or engineering. Although R programming is an essential part of the book, we do not teach more advanced computer science topics such as data structures, optimization, and algorithm theory. Similarly, we do not cover topics such as web services, interactive graphics, parallel computing, and data streaming processing."
  },
  {
    "objectID": "R/intro-to-R.html#footnotes",
    "href": "R/intro-to-R.html#footnotes",
    "title": "R",
    "section": "",
    "text": "https://rstudio.cloud↩︎\nhttps://rafalab.github.io/dsbook/installing-r-rstudio.html↩︎"
  },
  {
    "objectID": "R/getting-started.html#why-r",
    "href": "R/getting-started.html#why-r",
    "title": "1  Getting started",
    "section": "\n1.1 Why R?",
    "text": "1.1 Why R?\nR is not a programming language like C or Java. It was not created by software engineers for software development. Instead, it was developed by statisticians as an interactive environment for data analysis. You can read the full history in the paper A Brief History of S1. The interactivity is an indispensable feature in data science because, as you will soon learn, the ability to quickly explore data is a necessity for success in this field. However, like in other programming languages, you can save your work as scripts that can be easily executed at any moment. These scripts serve as a record of the analysis you performed, a key feature that facilitates reproducible work. If you are an expert programmer, you should not expect R to follow the conventions you are used to since you will be disappointed. If you are patient, you will come to appreciate the unequal power of R when it comes to data analysis and, specifically, data visualization.\nOther attractive features of R are:\n\nR is free and open source2.\nIt runs on all major platforms: Windows, Mac Os, UNIX/Linux.\nScripts and data objects can be shared seamlessly across platforms.\nThere is a large, growing, and active community of R users and, as a result, there are numerous resources for learning and asking questions3 4 5.\nIt is easy for others to contribute add-ons which enables developers to share software implementations of new data science methodologies. This gives R users early access to the latest methods and to tools which are developed for a wide variety of disciplines, including ecology, molecular biology, social sciences, and geography, just to name a few examples."
  },
  {
    "objectID": "R/getting-started.html#the-r-console",
    "href": "R/getting-started.html#the-r-console",
    "title": "1  Getting started",
    "section": "\n1.2 The R console",
    "text": "1.2 The R console\nInteractive data analysis usually occurs on the R console that executes commands as you type them. There are several ways to gain access to an R console. One way is to simply start R on your computer. The console looks something like this:\n\nAs a quick example, try using the console to calculate a 15% tip on a meal that cost $19.71:\n\n0.15 * 19.71  \n#&gt; [1] 2.96\n\nNote that in this book, grey boxes are used to show R code typed into the R console. The symbol #&gt; is used to denote what the R console outputs."
  },
  {
    "objectID": "R/getting-started.html#scripts",
    "href": "R/getting-started.html#scripts",
    "title": "1  Getting started",
    "section": "\n1.3 Scripts",
    "text": "1.3 Scripts\nOne of the great advantages of R over point-and-click analysis software is that you can save your work as scripts. You can edit and save these scripts using a text editor. The material in this book was developed using the interactive integrated development environment (IDE) RStudio6. RStudio includes an editor with many R specific features, a console to execute your code, and other useful panes, including one to show figures.\n\nMost web-based R consoles also provide a pane to edit scripts, but not all permit you to save the scripts for later use.\nAll the R scripts used to generate this book can be found on GitHub7."
  },
  {
    "objectID": "R/getting-started.html#sec-rstudio",
    "href": "R/getting-started.html#sec-rstudio",
    "title": "1  Getting started",
    "section": "\n1.4 RStudio",
    "text": "1.4 RStudio\nRStudio will be our launching pad for data science projects. It not only provides an editor for us to create and edit our scripts but also provides many other useful tools. In this section, we go over some of the basics.\n\n1.4.1 The panes\nWhen you start RStudio for the first time, you will see three panes. The left pane shows the R console. On the right, the top pane includes tabs such as Environment and History, while the bottom pane shows five tabs: File, Plots, Packages, Help, and Viewer (these tabs may change in new versions). You can click on each tab to move across the different features.\n\nTo start a new script, you can click on File, then New File, then R Script.\n\nThis starts a new pane on the left and it is here where you can start writing your script.\n\n\n1.4.2 Key bindings\nMany tasks we perform with the mouse can be achieved with a combination of key strokes instead. These keyboard versions for performing tasks are referred to as key bindings. For example, we just showed how to use the mouse to start a new script, but you can also use a key binding: Ctrl+Shift+N on Windows and command+shift+N on the Mac.\nAlthough in this tutorial we often show how to use the mouse, we highly recommend that you memorize key bindings for the operations you use most. RStudio provides a useful cheat sheet with the most widely used commands. You can get it from RStudio directly:\n\nYou might want to keep this handy so you can look up key-bindings when you find yourself performing repetitive point-and-clicking.\n\n1.4.3 Running commands while editing scripts\nThere are many editors specifically made for coding. These are useful because color and indentation are automatically added to make code more readable. RStudio is one of these editors, and it was specifically developed for R. One of the main advantages provided by RStudio over other editors is that we can test our code easily as we edit our scripts. Below we show an example.\nLet’s start by opening a new script as we did before. A next step is to give the script a name. We can do this through the editor by saving the current new unnamed script. To do this, click on the save icon or use the key binding Ctrl+S on Windows and command+S on the Mac.\nWhen you ask for the document to be saved for the first time, RStudio will prompt you for a name. A good convention is to use a descriptive name, with lower case letters, no spaces, only hyphens to separate words, and then followed by the suffix .R. We will call this script my-first-script.R.\n\nNow we are ready to start editing our first script. The first lines of code in an R script are dedicated to loading the libraries we will use. Another useful RStudio feature is that once we type library() it starts auto-completing with libraries that we have installed. Note what happens when we type library(ti):\n\nAnother feature you may have noticed is that when you type library( the second parenthesis is automatically added. This will help you avoid one of the most common errors in coding: forgetting to close a parenthesis.\nNow we can continue to write code. As an example, we will make a graph showing murder totals versus population totals by state. Once you are done writing the code needed to make this plot, you can try it out by executing the code. To do this, click on the Run button on the upper right side of the editing pane. You can also use the key binding: Ctrl+Shift+Enter on Windows or command+shift+return on the Mac.\nOnce you run the code, you will see it appear in the R console and, in this case, the generated plot appears in the plots console. Note that the plot console has a useful interface that permits you to click back and forward across different plots, zoom in to the plot, or save the plots as files.\n\nTo run one line at a time instead of the entire script, you can use Control-Enter on Windows and command-return on the Mac.\n\n1.4.4 Changing global options\nYou can change the look and functionality of RStudio quite a bit.\nTo change the global options you click on Tools then Global Options….\nAs an example we show how to make a change that we highly recommend. This is to change the Save workspace to .RData on exit to Never and uncheck the Restore .RData into workspace at start. By default, when you exit R saves all the objects you have created into a file called .RData. This is done so that when you restart the session in the same folder, it will load these objects. We find that this causes confusion especially when we share code with colleagues and assume they have this .RData file. To change these options, make your General settings look like this:"
  },
  {
    "objectID": "R/getting-started.html#installing-r-packages",
    "href": "R/getting-started.html#installing-r-packages",
    "title": "1  Getting started",
    "section": "\n1.5 Installing R packages",
    "text": "1.5 Installing R packages\nThe functionality provided by a fresh install of R is only a small fraction of what is possible. In fact, we refer to what you get after your first install as base R. The extra functionality comes from add-ons available from developers. There are currently hundreds of these available from CRAN and many others shared via other repositories such as GitHub. However, because not everybody needs all available functionality, R instead makes different components available via packages. R makes it very easy to install packages from within R. For example, to install the dslabs package, which we use to share datasets and code related to this book, you would type:\n\ninstall.packages(\"dslabs\")\n\nIn RStudio, you can navigate to the Tools tab and select install packages. We can then load the package into our R sessions using the library function:\n\nlibrary(dslabs)\n\nAs you go through this book, you will see that we load packages without installing them. This is because once you install a package, it remains installed and only needs to be loaded with library. The package remains loaded until we quit the R session. If you try to load a package and get an error, it probably means you need to install it first.\nWe can install more than one package at once by feeding a character vector to this function:\n\ninstall.packages(c(\"tidyverse\", \"dslabs\"))\n\nOne advantage of using RStudio is that it auto-completes package names once you start typing, which is helpful when you do not remember the exact spelling of the package:\n\nOnce you select your package, we recommend selecting all the defaults:\n\n\n\n\n\n\n\n\n\n\nNote that installing tidyverse actually installs several packages. This commonly occurs when a package has dependencies, or uses functions from other packages. When you load a package using library, you also load its dependencies.\nOnce packages are installed, you can load them into R and you do not need to install them again, unless you install a fresh version of R. Remember packages are installed in R not RStudio.\nIt is helpful to keep a list of all the packages you need for your work in a script because if you need to perform a fresh install of R, you can re-install all your packages by simply running a script.\nYou can see all the packages you have installed using the following function:\n\ninstalled.packages()"
  },
  {
    "objectID": "R/getting-started.html#footnotes",
    "href": "R/getting-started.html#footnotes",
    "title": "1  Getting started",
    "section": "",
    "text": "https://pdfs.semanticscholar.org/9b48/46f192aa37ca122cfabb1ed1b59866d8bfda.pdf↩︎\nhttps://opensource.org/history↩︎\nhttps://stats.stackexchange.com/questions/138/free-resources-for-learning-r↩︎\nhttps://www.r-project.org/help.html↩︎\nhttps://stackoverflow.com/documentation/r/topics↩︎\nhttps://www.rstudio.com/↩︎\nhttps://github.com/rafalab/dsbook↩︎"
  },
  {
    "objectID": "R/R-basics.html#motivating-example-us-gun-murders",
    "href": "R/R-basics.html#motivating-example-us-gun-murders",
    "title": "2  R basics",
    "section": "\n2.1 Motivating example: US Gun Murders",
    "text": "2.1 Motivating example: US Gun Murders\nImagine you live in Europe and are offered a job in a US company with many locations across all states. It is a great job, but news with headlines such as US Gun Homicide Rate Higher Than Other Developed Countries3 have you worried. Charts like this may concern you even more:\n\n\n\n\n\n\n\n\n\nOr even worse, this version from everytown.org:\n\n\n\n\n\n\n\n\n\nBut then you remember that the US is a large and diverse country with 50 very different states as well as the District of Columbia (DC).\n\n\n\n\n\n\n\n\nCalifornia, for example, has a larger population than Canada, and 20 US states have populations larger than that of Norway. In some respects, the variability across states in the US is akin to the variability across countries in Europe. Furthermore, although not included in the charts above, the murder rates in Lithuania, Ukraine, and Russia are higher than 4 per 100,000. So perhaps the news reports that worried you are too superficial. You have options of where to live and want to determine the safety of each particular state. We will gain some insights by examining data related to gun homicides in the US during 2010 using R.\nBefore we get started with our example, we need to cover logistics as well as some of the very basic building blocks that are required to gain more advanced R skills. Be aware that the usefulness of some of these building blocks may not be immediately obvious, but later in the book you will appreciate having mastered these skills."
  },
  {
    "objectID": "R/R-basics.html#the-very-basics",
    "href": "R/R-basics.html#the-very-basics",
    "title": "2  R basics",
    "section": "\n2.2 The very basics",
    "text": "2.2 The very basics\nBefore we get started with the motivating dataset, we need to cover the very basics of R.\n\n2.2.1 Objects\nSuppose a high school student asks us for help solving several quadratic equations of the form \\(ax^2+bx+c = 0\\). The quadratic formula gives us the solutions:\n\\[\n\\frac{-b - \\sqrt{b^2 - 4ac}}{2a}\\,\\, \\mbox{ and } \\frac{-b + \\sqrt{b^2 - 4ac}}{2a}\n\\] which of course change depending on the values of \\(a\\), \\(b\\), and \\(c\\). One advantage of programming languages is that we can define variables and write expressions with these variables, similar to how we do so in math, but obtain a numeric solution. We will write out general code for the quadratic equation below, but if we are asked to solve \\(x^2 + x -1 = 0\\), then we define:\n\na &lt;- 1\nb &lt;- 1\nc &lt;- -1\n\nwhich stores the values for later use. We use &lt;- to assign values to the variables.\nWe can also assign values using = instead of &lt;-, but we recommend against using = to avoid confusion.\nCopy and paste the code above into your console to define the three variables. Note that R does not print anything when we make this assignment. This means the objects were defined successfully. Had you made a mistake, you would have received an error message.\nTo see the value stored in a variable, we simply ask R to evaluate a and it shows the stored value:\n\na\n#&gt; [1] 1\n\nA more explicit way to ask R to show us the value stored in a is using print like this:\n\nprint(a)\n#&gt; [1] 1\n\nWe use the term object to describe stuff that is stored in R. Variables are examples, but objects can also be more complicated entities such as functions, which are described later.\n\n2.2.2 The workspace\nAs we define objects in the console, we are actually changing the workspace. You can see all the variables saved in your workspace by typing:\n\nls()\n#&gt; [1] \"a\"               \"b\"               \"c\"              \n#&gt; [4] \"dat\"             \"has_annotations\"\n\nIn RStudio, the Environment tab shows the values:\n\nWe should see a, b, and c. If you try to recover the value of a variable that is not in your workspace, you receive an error. For example, if you type x you will receive the following message: Error: object 'x' not found.\nNow since these values are saved in variables, to obtain a solution to our equation, we use the quadratic formula:\n\n(-b + sqrt(b^2 - 4*a*c))/(2*a)\n#&gt; [1] 0.618\n(-b - sqrt(b^2 - 4*a*c))/(2*a)\n#&gt; [1] -1.62\n\n\n2.2.3 Prebuilt functions\nOnce you define variables, the data analysis process can usually be described as a series of functions applied to the data. R includes several predefined functions and most of the analysis pipelines we construct make extensive use of these.\nWe already used the install.packages, library, and ls functions. We also used the function sqrt to solve the quadratic equation above. There are many more prebuilt functions and even more can be added through packages. These functions do not appear in the workspace because you did not define them, but they are available for immediate use.\nIn general, we need to use parentheses to evaluate a function. If you type ls, the function is not evaluated and instead R shows you the code that defines the function. If you type ls() the function is evaluated and, as seen above, we see objects in the workspace.\nUnlike ls, most functions require one or more arguments. Below is an example of how we assign an object to the argument of the function log. Remember that we earlier defined a to be 1:\n\nlog(8)\n#&gt; [1] 2.08\nlog(a) \n#&gt; [1] 0\n\nYou can find out what the function expects and what it does by reviewing the very useful manuals included in R. You can get help by using the help function like this:\n\nhelp(\"log\")\n\nFor most functions, we can also use this shorthand:\n\n?log\n\nThe help page will show you what arguments the function is expecting. For example, log needs x and base to run. However, some arguments are required and others are optional. You can determine which arguments are optional by noting in the help document that a default value is assigned with =. Defining these is optional. For example, the base of the function log defaults to base = exp(1) making log the natural log by default.\nIf you want a quick look at the arguments without opening the help system, you can type:\n\nargs(log)\n#&gt; function (x, base = exp(1)) \n#&gt; NULL\n\nYou can change the default values by simply assigning another object:\n\nlog(8, base = 2)\n#&gt; [1] 3\n\nNote that we have not been specifying the argument x as such:\n\nlog(x = 8, base = 2)\n#&gt; [1] 3\n\nThe above code works, but we can save ourselves some typing: if no argument name is used, R assumes you are entering arguments in the order shown in the help file or by args. So by not using the names, it assumes the arguments are x followed by base:\n\nlog(8, 2)\n#&gt; [1] 3\n\nIf using the arguments’ names, then we can include them in whatever order we want:\n\nlog(base = 2, x = 8)\n#&gt; [1] 3\n\nTo specify arguments, we must use =, and cannot use &lt;-.\nThere are some exceptions to the rule that functions need the parentheses to be evaluated. Among these, the most commonly used are the arithmetic and relational operators. For example:\n\n2 ^ 3\n#&gt; [1] 8\n\nYou can see the arithmetic operators by typing:\n\nhelp(\"+\") \n\nor\n\n?\"+\"\n\nand the relational operators by typing:\n\nhelp(\"&gt;\") \n\nor\n\n?\"&gt;\"\n\n\n2.2.4 Other prebuilt objects\nThere are several datasets that are included for users to practice and test out functions. You can see all the available datasets by typing:\n\ndata()\n\nThis shows you the object name for these datasets. These datasets are objects that can be used by simply typing the name. For example, if you type:\n\nco2\n\nR will show you Mauna Loa atmospheric CO2 concentration data.\nOther prebuilt objects are mathematical quantities, such as the constant \\(\\pi\\) and \\(\\infty\\):\n\npi\n#&gt; [1] 3.14\nInf + 1\n#&gt; [1] Inf\n\n\n2.2.5 Variable names\nWe have used the letters a, b, and c as variable names, but variable names can be almost anything. Some basic rules in R are that variable names have to start with a letter, can’t contain spaces, and should not be variables that are predefined in R. For example, don’t name one of your variables install.packages by typing something like install.packages &lt;- 2.\nA nice convention to follow is to use meaningful words that describe what is stored, use only lower case, and use underscores as a substitute for spaces. For the quadratic equations, we could use something like this:\n\nsolution_1 &lt;- (-b + sqrt(b^2 - 4*a*c))/(2*a)\nsolution_2 &lt;- (-b - sqrt(b^2 - 4*a*c))/(2*a)\n\nFor more advice, we highly recommend studying Hadley Wickham’s style guide4.\n\n2.2.6 Saving your workspace\nValues remain in the workspace until you end your session or erase them with the function rm. But workspaces also can be saved for later use. In fact, when you quit R, the program asks you if you want to save your workspace. If you do save it, the next time you start R, the program will restore the workspace.\nWe actually recommend against saving the workspace this way because, as you start working on different projects, it will become harder to keep track of what is saved. Instead, we recommend you assign the workspace a specific name. You can do this by using the function save or save.image. To load, use the function load. When saving a workspace, we recommend the suffix rda or RData. In RStudio, you can also do this by navigating to the Session tab and choosing Save Workspace as. You can later load it using the Load Workspace options in the same tab. You can read the help pages on save, save.image, and load to learn more.\n\n2.2.7 Motivating scripts\nTo solve another equation such as \\(3x^2 + 2x -1\\), we can copy and paste the code above and then redefine the variables and recompute the solution:\n\na &lt;- 3\nb &lt;- 2\nc &lt;- -1\n(-b + sqrt(b^2 - 4*a*c))/(2*a)\n(-b - sqrt(b^2 - 4*a*c))/(2*a)\n\nBy creating and saving a script with the code above, we would not need to retype everything each time and, instead, simply change the variable names. Try writing the script above into an editor and notice how easy it is to change the variables and receive an answer.\n\n2.2.8 Commenting your code\nIf a line of R code starts with the symbol #, it is not evaluated. We can use this to write reminders of why we wrote particular code. For example, in the script above we could add:\n\n## Code to compute solution to quadratic equation\n\n## Define the variables\na &lt;- 3 \nb &lt;- 2\nc &lt;- -1\n\n## now compute the solution\n(-b + sqrt(b^2 - 4*a*c))/(2*a)\n(-b - sqrt(b^2 - 4*a*c))/(2*a)\n\n\n\n\n\n\n\nYou are ready to do exercises 1-5."
  },
  {
    "objectID": "R/R-basics.html#data-types",
    "href": "R/R-basics.html#data-types",
    "title": "2  R basics",
    "section": "\n2.3 Data types",
    "text": "2.3 Data types\nVariables in R can be of different types. For example, we need to distinguish numbers from character strings and tables from simple lists of numbers. The function class helps us determine what type of object we have:\n\na &lt;- 2\nclass(a)\n#&gt; [1] \"numeric\"\n\nTo work efficiently in R, it is important to learn the different types of variables and what we can do with these.\n\n2.3.1 Data frames\nUp to now, the variables we have defined are just one number. This is not very useful for storing data. The most common way of storing a dataset in R is in a data frame. Conceptually, we can think of a data frame as a table with rows representing observations and the different variables reported for each observation defining the columns. Data frames are particularly useful for datasets because we can combine different data types into one object.\nA large proportion of data analysis challenges start with data stored in a data frame. For example, we stored the data for our motivating example in a data frame. You can access this dataset by loading the dslabs library and loading the murders dataset using the data function:\n\nlibrary(dslabs)\n\nTo see that this is in fact a data frame, we type:\n\nclass(murders)\n#&gt; [1] \"data.frame\"\n\n\n2.3.2 Examining an object\nThe function str is useful for finding out more about the structure of an object:\n\nstr(murders)\n#&gt; 'data.frame':    51 obs. of  5 variables:\n#&gt; $ state : chr \"Alabama\" \"Alaska\" \"Arizona\" \"Arkansas\" ...\n#&gt; $ abb : chr \"AL\" \"AK\" \"AZ\" \"AR\" ...\n#&gt; $ region : Factor w/ 4 levels \"Northeast\",\"South\",..: 2 4 4 2 4 4 1 2 2\n#&gt;    2 ...\n#&gt; $ population: num 4779736 710231 6392017 2915918 37253956 ...\n#&gt; $ total : num 135 19 232 93 1257 ...\n\nThis tells us much more about the object. We see that the table has 51 rows (50 states plus DC) and five variables. We can show the first six lines using the function head:\n\nhead(murders)\n#&gt;        state abb region population total\n#&gt; 1    Alabama  AL  South    4779736   135\n#&gt; 2     Alaska  AK   West     710231    19\n#&gt; 3    Arizona  AZ   West    6392017   232\n#&gt; 4   Arkansas  AR  South    2915918    93\n#&gt; 5 California  CA   West   37253956  1257\n#&gt; 6   Colorado  CO   West    5029196    65\n\nIn this dataset, each state is considered an observation and five variables are reported for each state.\nBefore we go any further in answering our original question about different states, let’s learn more about the components of this object.\n\n2.3.3 The accessor: $\n\nFor our analysis, we will need to access the different variables represented by columns included in this data frame. To do this, we use the accessor operator $ in the following way:\n\nmurders$population\n#&gt;  [1]  4779736   710231  6392017  2915918 37253956  5029196  3574097\n#&gt;  [8]   897934   601723 19687653  9920000  1360301  1567582 12830632\n#&gt; [15]  6483802  3046355  2853118  4339367  4533372  1328361  5773552\n#&gt; [22]  6547629  9883640  5303925  2967297  5988927   989415  1826341\n#&gt; [29]  2700551  1316470  8791894  2059179 19378102  9535483   672591\n#&gt; [36] 11536504  3751351  3831074 12702379  1052567  4625364   814180\n#&gt; [43]  6346105 25145561  2763885   625741  8001024  6724540  1852994\n#&gt; [50]  5686986   563626\n\nBut how did we know to use population? Previously, by applying the function str to the object murders, we revealed the names for each of the five variables stored in this table. We can quickly access the variable names using:\n\nnames(murders)\n#&gt; [1] \"state\"      \"abb\"        \"region\"     \"population\" \"total\"\n\nIt is important to know that the order of the entries in murders$population preserves the order of the rows in our data table. This will later permit us to manipulate one variable based on the results of another. For example, we will be able to order the state names by the number of murders.\n\n\n\n\n\n\nR comes with a very nice auto-complete functionality that saves us the trouble of typing out all the names. Try typing murders$p then hitting the tab key on your keyboard. This functionality and many other useful auto-complete features are available when working in RStudio.\n\n\n\n\n2.3.4 Vectors: numerics, characters, and logical\nThe object murders$population is not one number but several. We call these types of objects vectors. A single number is technically a vector of length 1, but in general we use the term vectors to refer to objects with several entries. The function length tells you how many entries are in the vector:\n\npop &lt;- murders$population\nlength(pop)\n#&gt; [1] 51\n\nThis particular vector is numeric since population sizes are numbers:\n\nclass(pop)\n#&gt; [1] \"numeric\"\n\nIn a numeric vector, every entry must be a number.\nTo store character strings, vectors can also be of class character. For example, the state names are characters:\n\nclass(murders$state)\n#&gt; [1] \"character\"\n\nAs with numeric vectors, all entries in a character vector need to be a character.\nAnother important type of vectors are logical vectors. These must be either TRUE or FALSE.\n\nz &lt;- 3 == 2\nz\n#&gt; [1] FALSE\nclass(z)\n#&gt; [1] \"logical\"\n\nHere the == is a relational operator asking if 3 is equal to 2. In R, if you just use one =, you actually assign a variable, but if you use two == you test for equality.\nYou can see the other relational operators by typing:\n\n?Comparison\n\nIn future sections, you will see how useful relational operators can be.\nWe discuss more important features of vectors after the next set of exercises.\n\n\n\n\n\n\nMathematically, the values in pop are integers and there is an integer class in R. However, by default, numbers are assigned class numeric even when they are round integers. For example, class(1) returns numeric. You can turn them into class integer with the as.integer() function or by adding an L like this: 1L. Note the class by typing: class(1L)\n\n\n\n\n2.3.5 Factors\nIn the murders dataset, we might expect the region to also be a character vector. However, it is not:\n\nclass(murders$region)\n#&gt; [1] \"factor\"\n\nIt is a factor. Factors are useful for storing categorical data. We can see that there are only 4 regions by using the levels function:\n\nlevels(murders$region)\n#&gt; [1] \"Northeast\"     \"South\"         \"North Central\" \"West\"\n\nIn the background, R stores these levels as integers and keeps a map to keep track of the labels. This is more memory efficient than storing all the characters.\nNote that the levels have an order that is different from the order of appearance in the factor object. The default in R is for the levels to follow alphabetical order. However, often we want the levels to follow a different order. You can specify an order through the levels argument when creating the factor with the factor function. For example, in the murders dataset regions are ordered from east to west. The function reorder lets us change the order of the levels of a factor variable based on a summary computed on a numeric vector. We will demonstrate this with a simple example, and will see more advanced ones in the Data Visualization part of the book.\nSuppose we want the levels of the region by the total number of murders rather than alphabetical order. If there are values associated with each level, we can use the reorder and specify a data summary to determine the order. The following code takes the sum of the total murders in each region, and reorders the factor following these sums.\n\nregion &lt;- murders$region\nvalue &lt;- murders$total\nregion &lt;- reorder(region, value, FUN = sum)\nlevels(region)\n#&gt; [1] \"Northeast\"     \"North Central\" \"West\"          \"South\"\n\nThe new order is in agreement with the fact that the Northeast has the least murders and the South has the most.\n\n\n\n\n\n\nFactors can be a source of confusion since sometimes they behave like characters and sometimes they do not. As a result, confusing factors and characters are a common source of bugs.\n\n\n\n\n2.3.6 Lists\nData frames are a special case of lists. Lists are useful because you can store any combination of different types. You can create a list using the list function like this:\n\nrecord &lt;- list(name = \"John Doe\",\n             student_id = 1234,\n             grades = c(95, 82, 91, 97, 93),\n             final_grade = \"A\")\n\nThe function c is described in Section Section 2.4.\nThis list includes a character, a number, a vector with five numbers, and another character.\n\nrecord\n#&gt; $name\n#&gt; [1] \"John Doe\"\n#&gt; \n#&gt; $student_id\n#&gt; [1] 1234\n#&gt; \n#&gt; $grades\n#&gt; [1] 95 82 91 97 93\n#&gt; \n#&gt; $final_grade\n#&gt; [1] \"A\"\nclass(record)\n#&gt; [1] \"list\"\n\nAs with data frames, you can extract the components of a list with the accessor $.\n\nrecord$student_id\n#&gt; [1] 1234\n\nWe can also use double square brackets ([[) like this:\n\nrecord[[\"student_id\"]]\n#&gt; [1] 1234\n\nYou should get used to the fact that in R, there are often several ways to do the same thing, such as accessing entries.\nYou might also encounter lists without variable names.\n\nrecord2 &lt;- list(\"John Doe\", 1234)\nrecord2\n#&gt; [[1]]\n#&gt; [1] \"John Doe\"\n#&gt; \n#&gt; [[2]]\n#&gt; [1] 1234\n\nIf a list does not have names, you cannot extract the elements with $, but you can still use the brackets method and instead of providing the variable name, you provide the list index, like this:\n\nrecord2[[1]]\n#&gt; [1] \"John Doe\"\n\nWe won’t be using lists until later, but you might encounter one in your own exploration of R. For this reason, we show you some basics here.\n\n2.3.7 Matrices\nMatrices are another type of object that are common in R. Matrices are similar to data frames in that they are two-dimensional: they have rows and columns. However, like numeric, character and logical vectors, entries in matrices have to be all the same type. For this reason data frames are much more useful for storing data, since we can have characters, factors, and numbers in them.\nYet matrices have a major advantage over data frames: we can perform matrix algebra operations, a powerful type of mathematical technique. We do not describe these operations in this book, but much of what happens in the background when you perform a data analysis involves matrices. We only cover matrices briefly here since some of the functions we will learn return matrices. However, if you plan to perform more advanced work, we highly recommend learning more as they are widely used in data analysis.\n\nWe can define a matrix using the matrix function. We need to specify the number of rows and columns.\n\nmat &lt;- matrix(1:12, 4, 3)\nmat\n#&gt;      [,1] [,2] [,3]\n#&gt; [1,]    1    5    9\n#&gt; [2,]    2    6   10\n#&gt; [3,]    3    7   11\n#&gt; [4,]    4    8   12\n\nYou can access specific entries in a matrix using square brackets ([). If you want the second row, third column, you use:\n\nmat[2, 3]\n#&gt; [1] 10\n\nIf you want the entire second row, you leave the column spot empty:\n\nmat[2, ]\n#&gt; [1]  2  6 10\n\nNotice that this returns a vector, not a matrix.\nSimilarly, if you want the entire third column, you leave the row spot empty:\n\nmat[, 3]\n#&gt; [1]  9 10 11 12\n\nThis is also a vector, not a matrix.\nYou can access more than one column or more than one row if you like. This will give you a new matrix.\n\nmat[, 2:3]\n#&gt;      [,1] [,2]\n#&gt; [1,]    5    9\n#&gt; [2,]    6   10\n#&gt; [3,]    7   11\n#&gt; [4,]    8   12\n\nYou can subset both rows and columns:\n\nmat[1:2, 2:3]\n#&gt;      [,1] [,2]\n#&gt; [1,]    5    9\n#&gt; [2,]    6   10\n\nWe can convert matrices into data frames using the function as.data.frame:\n\nas.data.frame(mat)\n#&gt;   V1 V2 V3\n#&gt; 1  1  5  9\n#&gt; 2  2  6 10\n#&gt; 3  3  7 11\n#&gt; 4  4  8 12\n\nYou can also use single square brackets ([) to access rows and columns of a data frame:\n\nmurders[25, 1]\n#&gt; [1] \"Mississippi\"\nmurders[2:3, ]\n#&gt;     state abb region population total\n#&gt; 2  Alaska  AK   West     710231    19\n#&gt; 3 Arizona  AZ   West    6392017   232\n\n\n\n\n\n\n\nYou are ready to do exercises 6-11."
  },
  {
    "objectID": "R/R-basics.html#sec-vectors",
    "href": "R/R-basics.html#sec-vectors",
    "title": "2  R basics",
    "section": "\n2.4 Vectors",
    "text": "2.4 Vectors\nIn R, the most basic objects available to store data are vectors. As we have seen, complex datasets can usually be broken down into components that are vectors. For example, in a data frame, each column is a vector. Here we learn more about this important class.\n\n2.4.1 Creating vectors\nWe can create vectors using the function c, which stands for concatenate. We use c to concatenate entries in the following way:\n\ncodes &lt;- c(380, 124, 818)\ncodes\n#&gt; [1] 380 124 818\n\nWe can also create character vectors. We use the quotes to denote that the entries are characters rather than variable names.\n\ncountry &lt;- c(\"italy\", \"canada\", \"egypt\")\n\nIn R you can also use single quotes:\n\ncountry &lt;- c('italy', 'canada', 'egypt')\n\nBut be careful not to confuse the single quote ’ with the back quote `.\nBy now you should know that if you type:\n\ncountry &lt;- c(italy, canada, egypt)\n\nyou receive an error because the variables italy, canada, and egypt are not defined. If we do not use the quotes, R looks for variables with those names and returns an error.\n\n2.4.2 Names\nSometimes it is useful to name the entries of a vector. For example, when defining a vector of country codes, we can use the names to connect the two:\n\ncodes &lt;- c(italy = 380, canada = 124, egypt = 818)\ncodes\n#&gt;  italy canada  egypt \n#&gt;    380    124    818\n\nThe object codes continues to be a numeric vector:\n\nclass(codes)\n#&gt; [1] \"numeric\"\n\nbut with names:\n\nnames(codes)\n#&gt; [1] \"italy\"  \"canada\" \"egypt\"\n\nIf the use of strings without quotes looks confusing, know that you can use the quotes as well:\n\ncodes &lt;- c(\"italy\" = 380, \"canada\" = 124, \"egypt\" = 818)\ncodes\n#&gt;  italy canada  egypt \n#&gt;    380    124    818\n\nThere is no difference between this function call and the previous one. This is one of the many ways in which R is quirky compared to other languages.\nWe can also assign names using the names functions:\n\ncodes &lt;- c(380, 124, 818)\ncountry &lt;- c(\"italy\",\"canada\",\"egypt\")\nnames(codes) &lt;- country\ncodes\n#&gt;  italy canada  egypt \n#&gt;    380    124    818\n\n\n2.4.3 Sequences\nAnother useful function for creating vectors generates sequences:\n\nseq(1, 10)\n#&gt;  [1]  1  2  3  4  5  6  7  8  9 10\n\nThe first argument defines the start, and the second defines the end which is included. The default is to go up in increments of 1, but a third argument lets us tell it how much to jump by:\n\nseq(1, 10, 2)\n#&gt; [1] 1 3 5 7 9\n\nIf we want consecutive integers, we can use the following shorthand:\n\n1:10\n#&gt;  [1]  1  2  3  4  5  6  7  8  9 10\n\nWhen we use these functions, R produces integers, not numerics, because they are typically used to index something:\n\nclass(1:10)\n#&gt; [1] \"integer\"\n\nHowever, if we create a sequence including non-integers, the class changes:\n\nclass(seq(1, 10, 0.5))\n#&gt; [1] \"numeric\"\n\n\n2.4.4 Subsetting\nWe use square brackets to access specific elements of a vector. For the vector codes we defined above, we can access the second element using:\n\ncodes[2]\n#&gt; canada \n#&gt;    124\n\nYou can get more than one entry by using a multi-entry vector as an index:\n\ncodes[c(1,3)]\n#&gt; italy egypt \n#&gt;   380   818\n\nThe sequences defined above are particularly useful if we want to access, say, the first two elements:\n\ncodes[1:2]\n#&gt;  italy canada \n#&gt;    380    124\n\nIf the elements have names, we can also access the entries using these names. Below are two examples.\n\ncodes[\"canada\"]\n#&gt; canada \n#&gt;    124\ncodes[c(\"egypt\",\"italy\")]\n#&gt; egypt italy \n#&gt;   818   380"
  },
  {
    "objectID": "R/R-basics.html#coercion",
    "href": "R/R-basics.html#coercion",
    "title": "2  R basics",
    "section": "\n2.5 Coercion",
    "text": "2.5 Coercion\nIn general, coercion is an attempt by R to be flexible with data types. When an entry does not match the expected, some of the prebuilt R functions try to guess what was meant before throwing an error. This can also lead to confusion. Failing to understand coercion can drive programmers crazy when attempting to code in R since it behaves quite differently from most other languages in this regard. Let’s learn about it with some examples.\nWe said that vectors must be all of the same type. So if we try to combine, say, numbers and characters, you might expect an error:\n\nx &lt;- c(1, \"canada\", 3)\n\nBut we don’t get one, not even a warning! What happened? Look at x and its class:\n\nx\n#&gt; [1] \"1\"      \"canada\" \"3\"\nclass(x)\n#&gt; [1] \"character\"\n\nR coerced the data into characters. It guessed that because you put a character string in the vector, you meant the 1 and 3 to actually be character strings \"1\" and “3”. The fact that not even a warning is issued is an example of how coercion can cause many unnoticed errors in R.\nR also offers functions to change from one type to another. For example, you can turn numbers into characters with:\n\nx &lt;- 1:5\ny &lt;- as.character(x)\ny\n#&gt; [1] \"1\" \"2\" \"3\" \"4\" \"5\"\n\nYou can turn it back with as.numeric:\n\nas.numeric(y)\n#&gt; [1] 1 2 3 4 5\n\nThis function is actually quite useful since datasets that include numbers as character strings are common."
  },
  {
    "objectID": "R/R-basics.html#not-availables-na",
    "href": "R/R-basics.html#not-availables-na",
    "title": "2  R basics",
    "section": "\n2.6 Not availables (NA)",
    "text": "2.6 Not availables (NA)\nWhen a function tries to coerce one type to another and encounters an impossible case, it usually gives us a warning and turns the entry into a special value called an NA for “not available”. For example:\n\nx &lt;- c(\"1\", \"b\", \"3\")\nas.numeric(x)\n#&gt; Warning: NAs introduced by coercion\n#&gt; [1]  1 NA  3\n\nR does not have any guesses for what number you want when you type b, so it does not try.\nAs a data scientist you will encounter the NAs often as they are generally used for missing data, a common problem in real-world datasets.\n\n\n\n\n\n\nYou are ready to do exercises 12-23."
  },
  {
    "objectID": "R/R-basics.html#sorting",
    "href": "R/R-basics.html#sorting",
    "title": "2  R basics",
    "section": "\n2.7 Sorting",
    "text": "2.7 Sorting\nNow that we have mastered some basic R knowledge, let’s try to gain some insights into the safety of different states in the context of gun murders.\n\n2.7.1 sort\n\nSay we want to rank the states from least to most gun murders. The function sort sorts a vector in increasing order. We can therefore see the largest number of gun murders by typing:\n\nlibrary(dslabs)\nsort(murders$total)\n#&gt;  [1]    2    4    5    5    7    8   11   12   12   16   19   21   22\n#&gt; [14]   27   32   36   38   53   63   65   67   84   93   93   97   97\n#&gt; [27]   99  111  116  118  120  135  142  207  219  232  246  250  286\n#&gt; [40]  293  310  321  351  364  376  413  457  517  669  805 1257\n\nHowever, this does not give us information about which states have which murder totals. For example, we don’t know which state had 1257.\n\n2.7.2 order\n\nThe function order is closer to what we want. It takes a vector as input and returns the vector of indexes that sorts the input vector. This may sound confusing so let’s look at a simple example. We can create a vector and sort it:\n\nx &lt;- c(31, 4, 15, 92, 65)\nsort(x)\n#&gt; [1]  4 15 31 65 92\n\nRather than sort the input vector, the function order returns the index that sorts input vector:\n\nindex &lt;- order(x)\nx[index]\n#&gt; [1]  4 15 31 65 92\n\nThis is the same output as that returned by sort(x). If we look at this index, we see why it works:\n\nx\n#&gt; [1] 31  4 15 92 65\norder(x)\n#&gt; [1] 2 3 1 5 4\n\nThe second entry of x is the smallest, so order(x) starts with 2. The next smallest is the third entry, so the second entry is 3 and so on.\nHow does this help us order the states by murders? First, remember that the entries of vectors you access with $ follow the same order as the rows in the table. For example, these two vectors containing state names and abbreviations, respectively, are matched by their order:\n\nmurders$state[1:6]\n#&gt; [1] \"Alabama\"    \"Alaska\"     \"Arizona\"    \"Arkansas\"   \"California\"\n#&gt; [6] \"Colorado\"\nmurders$abb[1:6]\n#&gt; [1] \"AL\" \"AK\" \"AZ\" \"AR\" \"CA\" \"CO\"\n\nThis means we can order the state names by their total murders. We first obtain the index that orders the vectors according to murder totals and then index the state names vector:\n\nind &lt;- order(murders$total) \nmurders$abb[ind] \n#&gt;  [1] \"VT\" \"ND\" \"NH\" \"WY\" \"HI\" \"SD\" \"ME\" \"ID\" \"MT\" \"RI\" \"AK\" \"IA\" \"UT\"\n#&gt; [14] \"WV\" \"NE\" \"OR\" \"DE\" \"MN\" \"KS\" \"CO\" \"NM\" \"NV\" \"AR\" \"WA\" \"CT\" \"WI\"\n#&gt; [27] \"DC\" \"OK\" \"KY\" \"MA\" \"MS\" \"AL\" \"IN\" \"SC\" \"TN\" \"AZ\" \"NJ\" \"VA\" \"NC\"\n#&gt; [40] \"MD\" \"OH\" \"MO\" \"LA\" \"IL\" \"GA\" \"MI\" \"PA\" \"NY\" \"FL\" \"TX\" \"CA\"\n\nAccording to the above, California had the most murders.\n\n2.7.3 max and which.max\n\nIf we are only interested in the entry with the largest value, we can use max for the value:\n\nmax(murders$total)\n#&gt; [1] 1257\n\nand which.max for the index of the largest value:\n\ni_max &lt;- which.max(murders$total)\nmurders$state[i_max]\n#&gt; [1] \"California\"\n\nFor the minimum, we can use min and which.min in the same way.\nDoes this mean California is the most dangerous state? In an upcoming section, we argue that we should be considering rates instead of totals. Before doing that, we introduce one last order-related function: rank.\n\n2.7.4 rank\n\nAlthough not as frequently used as order and sort, the function rank is also related to order and can be useful. For any given vector it returns a vector with the rank of the first entry, second entry, etc., of the input vector. Here is a simple example:\n\nx &lt;- c(31, 4, 15, 92, 65)\nrank(x)\n#&gt; [1] 3 1 2 5 4\n\nTo summarize, let’s look at the results of the three functions we have introduced:\n\n\n\n\noriginal\nsort\norder\nrank\n\n\n\n31\n4\n2\n3\n\n\n4\n15\n3\n1\n\n\n15\n31\n1\n2\n\n\n92\n65\n5\n5\n\n\n65\n92\n4\n4\n\n\n\n\n\n\n2.7.5 Beware of recycling\nAnother common source of unnoticed errors in R is the use of recycling. We saw that vectors are added elementwise. So if the vectors don’t match in length, it is natural to assume that we should get an error. But we don’t. Notice what happens:\n\nx &lt;- c(1, 2, 3)\ny &lt;- c(10, 20, 30, 40, 50, 60, 70)\nx + y\n#&gt; Warning in x + y: longer object length is not a multiple of shorter\n#&gt; object length\n#&gt; [1] 11 22 33 41 52 63 71\n\nWe do get a warning, but no error. For the output, R has recycled the numbers in x. Notice the last digit of numbers in the output.\n\n\n\n\n\n\nYou are ready to do exercises 24-31"
  },
  {
    "objectID": "R/R-basics.html#vector-arithmetics",
    "href": "R/R-basics.html#vector-arithmetics",
    "title": "2  R basics",
    "section": "\n2.8 Vector arithmetics",
    "text": "2.8 Vector arithmetics\nCalifornia had the most murders, but does this mean it is the most dangerous state? What if it just has many more people than any other state? We can quickly confirm that California indeed has the largest population:\n\nlibrary(dslabs)\nmurders$state[which.max(murders$population)]\n#&gt; [1] \"California\"\n\nwith over 37 million inhabitants. It is therefore unfair to compare the totals if we are interested in learning how safe the state is. What we really should be computing is the murders per capita. The reports we describe in the motivating section used murders per 100,000 as the unit. To compute this quantity, the powerful vector arithmetic capabilities of R come in handy.\n\n2.8.1 Rescaling a vector\nIn R, arithmetic operations on vectors occur element-wise. For a quick example, suppose we have height in inches:\n\ninches &lt;- c(69, 62, 66, 70, 70, 73, 67, 73, 67, 70)\n\nand want to convert to centimeters. Notice what happens when we multiply inches by 2.54:\n\ninches * 2.54\n#&gt;  [1] 175 157 168 178 178 185 170 185 170 178\n\nIn the line above, we multiplied each element by 2.54. Similarly, if for each entry we want to compute how many inches taller or shorter than 69 inches, the average height for males, we can subtract it from every entry like this:\n\ninches - 69\n#&gt;  [1]  0 -7 -3  1  1  4 -2  4 -2  1\n\n\n2.8.2 Two vectors\nIf we have two vectors of the same length, and we sum them in R, they will be added entry by entry as follows:\n\\[\n\\begin{pmatrix}\na\\\\\nb\\\\\nc\\\\\nd\n\\end{pmatrix}\n+\n\\begin{pmatrix}\ne\\\\\nf\\\\\ng\\\\\nh\n\\end{pmatrix}\n=\n\\begin{pmatrix}\na +e\\\\\nb + f\\\\\nc + g\\\\\nd + h\n\\end{pmatrix}\n\\]\nThe same holds for other mathematical operations, such as -, * and /.\nThis implies that to compute the murder rates we can simply type:\n\nmurder_rate &lt;- murders$total / murders$population * 100000\n\nOnce we do this, we notice that California is no longer near the top of the list. In fact, we can use what we have learned to order the states by murder rate:\n\nmurders$abb[order(murder_rate)]\n#&gt;  [1] \"VT\" \"NH\" \"HI\" \"ND\" \"IA\" \"ID\" \"UT\" \"ME\" \"WY\" \"OR\" \"SD\" \"MN\" \"MT\"\n#&gt; [14] \"CO\" \"WA\" \"WV\" \"RI\" \"WI\" \"NE\" \"MA\" \"IN\" \"KS\" \"NY\" \"KY\" \"AK\" \"OH\"\n#&gt; [27] \"CT\" \"NJ\" \"AL\" \"IL\" \"OK\" \"NC\" \"NV\" \"VA\" \"AR\" \"TX\" \"NM\" \"CA\" \"FL\"\n#&gt; [40] \"TN\" \"PA\" \"AZ\" \"GA\" \"MS\" \"MI\" \"DE\" \"SC\" \"MD\" \"MO\" \"LA\" \"DC\"\n\n\n\n\n\n\n\nYou are now ready to do exercises 32-34."
  },
  {
    "objectID": "R/R-basics.html#indexing",
    "href": "R/R-basics.html#indexing",
    "title": "2  R basics",
    "section": "\n2.9 Indexing",
    "text": "2.9 Indexing\nR provides a powerful and convenient way of indexing vectors. We can, for example, subset a vector based on properties of another vector. In this section, we continue working with our US murders example, which we can load like this:\n\nlibrary(dslabs)\n\n\n2.9.1 Subsetting with logicals\nWe have now calculated the murder rate using:\n\nmurder_rate &lt;- murders$total / murders$population * 100000 \n\nImagine you are moving from Italy where, according to an ABC news report, the murder rate is only 0.71 per 100,000. You would prefer to move to a state with a similar murder rate. Another powerful feature of R is that we can use logicals to index vectors. If we compare a vector to a single number, it actually performs the test for each entry. The following is an example related to the question above:\n\nind &lt;- murder_rate &lt; 0.71\n\nIf we instead want to know if a value is less or equal, we can use:\n\nind &lt;- murder_rate &lt;= 0.71\n\nNote that we get back a logical vector with TRUE for each entry smaller than or equal to 0.71. To see which states these are, we can leverage the fact that vectors can be indexed with logicals.\n\nmurders$state[ind]\n#&gt; [1] \"Hawaii\"        \"Iowa\"          \"New Hampshire\" \"North Dakota\" \n#&gt; [5] \"Vermont\"\n\nIn order to count how many are TRUE, the function sum returns the sum of the entries of a vector and logical vectors get coerced to numeric with TRUE coded as 1 and FALSE as 0. Thus we can count the states using:\n\nsum(ind)\n#&gt; [1] 5\n\n\n2.9.2 Logical operators\nSuppose we like the mountains and we want to move to a safe state in the western region of the country. We want the murder rate to be at most 1. In this case, we want two different things to be true. Here we can use the logical operator and, which in R is represented with &. This operation results in TRUE only when both logicals are TRUE. To see this, consider this example:\n\nTRUE & TRUE\n#&gt; [1] TRUE\nTRUE & FALSE\n#&gt; [1] FALSE\nFALSE & FALSE\n#&gt; [1] FALSE\n\nFor our example, we can form two logicals:\n\nwest &lt;- murders$region == \"West\"\nsafe &lt;- murder_rate &lt;= 1\n\nand we can use the & to get a vector of logicals that tells us which states satisfy both conditions:\n\nind &lt;- safe & west\nmurders$state[ind]\n#&gt; [1] \"Hawaii\"  \"Idaho\"   \"Oregon\"  \"Utah\"    \"Wyoming\"\n\n\n2.9.3 which\n\nSuppose we want to look up California’s murder rate. For this type of operation, it is convenient to convert vectors of logicals into indexes instead of keeping long vectors of logicals. The function which tells us which entries of a logical vector are TRUE. So we can type:\n\nind &lt;- which(murders$state == \"California\")\nmurder_rate[ind]\n#&gt; [1] 3.37\n\n\n2.9.4 match\n\nIf instead of just one state we want to find out the murder rates for several states, say New York, Florida, and Texas, we can use the function match. This function tells us which indexes of a second vector match each of the entries of a first vector:\n\nind &lt;- match(c(\"New York\", \"Florida\", \"Texas\"), murders$state)\nind\n#&gt; [1] 33 10 44\n\nNow we can look at the murder rates:\n\nmurder_rate[ind]\n#&gt; [1] 2.67 3.40 3.20\n\n\n2.9.5 %in%\n\nIf rather than an index we want a logical that tells us whether or not each element of a first vector is in a second, we can use the function %in%. Let’s imagine you are not sure if Boston, Dakota, and Washington are states. You can find out like this:\n\nc(\"Boston\", \"Dakota\", \"Washington\") %in% murders$state\n#&gt; [1] FALSE FALSE  TRUE\n\nNote that we will be using %in% often throughout the book.\nThere is a connection between match and %in% through which. To see this, notice that the following two lines produce the same index (although in different order):::::\n\nmatch(c(\"New York\", \"Florida\", \"Texas\"), murders$state)\n#&gt; [1] 33 10 44\nwhich(murders$state %in% c(\"New York\", \"Florida\", \"Texas\"))\n#&gt; [1] 10 33 44\n\n\n\n\n\n\n\nYou are now ready to do exercies 35-42."
  },
  {
    "objectID": "R/R-basics.html#basic-plots",
    "href": "R/R-basics.html#basic-plots",
    "title": "2  R basics",
    "section": "\n2.10 Basic plots",
    "text": "2.10 Basic plots\nIn Chapter Chapter 8 we describe an add-on package that provides a powerful approach to producing plots in R. We then have an entire part on Data Visualization in which we provide many examples. Here we briefly describe some of the functions that are available in a basic R installation.\n\n2.10.1 plot\n\nThe plot function can be used to make scatterplots. Here is a plot of total murders versus population.\n\nx &lt;- murders$population / 10^6\ny &lt;- murders$total\nplot(x, y)\n\n\n\n\n\n\n\n\n\nFor a quick plot that avoids accessing variables twice, we can use the with function:\n\nwith(murders, plot(population, total))\n\nThe function with lets us use the murders column names in the plot function. It also works with any data frames and any function.\n\n2.10.2 hist\n\nWe will describe histograms as they relate to distributions in the Data Visualization part of the book. Here we will simply note that histograms are a powerful graphical summary of a list of numbers that gives you a general overview of the types of values you have. We can make a histogram of our murder rates by simply typing:\n\nx &lt;- with(murders, total / population * 100000)\nhist(x)\n\n\n\n\n\n\n\n\n\nWe can see that there is a wide range of values with most of them between 2 and 3 and one very extreme case with a murder rate of more than 15:\n\nmurders$state[which.max(x)]\n#&gt; [1] \"District of Columbia\"\n\n\n2.10.3 boxplot\n\nBoxplots will also be described in the Data Visualization part of the book. They provide a more terse summary than histograms, but they are easier to stack with other boxplots. For example, here we can use them to compare the different regions:\n\nmurders$rate &lt;- with(murders, total / population * 100000)\nboxplot(rate~region, data = murders)\n\n\n\n\n\n\n\n\n\nWe can see that the South has higher murder rates than the other three regions.\n\n2.10.4 image\n\nThe image function displays the values in a matrix using color. Here is a quick example:\n\nx &lt;- matrix(1:120, 12, 10)\nimage(x)"
  },
  {
    "objectID": "R/R-basics.html#exercises",
    "href": "R/R-basics.html#exercises",
    "title": "2  R basics",
    "section": "\n2.11 Exercises",
    "text": "2.11 Exercises\n1. What is the sum of the first 100 positive integers? The formula for the sum of integers \\(1\\) through \\(n\\) is \\(n(n+1)/2\\). Define \\(n=100\\) and then use R to compute the sum of \\(1\\) through \\(100\\) using the formula. What is the sum?\n2. Now use the same formula to compute the sum of the integers from 1 through 1,000.\n3. Look at the result of typing the following code into R:\n\nn &lt;- 1000\nx &lt;- seq(1, n)\nsum(x)\n\nBased on the result, what do you think the functions seq and sum do? You can use help.\n\n\nsum creates a list of numbers and seq adds them up.\n\nseq creates a list of numbers and sum adds them up.\n\nseq creates a random list and sum computes the sum of 1 through 1,000.\n\nsum always returns the same number.\n\n4. In math and programming, we say that we evaluate a function when we replace the argument with a given number. So if we type sqrt(4), we evaluate the sqrt function. In R, you can evaluate a function inside another function. The evaluations happen from the inside out. Use one line of code to compute the log, in base 10, of the square root of 100.\n5. Which of the following will always return the numeric value stored in x? You can try out examples and use the help system if you want.\n\nlog(10^x)\nlog10(x^10)\nlog(exp(x))\nexp(log(x, base = 2))\n\n6. Make sure the US murders dataset is loaded. Use the function str to examine the structure of the murders object. Which of the following best describes the variables represented in this data frame?\n\nThe 51 states.\nThe murder rates for all 50 states and DC.\nThe state name, the abbreviation of the state name, the state’s region, and the state’s population and total number of murders for 2010.\n\nstr shows no relevant information.\n\n7. What are the column names used by the data frame for these five variables?\n8. Use the accessor $ to extract the state abbreviations and assign them to the object a. What is the class of this object?\n9. Now use the square brackets to extract the state abbreviations and assign them to the object b. Use the identical function to determine if a and b are the same.\n10. We saw that the region column stores a factor. You can corroborate this by typing:\n\nclass(murders$region)\n\nWith one line of code, use the function levels and length to determine the number of regions defined by this dataset.\n11. The function table takes a vector and returns the frequency of each element. You can quickly see how many states are in each region by applying this function. Use this function in one line of code to create a table of states per region.\n12. Use the function c to create a vector with the average high temperatures in January for Beijing, Lagos, Paris, Rio de Janeiro, San Juan, and Toronto, which are 35, 88, 42, 84, 81, and 30 degrees Fahrenheit. Call the object temp.\n13. Now create a vector with the city names and call the object city.\n14. Use the names function and the objects defined in the previous exercises to associate the temperature data with its corresponding city.\n15. Use the [ and : operators to access the temperature of the first three cities on the list.\n16. Use the [ operator to access the temperature of Paris and San Juan.\n17. Use the : operator to create a sequence of numbers \\(12,13,14,\\dots,73\\).\n18. Create a vector containing all the positive odd numbers smaller than 100.\n19. Create a vector of numbers that starts at 6, does not pass 55, and adds numbers in increments of 4/7: 6, 6 + 4/7, 6 + 8/7, and so on. How many numbers does the list have? Hint: use seq and length.\n20. What is the class of the following object a &lt;- seq(1, 10, 0.5)?\n21. What is the class of the following object a &lt;- seq(1, 10)?\n22. The class of class(a&lt;-1) is numeric, not integer. R defaults to numeric and to force an integer, you need to add the letter L. Confirm that the class of 1L is integer.\n23. Define the following vector:\n\nx &lt;- c(\"1\", \"3\", \"5\")\n\nand coerce it to get integers.\n24. For exercises 24-31 we will use the US murders dataset. Make sure you load it prior to starting. Use the $ operator to access the population size data and store it as the object pop. Then use the sort function to redefine pop so that it is sorted. Finally, use the [ operator to report the smallest population size.\n25. Now instead of the smallest population size, find the index of the entry with the smallest population size. Hint: use order instead of sort.\n26. We can actually perform the same operation as in the previous exercise using the function which.min. Write one line of code that does this.\n27. Now we know how small the smallest state is and we know which row represents it. Which state is it? Define a variable states to be the state names from the murders data frame. Report the name of the state with the smallest population.\n28. You can create a data frame using the data.frame function. Here is a quick example:\n\ntemp &lt;- c(35, 88, 42, 84, 81, 30)\ncity &lt;- c(\"Beijing\", \"Lagos\", \"Paris\", \"Rio de Janeiro\", \n          \"San Juan\", \"Toronto\")\ncity_temps &lt;- data.frame(name = city, temperature = temp)\n\nUse the rank function to determine the population rank of each state from smallest population size to biggest. Save these ranks in an object called ranks, then create a data frame with the state name and its rank. Call the data frame my_df.\n29. Repeat the previous exercise, but this time order my_df so that the states are ordered from least populous to most populous. Hint: create an object ind that stores the indexes needed to order the population values. Then use the bracket operator [ to re-order each column in the data frame.\n30. The na_example vector represents a series of counts. You can quickly examine the object using:\n\nstr(na_example)\n#&gt;  int [1:1000] 2 1 3 2 1 3 1 4 3 2 ...\n\nHowever, when we compute the average with the function mean, we obtain an NA:\n\nmean(na_example)\n#&gt; [1] NA\n\nThe is.na function returns a logical vector that tells us which entries are NA. Assign this logical vector to an object called ind and determine how many NAs does na_example have.\n31. Now compute the average again, but only for the entries that are not NA. Hint: remember the ! operator.\n32. In exercises 28 we created the temp data frame:\n\ntemp &lt;- c(35, 88, 42, 84, 81, 30)\ncity &lt;- c(\"Beijing\", \"Lagos\", \"Paris\", \"Rio de Janeiro\", \n          \"San Juan\", \"Toronto\")\ncity_temps &lt;- data.frame(name = city, temperature = temp)\n\nRemake the data frame using the code above, but add a line that converts the temperature from Fahrenheit to Celsius. The conversion is \\(C = \\frac{5}{9} \\times (F - 32)\\).\n33. What is the following sum \\(1+1/2^2 + 1/3^2 + \\dots 1/100^2\\)? Hint: thanks to Euler, we know it should be close to \\(\\pi^2/6\\).\n34. Compute the per 100,000 murder rate for each state and store it in the object murder_rate. Then compute the average murder rate for the US using the function mean. What is the average?\n35. For remaining exercises 35-42, start by loading the library and data.\n\nlibrary(dslabs)\n\nCompute the per 100,000 murder rate for each state and store it in an object called murder_rate. Then use logical operators to create a logical vector named low that tells us which entries of murder_rate are lower than 1.\n36. Now use the results from the previous exercise and the function which to determine the indices of murder_rate associated with values lower than 1.\n37. Use the results from the previous exercise to report the names of the states with murder rates lower than 1.\n38. Now extend the code from exercises 2 and 3 to report the states in the Northeast with murder rates lower than 1. Hint: use the previously defined logical vector low and the logical operator &.\n39. In a previous exercise we computed the murder rate for each state and the average of these numbers. How many states are below the average?\n40. Use the match function to identify the states with abbreviations AK, MI, and IA. Hint: start by defining an index of the entries of murders$abb that match the three abbreviations, then use the [ operator to extract the states.\n41. Use the %in% operator to create a logical vector that answers the question: which of the following are actual abbreviations: MA, ME, MI, MO, MU?\n42. Extend the code you used in exercise 7 to report the one entry that is not an actual abbreviation. Hint: use the ! operator, which turns FALSE into TRUE and vice versa, then which to obtain an index.\n43. We made a plot of total murders versus population and noted a strong relationship. Not surprisingly, states with larger populations had more murders.\n\npopulation_in_millions &lt;- murders$population/10^6\ntotal_gun_murders &lt;- murders$total\nplot(population_in_millions, total_gun_murders)\n\nKeep in mind that many states have populations below 5 million and are bunched up. We may gain further insights from making this plot in the log scale. Transform the variables using the log10 transformation and then plot them.\n44. Create a histogram of the state populations.\n45. Generate boxplots of the state populations by region."
  },
  {
    "objectID": "R/R-basics.html#footnotes",
    "href": "R/R-basics.html#footnotes",
    "title": "2  R basics",
    "section": "",
    "text": "https://rstudio.cloud↩︎\nhttps://rafalab.github.io/dsbook/installing-r-rstudio.html↩︎\nhttp://abcnews.go.com/blogs/headlines/2012/12/us-gun-ownership-homicide-rate-higher-than-other-developed-countries/↩︎\nhttp://adv-r.had.co.nz/Style.html↩︎"
  },
  {
    "objectID": "R/programming-basics.html#sec-conditionals",
    "href": "R/programming-basics.html#sec-conditionals",
    "title": "\n3  Programming basics\n",
    "section": "\n3.1 Conditional expressions",
    "text": "3.1 Conditional expressions\nConditional expressions are one of the basic features of programming. They are used for what is called flow control. The most common conditional expression is the if-else statement. In R, we can actually perform quite a bit of data analysis without conditionals. However, they do come up occasionally, and you will need them once you start writing your own functions and packages.\nHere is a very simple example showing the general structure of an if-else statement. The basic idea is to print the reciprocal of a unless a is 0:\n\na &lt;- 0\n\nif (a != 0) {\n  print(1/a)\n} else{\n  print(\"No reciprocal for 0.\")\n}\n#&gt; [1] \"No reciprocal for 0.\"\n\nLet’s look at one more example using the US murders data frame:\n\nlibrary(dslabs)\nmurder_rate &lt;- murders$total / murders$population*100000\n\nHere is a very simple example that tells us which states, if any, have a murder rate lower than 0.5 per 100,000. The if-else statement protects us from the case in which no state satisfies the condition.\n\nind &lt;- which.min(murder_rate)\n\nif (murder_rate[ind] &lt; 0.5) {\n  print(murders$state[ind]) \n} else{\n  print(\"No state has murder rate that low\")\n}\n#&gt; [1] \"Vermont\"\n\nIf we try it again with a rate of 0.25, we get a different answer:\n\nif (murder_rate[ind] &lt; 0.25) {\n  print(murders$state[ind]) \n} else{\n  print(\"No state has a murder rate that low.\")\n}\n#&gt; [1] \"No state has a murder rate that low.\"\n\nA related function that is very useful is ifelse. This function takes three arguments: a logical and two possible answers. If the logical is TRUE, the value in the second argument is returned and if FALSE, the value in the third argument is returned. Here is an example:\n\na &lt;- 0\nifelse(a &gt; 0, 1/a, NA)\n#&gt; [1] NA\n\nThe function is particularly useful because it works on vectors. It examines each entry of the logical vector and returns elements from the vector provided in the second argument, if the entry is TRUE, or elements from the vector provided in the third argument, if the entry is FALSE.\n\na &lt;- c(0, 1, 2, -4, 5)\nresult &lt;- ifelse(a &gt; 0, 1/a, NA)\n\nThis table helps us see what happened:\n\n\n\n\na\nis_a_positive\nanswer1\nanswer2\nresult\n\n\n\n0\nFALSE\nInf\nNA\nNA\n\n\n1\nTRUE\n1.00\nNA\n1.0\n\n\n2\nTRUE\n0.50\nNA\n0.5\n\n\n-4\nFALSE\n-0.25\nNA\nNA\n\n\n5\nTRUE\n0.20\nNA\n0.2\n\n\n\n\n\nHere is an example of how this function can be readily used to replace all the missing values in a vector with zeros:\n\nno_nas &lt;- ifelse(is.na(na_example), 0, na_example) \nsum(is.na(no_nas))\n#&gt; [1] 0\n\nTwo other useful functions are any and all. The any function takes a vector of logicals and returns TRUE if any of the entries is TRUE. The all function takes a vector of logicals and returns TRUE if all of the entries are TRUE. Here is an example:\n\nz &lt;- c(TRUE, TRUE, FALSE)\nany(z)\n#&gt; [1] TRUE\nall(z)\n#&gt; [1] FALSE"
  },
  {
    "objectID": "R/programming-basics.html#defining-functions",
    "href": "R/programming-basics.html#defining-functions",
    "title": "\n3  Programming basics\n",
    "section": "\n3.2 Defining functions",
    "text": "3.2 Defining functions\nAs you become more experienced, you will find yourself needing to perform the same operations over and over. A simple example is computing averages. We can compute the average of a vector x using the sum and length functions: sum(x)/length(x). Because we do this repeatedly, it is much more efficient to write a function that performs this operation. This particular operation is so common that someone already wrote the mean function and it is included in base R. However, you will encounter situations in which the function does not already exist, so R permits you to write your own. A simple version of a function that computes the average can be defined like this:\n\navg &lt;- function(x){\n  s &lt;- sum(x)\n  n &lt;- length(x)\n  s/n\n}\n\nNow avg is a function that computes the mean:\n\nx &lt;- 1:100\nidentical(mean(x), avg(x))\n#&gt; [1] TRUE\n\nNotice that variables defined inside a function are not saved in the workspace. So while we use s and n when we call avg, the values are created and changed only during the call. Here is an illustrative example:\n\ns &lt;- 3\navg(1:10)\n#&gt; [1] 5.5\ns\n#&gt; [1] 3\n\nNote how s is still 3 after we call avg.\nIn general, functions are objects, so we assign them to variable names with &lt;-. The function function tells R you are about to define a function. The general form of a function definition looks like this:\n\nmy_function &lt;- function(VARIABLE_NAME){\n  perform operations on VARIABLE_NAME and calculate VALUE\n  VALUE\n}\n\nThe functions you define can have multiple arguments as well as default values. For example, we can define a function that computes either the arithmetic or geometric average depending on a user defined variable like this:\n\navg &lt;- function(x, arithmetic = TRUE){\n  n &lt;- length(x)\n  ifelse(arithmetic, sum(x)/n, prod(x)^(1/n))\n}\n\nWe will learn more about how to create functions through experience as we face more complex tasks."
  },
  {
    "objectID": "R/programming-basics.html#namespaces",
    "href": "R/programming-basics.html#namespaces",
    "title": "\n3  Programming basics\n",
    "section": "\n3.3 Namespaces",
    "text": "3.3 Namespaces\nOnce you start becoming more of an R expert user, you will likely need to load several add-on packages for some of your analysis. Once you start doing this, it is likely that two packages use the same name for two different functions. And often these functions do completely different things. In fact, you have already encountered this because both dplyr and the R-base stats package define a filter function. There are five other examples in dplyr. We know this because when we first load dplyr we see the following message:\nThe following objects are masked from ‘package:stats’:\n\n    filter, lag\n\nThe following objects are masked from ‘package:base’:\n\n    intersect, setdiff, setequal, union\nNow when we type filter it uses the dplyr one. But what if we want to use the stats version?\nThese functions live in different namespaces. R will follow a certain order when searching for a function in these namespaces. You can see the order by typing:\n\nsearch()\n\nThe first entry in this list is the global environment which includes all the objects you define.\nSo what if we want to use the stats filter instead of the dplyr filter but dplyr appears first in the search list? You can force the use of a specific namespace by using double colons (::) like this:\n\nstats::filter\n\nIf we want to be absolutely sure that we use the dplyr filter, we can use\n\ndplyr::filter\n\nAlso note that if we want to use a function in a package without loading the entire package, we can use the double colon as well.\n\n\n\n\n\n\nIf you want to see all the packages that have function called, for example filter, you can use double questions marks: ??filter.\n\n\n\nFor more on this more advanced topic we recommend the R packages book1."
  },
  {
    "objectID": "R/programming-basics.html#for-loops",
    "href": "R/programming-basics.html#for-loops",
    "title": "\n3  Programming basics\n",
    "section": "\n3.4 For-loops",
    "text": "3.4 For-loops\nThe formula for the sum of the series \\(1+2+\\dots+n\\) is \\(n(n+1)/2\\). What if we weren’t sure that was the right function? How could we check? Using what we learned about functions we can create one that computes the \\(S_n\\):\n\ncompute_s_n &lt;- function(n) { \n  sum(1:n)\n}\n\nHow can we compute \\(S_n\\) for various values of \\(n\\), say \\(n=1,\\dots,25\\)? Do we write 25 lines of code calling compute_s_n? No, that is what for-loops are for in programming. In this case, we are performing exactly the same task over and over, and the only thing that is changing is the value of \\(n\\). For-loops let us define the range that our variable takes (in our example \\(n=1,\\dots,10\\)), then change the value and evaluate expression as you loop.\nPerhaps the simplest example of a for-loop is this useless piece of code:\n\nfor (i in 1:5) {\n  print(i)\n}\n#&gt; [1] 1\n#&gt; [1] 2\n#&gt; [1] 3\n#&gt; [1] 4\n#&gt; [1] 5\n\nHere is the for-loop we would write for our \\(S_n\\) example:\n\nm &lt;- 25\ns_n &lt;- vector(length = m) # create an empty vector\nfor (n in 1:m) {\n  s_n[n] &lt;- compute_s_n(n)\n}\n\nIn each iteration \\(n=1\\), \\(n=2\\), etc…, we compute \\(S_n\\) and store it in the \\(n\\)th entry of s_n.\nNow we can create a plot to search for a pattern:\n\nn &lt;- 1:m\nplot(n, s_n)\n\n\n\n\n\n\n\n\n\nIf you noticed that it appears to be a quadratic, you are on the right track because the formula is \\(n(n+1)/2\\)."
  },
  {
    "objectID": "R/programming-basics.html#sec-vectorization",
    "href": "R/programming-basics.html#sec-vectorization",
    "title": "\n3  Programming basics\n",
    "section": "\n3.5 Vectorization and functionals",
    "text": "3.5 Vectorization and functionals\nAlthough for-loops are an important concept to understand, in R we rarely use them. As you learn more R, you will realize that vectorization is preferred over for-loops since it results in shorter and clearer code. We already saw examples in the Vector Arithmetic section. A vectorized function is a function that will apply the same operation on each of the vectors.\n\nx &lt;- 1:10\nsqrt(x)\n#&gt;  [1] 1.00 1.41 1.73 2.00 2.24 2.45 2.65 2.83 3.00 3.16\ny &lt;- 1:10\nx*y\n#&gt;  [1]   1   4   9  16  25  36  49  64  81 100\n\nTo make this calculation, there is no need for for-loops. However, not all functions work this way. For instance, the function we just wrote, compute_s_n, does not work element-wise since it is expecting a scalar. This piece of code does not run the function on each entry of n:\n\nn &lt;- 1:25\ncompute_s_n(n)\n\nFunctionals are functions that help us apply the same function to each entry in a vector, matrix, data frame, or list. Here we cover the functional that operates on numeric, logical, and character vectors: sapply.\nThe function sapply permits us to perform element-wise operations on any function. Here is how it works:\n\nx &lt;- 1:10\nsapply(x, sqrt)\n#&gt;  [1] 1.00 1.41 1.73 2.00 2.24 2.45 2.65 2.83 3.00 3.16\n\nEach element of x is passed on to the function sqrt and the result is returned. These results are concatenated. In this case, the result is a vector of the same length as the original x. This implies that the for-loop above can be written as follows:\n\nn &lt;- 1:25\ns_n &lt;- sapply(n, compute_s_n)\n\nOther functionals are apply, lapply, tapply, mapply, vapply, and replicate. We mostly use sapply, apply, and replicate in this book, but we recommend familiarizing yourselves with the others as they can be very useful."
  },
  {
    "objectID": "R/programming-basics.html#exercises",
    "href": "R/programming-basics.html#exercises",
    "title": "\n3  Programming basics\n",
    "section": "\n3.6 Exercises",
    "text": "3.6 Exercises\n1. What will this conditional expression return?\n\nx &lt;- c(1,2,-3,4)\n\nif(all(x&gt;0)){\n  print(\"All Postives\")\n} else{\n  print(\"Not all positives\")\n}\n\n2. Which of the following expressions is always FALSE when at least one entry of a logical vector x is TRUE?\n\nall(x)\nany(x)\nany(!x)\nall(!x)\n\n3. The function nchar tells you how many characters long a character vector is. Write a line of code that assigns to the object new_names the state abbreviation when the state name is longer than 8 characters.\n4. Create a function sum_n that for any given value, say \\(n\\), computes the sum of the integers from 1 to n (inclusive). Use the function to determine the sum of integers from 1 to 5,000.\n5. Create a function altman_plot that takes two arguments, x and y, and plots the difference against the sum.\n6. After running the code below, what is the value of x?\n\nx &lt;- 3\nmy_func &lt;- function(y){\n  x &lt;- 5\n  y+5\n}\n\n7. Write a function compute_s_n that for any given \\(n\\) computes the sum \\(S_n = 1^2 + 2^2 + 3^2 + \\dots n^2\\). Report the value of the sum when \\(n=10\\).\n8. Define an empty numerical vector s_n of size 25 using s_n &lt;- vector(\"numeric\", 25) and store in the results of \\(S_1, S_2, \\dots S_{25}\\) using a for-loop.\n9. Repeat exercise 8, but this time use sapply.\n10. Repeat exercise 8, but this time use map_dbl.\n11. Plot \\(S_n\\) versus \\(n\\). Use points defined by \\(n=1,\\dots,25\\).\n12. Confirm that the formula for this sum is \\(S_n= n(n+1)(2n+1)/6\\)."
  },
  {
    "objectID": "R/programming-basics.html#footnotes",
    "href": "R/programming-basics.html#footnotes",
    "title": "\n3  Programming basics\n",
    "section": "",
    "text": "http://r-pkgs.had.co.nz/namespace.html↩︎"
  },
  {
    "objectID": "R/tidyverse.html#sec-tidy-data",
    "href": "R/tidyverse.html#sec-tidy-data",
    "title": "4  The tidyverse",
    "section": "\n4.1 Tidy data",
    "text": "4.1 Tidy data\nWe say that a data table is in tidy format if each row represents one observation and columns represent the different variables available for each of these observations. The murders dataset is an example of a tidy data frame.\n\n#&gt;        state abb region population total\n#&gt; 1    Alabama  AL  South    4779736   135\n#&gt; 2     Alaska  AK   West     710231    19\n#&gt; 3    Arizona  AZ   West    6392017   232\n#&gt; 4   Arkansas  AR  South    2915918    93\n#&gt; 5 California  CA   West   37253956  1257\n#&gt; 6   Colorado  CO   West    5029196    65\n\nEach row represent a state with each of the five columns providing a different variable related to these states: name, abbreviation, region, population, and total murders.\nTo see how the same information can be provided in different formats, consider the following example:\n\n#&gt;       country year fertility\n#&gt; 1     Germany 1960      2.41\n#&gt; 2 South Korea 1960      6.16\n#&gt; 3     Germany 1961      2.44\n#&gt; 4 South Korea 1961      5.99\n#&gt; 5     Germany 1962      2.47\n#&gt; 6 South Korea 1962      5.79\n\nThis tidy dataset provides fertility rates for two countries across the years. This is a tidy dataset because each row presents one observation with the three variables being country, year, and fertility rate. However, this dataset originally came in another format and was reshaped for the dslabs package. Originally, the data was in the following format:\n\n#&gt;       country 1960 1961 1962\n#&gt; 1     Germany 2.41 2.44 2.47\n#&gt; 2 South Korea 6.16 5.99 5.79\n\nThe same information is provided, but there are two important differences in the format: 1) each row includes several observations and 2) one of the variables, year, is stored in the header. For the tidyverse packages to be optimally used, data need to be reshaped into tidy format, which you will learn to do in the Data Wrangling part of the book. Until then, we will use example datasets that are already in tidy format.\nAlthough not immediately obvious, as you go through the book you will start to appreciate the advantages of working in a framework in which functions use tidy formats for both inputs and outputs. You will see how this permits the data analyst to focus on more important aspects of the analysis rather than the format of the data.\n\n\n\n\n\n\nYou are ready to do exercises 1-4."
  },
  {
    "objectID": "R/tidyverse.html#refining-data-frames",
    "href": "R/tidyverse.html#refining-data-frames",
    "title": "4  The tidyverse",
    "section": "\n4.2 Refining data frames",
    "text": "4.2 Refining data frames\nThe dplyr package from the tidyverse introduces functions that perform some of the most common operations when working with data frames and uses names for these functions that are relatively easy to remember. For instance, to change the data table by adding a new column, we use mutate. To filter the data table to a subset of rows, we use filter. Finally, to subset the data by selecting specific columns, we use select.\n\n4.2.1 Adding columns\nWe want all the necessary information for our analysis to be included in the data table. So the first task is to add the murder rates to our murders data frame. The function mutate takes the data frame as a first argument and the name and values of the variable as a second argument using the convention name = values. So, to add murder rates, we use:\n\nmurders &lt;- mutate(murders, rate = total/population*100000)\n\nNotice that here we used total and population inside the function, which are objects that are not defined in our workspace. But why don’t we get an error?\nThis is one of dplyr’s main features. Functions in this package, such as mutate, know to look for variables in the data frame provided in the first argument. In the call to mutate above, total will have the values in murders$total. This approach makes the code much more readable.\nWe can see that the new column is added:\n\nhead(murders)\n#&gt;        state abb region population total rate\n#&gt; 1    Alabama  AL  South    4779736   135 2.82\n#&gt; 2     Alaska  AK   West     710231    19 2.68\n#&gt; 3    Arizona  AZ   West    6392017   232 3.63\n#&gt; 4   Arkansas  AR  South    2915918    93 3.19\n#&gt; 5 California  CA   West   37253956  1257 3.37\n#&gt; 6   Colorado  CO   West    5029196    65 1.29\n\n\n4.2.2 Row-wise subsetting\nNow suppose that we want to filter the data table to only show the entries for which the murder rate is lower than 0.71. To do this we use the filter function, which takes the data table as the first argument and then the conditional statement as the second. Like mutate, we can use the unquoted variable names from murders inside the function and it will know we mean the columns and not objects in the workspace.\n\nfilter(murders, rate &lt;= 0.71)\n#&gt;           state abb        region population total  rate\n#&gt; 1        Hawaii  HI          West    1360301     7 0.515\n#&gt; 2          Iowa  IA North Central    3046355    21 0.689\n#&gt; 3 New Hampshire  NH     Northeast    1316470     5 0.380\n#&gt; 4  North Dakota  ND North Central     672591     4 0.595\n#&gt; 5       Vermont  VT     Northeast     625741     2 0.320\n\n\n4.2.3 Column-wise subsetting\nAlthough our data table only has six columns, some data tables include hundreds. If we want to view just a few, we can use the dplyr select function. In the code below we select three columns, assign this to a new object and then filter the new object:\n\nnew_table &lt;- select(murders, state, region, rate)\nfilter(new_table, rate &lt;= 0.71)\n#&gt;           state        region  rate\n#&gt; 1        Hawaii          West 0.515\n#&gt; 2          Iowa North Central 0.689\n#&gt; 3 New Hampshire     Northeast 0.380\n#&gt; 4  North Dakota North Central 0.595\n#&gt; 5       Vermont     Northeast 0.320\n\nIn the call to select, the first argument murders is an object, but state, region, and rate are variable names.\ndplyr offers a series of help functions to select columns based on their content. For example, the following code uses the function where to keep only the numeric columns:\n\nmurders |&gt; select(where(is.numeric)) |&gt; names()\n#&gt; [1] \"population\" \"total\"      \"rate\"\n\nThe helper functions starts_with, ends_with, contains, matches, and num_range can be used to select columns based on their names. Here is an exanple showing all the rows that start with r:\n\nnew_table |&gt; select(starts_with(\"r\")) |&gt; names()\n#&gt; [1] \"region\" \"rate\"\n\nThe helper function everything selects all columns.\n\n4.2.4 Transforming variables\nThe function mutate can also be used to transform variables. For example, the following code takes the log transformation of the population variable:\n\nmurders |&gt; mutate(population = log10(population))\n\nOften, we need to apply the same transformation to several variables. The function across facilitates the operation. For example if want to log transform both population and total murders we can use:\n\nmurders |&gt; mutate(across(c(population, total), log10))\n\nThe helper functions come in handy when using across. An example is if we want to apply the same transformation to all numeric variables:\n\nmurders |&gt; mutate(across(where(is.numeric), log10))\n\nor all character variables:\n\nmurders |&gt; mutate(across(where(is.character), tolower))\n\n\n\n\n\n\n\nYou are ready to do exercises 5-11."
  },
  {
    "objectID": "R/tidyverse.html#the-pipe",
    "href": "R/tidyverse.html#the-pipe",
    "title": "4  The tidyverse",
    "section": "\n4.3 The pipe",
    "text": "4.3 The pipe\nIn R we can perform a series of operations, for example select and then filter, by sending the results of one function to another using what is called the pipe operator: |&gt;. Since R version 4.1.0, you can also use |&gt;. Some details are included below.\nWe wrote code above to show three variables (state, region, rate) for states that have murder rates below 0.71. To do this, we defined the intermediate object new_table. In dplyr we can write code that looks more like a description of what we want to do without intermediate objects:\n\\[ \\mbox{original data }\n\\rightarrow \\mbox{ select }\n\\rightarrow \\mbox{ filter } \\]\nFor such an operation, we can use the pipe |&gt;. The code looks like this:\n\nmurders |&gt; select(state, region, rate) |&gt; filter(rate &lt;= 0.71)\n#&gt;           state        region  rate\n#&gt; 1        Hawaii          West 0.515\n#&gt; 2          Iowa North Central 0.689\n#&gt; 3 New Hampshire     Northeast 0.380\n#&gt; 4  North Dakota North Central 0.595\n#&gt; 5       Vermont     Northeast 0.320\n\nThis line of code is equivalent to the two lines of code above. What is going on here?\nIn general, the pipe sends the result of the left side of the pipe to be the first argument of the function on the right side of the pipe. Here is a very simple example:\n\n16 |&gt; sqrt()\n#&gt; [1] 4\n\nWe can continue to pipe values along:\n\n16 |&gt; sqrt() |&gt; log2()\n#&gt; [1] 2\n\nThe above statement is equivalent to log2(sqrt(16)).\nRemember that the pipe sends values to the first argument, so we can define other arguments as if the first argument is already defined:\n\n16 |&gt; sqrt() |&gt; log(base = 2)\n#&gt; [1] 2\n\nTherefore, when using the pipe with data frames and dplyr, we no longer need to specify the required first argument since the dplyr functions we have described all take the data as the first argument. In the code we wrote:\n\nmurders |&gt; select(state, region, rate) |&gt; filter(rate &lt;= 0.71)\n\nmurders is the first argument of the select function, and the new data frame (formerly new_table) is the first argument of the filter function.\nNote that the pipe works well with functions where the first argument is the input data. Functions in tidyverse packages like dplyr have this format and can be used easily with the pipe."
  },
  {
    "objectID": "R/tidyverse.html#summarizing-data",
    "href": "R/tidyverse.html#summarizing-data",
    "title": "4  The tidyverse",
    "section": "\n4.4 Summarizing data",
    "text": "4.4 Summarizing data\nAn important part of exploratory data analysis is summarizing data. The average and standard deviation are two examples of widely used summary statistics. More informative summaries can often be achieved by first splitting data into groups. In this section, we cover two new dplyr verbs that make these computations easier: summarize and group_by. We learn to access resulting values using the pull function.\n\n4.4.1 summarize\n\nThe summarize function in dplyr provides a way to compute summary statistics with intuitive and readable code. We start with a simple example based on heights. The heights dataset includes heights and sex reported by students in an in-class survey.\n\nlibrary(dplyr)\nlibrary(dslabs)\n\nThe following code computes the average and standard deviation for females:\n\ns &lt;- heights |&gt; \n  filter(sex == \"Female\") |&gt;\n  summarize(average = mean(height), standard_deviation = sd(height))\ns\n#&gt;   average standard_deviation\n#&gt; 1    64.9               3.76\n\nThis takes our original data table as input, filters it to keep only females, and then produces a new summarized table with just the average and the standard deviation of heights. We get to choose the names of the columns of the resulting table. For example, above we decided to use average and standard_deviation, but we could have used other names just the same.\nBecause the resulting table stored in s is a data frame, we can access the components with the accessor $:\n\ns$average\n#&gt; [1] 64.9\ns$standard_deviation\n#&gt; [1] 3.76\n\nAs with most other dplyr functions, summarize is aware of the variable names and we can use them directly. So when inside the call to the summarize function we write mean(height), the function is accessing the column with the name “height” and then computing the average of the resulting numeric vector. We can compute any other summary that operates on vectors and returns a single value.\nFor another example of how we can use the summarize function, let’s compute the average murder rate for the United States. Remember our data table includes total murders and population size for each state and we have already used dplyr to add a murder rate column:\n\nmurders &lt;- murders |&gt; mutate(rate = total/population*100000)\n\nRemember that the US murder rate is not the average of the state murder rates:\n\nsummarize(murders, mean(rate))\n#&gt;   mean(rate)\n#&gt; 1       2.78\n\nThis is because in the computation above, the small states are given the same weight as the large ones. The US murder rate is the total number of murders in the US divided by the total US population. So the correct computation is:\n\nus_murder_rate &lt;- murders |&gt;\n  summarize(rate = sum(total)/sum(population)*100000)\nus_murder_rate\n#&gt;   rate\n#&gt; 1 3.03\n\nThis computation counts larger states proportionally to their size which results in a larger value.\n\n4.4.2 Multiple summaries\nSuppose we want three summaries from the same variable such as the median, minimum, and maximum heights. We can use summarize like this:\n\nheights |&gt; summarize(median = median(height), min = min(height), max = max(height))\n#&gt;   median min  max\n#&gt; 1   68.5  50 82.7\n\nBut we can obtain these three values with just one line using the quantile function: quantile(x, c(0.5, 0, 1)) returns the median (50th percentile), the min (0th percentile), and max (100th percentile) of the vector x. Here we can’t use summarize because it expects one value per row. Instead we have to use the reframe function:\n\nheights |&gt; reframe(quantiles = quantile(height, c(0.5, 0, 1)))\n#&gt;   quantiles\n#&gt; 1      68.5\n#&gt; 2      50.0\n#&gt; 3      82.7\n\nHowever, if we want a column per summary, as the summarize call above, we have to define a function that returns a data frame like this:\n\nmedian_min_max &lt;- function(x){\n  qs &lt;- quantile(x, c(0.5, 0, 1))\n  data.frame(median = qs[1], min = qs[2], max = qs[3])\n}\n\nThen we can call summarize as above:\n\nheights |&gt; summarize(median_min_max(height))\n#&gt;   median min  max\n#&gt; 1   68.5  50 82.7\n\n\n4.4.3 Group then summarize with group_by\n\nA common operation in data exploration is to first split data into groups and then compute summaries for each group. For example, we may want to compute the average and standard deviation for men’s and women’s heights separately. The group_by function helps us do this.\nIf we type this:\n\nheights |&gt; group_by(sex)\n#&gt; # A tibble: 1,050 × 2\n#&gt; # Groups:   sex [2]\n#&gt;   sex   height\n#&gt;   &lt;fct&gt;  &lt;dbl&gt;\n#&gt; 1 Male      75\n#&gt; 2 Male      70\n#&gt; 3 Male      68\n#&gt; 4 Male      74\n#&gt; 5 Male      61\n#&gt; # ℹ 1,045 more rows\n\nThe result does not look very different from heights, except we see Groups: sex [2] when we print the object. Although not immediately obvious from its appearance, this is now a special data frame called a grouped data frame, and dplyr functions, in particular summarize, will behave differently when acting on this object. Conceptually, you can think of this table as many tables, with the same columns but not necessarily the same number of rows, stacked together in one object. When we summarize the data after grouping, this is what happens:\n\nheights |&gt; \n  group_by(sex) |&gt;\n  summarize(average = mean(height), standard_deviation = sd(height))\n#&gt; # A tibble: 2 × 3\n#&gt;   sex    average standard_deviation\n#&gt;   &lt;fct&gt;    &lt;dbl&gt;              &lt;dbl&gt;\n#&gt; 1 Female    64.9               3.76\n#&gt; 2 Male      69.3               3.61\n\nThe summarize function applies the summarization to each group separately.\nFor another example, let’s compute the median, minimum, and maximum murder rate in the four regions of the country using the median_min_max defined above:\n\nmurders |&gt; \n  group_by(region) |&gt;\n  summarize(median_min_max(rate))\n#&gt; # A tibble: 4 × 4\n#&gt;   region        median   min   max\n#&gt;   &lt;fct&gt;          &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 Northeast       1.80 0.320  3.60\n#&gt; 2 South           3.40 1.46  16.5 \n#&gt; 3 North Central   1.97 0.595  5.36\n#&gt; 4 West            1.29 0.515  3.63\n\n\n4.4.4 pull\n\nThe us_murder_rate object defined above represents just one number. Yet we are storing it in a data frame:\n\nclass(us_murder_rate)\n#&gt; [1] \"data.frame\"\n\nsince, as most dplyr functions, summarize always returns a data frame.\nThis might be problematic if we want to use this result with functions that require a numeric value. Here we show a useful trick for accessing values stored in data when using pipes: when a data object is piped that object and its columns can be accessed using the pull function. To get a number from the original data table with one line of code we can type:\n\nus_murder_rate &lt;- murders |&gt; \n  summarize(rate = sum(total)/sum(population)*100000) |&gt;\n  pull(rate)\n\nus_murder_rate\n#&gt; [1] 3.03\n\nwhich is now a numeric:\n\nclass(us_murder_rate)\n#&gt; [1] \"numeric\""
  },
  {
    "objectID": "R/tidyverse.html#sorting",
    "href": "R/tidyverse.html#sorting",
    "title": "4  The tidyverse",
    "section": "\n4.5 Sorting",
    "text": "4.5 Sorting\nWhen examining a dataset, it is often convenient to sort the table by the different columns. We know about the order and sort function, but for ordering entire tables, the dplyr function arrange is useful. For example, here we order the states by population size:\n\nmurders |&gt; arrange(population) |&gt; head()\n#&gt;                  state abb        region population total   rate\n#&gt; 1              Wyoming  WY          West     563626     5  0.887\n#&gt; 2 District of Columbia  DC         South     601723    99 16.453\n#&gt; 3              Vermont  VT     Northeast     625741     2  0.320\n#&gt; 4         North Dakota  ND North Central     672591     4  0.595\n#&gt; 5               Alaska  AK          West     710231    19  2.675\n#&gt; 6         South Dakota  SD North Central     814180     8  0.983\n\nWith arrange we get to decide which column to sort by. To see the states by, for example, we would use arrange(rate) instead.\nNote that the default behavior is to order in ascending order. In dplyr, the function desc transforms a vector so that it is in descending order. To sort the table in descending order, we can type:\n\nmurders |&gt; arrange(desc(rate)) \n\n\n4.5.1 Nested sorting\nIf we are ordering by a column with ties, we can use a second column to break the tie. Similarly, a third column can be used to break ties between first and second and so on. Here we order by region, then within region we order by murder rate:\n\nmurders |&gt; \n  arrange(region, rate) |&gt; \n  head()\n#&gt;           state abb    region population total  rate\n#&gt; 1       Vermont  VT Northeast     625741     2 0.320\n#&gt; 2 New Hampshire  NH Northeast    1316470     5 0.380\n#&gt; 3         Maine  ME Northeast    1328361    11 0.828\n#&gt; 4  Rhode Island  RI Northeast    1052567    16 1.520\n#&gt; 5 Massachusetts  MA Northeast    6547629   118 1.802\n#&gt; 6      New York  NY Northeast   19378102   517 2.668\n\n\n4.5.2 The top \\(n\\)\n\nIn the code above, we have used the function head to avoid having the page fill up with the entire dataset. If we want to see a larger proportion, we can use the top_n function. This function takes a data frame as it’s first argument, the number of rows to show in the second, and the variable to filter by in the third. Here is an example of how to see the top 5 rows:\n\nmurders |&gt; top_n(5, rate)\n#&gt;                  state abb        region population total  rate\n#&gt; 1 District of Columbia  DC         South     601723    99 16.45\n#&gt; 2            Louisiana  LA         South    4533372   351  7.74\n#&gt; 3             Maryland  MD         South    5773552   293  5.07\n#&gt; 4             Missouri  MO North Central    5988927   321  5.36\n#&gt; 5       South Carolina  SC         South    4625364   207  4.48\n\nNote that rows are not sorted by rate, only filtered. If we want to sort, we need to use arrange. Note that if the third argument is left blank, top_n filters by the last column.\n\n\n\n\n\n\nYou are ready to do exercises 11-19."
  },
  {
    "objectID": "R/tidyverse.html#tibbles",
    "href": "R/tidyverse.html#tibbles",
    "title": "4  The tidyverse",
    "section": "\n4.6 Tibbles",
    "text": "4.6 Tibbles\nTo work with the tidyverse data must be stored in data frames. We introduced the data frame in Section Section 2.3.1 and have been using the murders data frame throughout the book. In Section Section 4.4.3 we introduced the group_by function, which permits stratifying data before computing summary statistics. But where is the group information stored in the data frame?\n\nmurders |&gt; group_by(region)\n#&gt; # A tibble: 51 × 6\n#&gt; # Groups:   region [4]\n#&gt;   state      abb   region population total  rate\n#&gt;   &lt;chr&gt;      &lt;chr&gt; &lt;fct&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 Alabama    AL    South     4779736   135  2.82\n#&gt; 2 Alaska     AK    West       710231    19  2.68\n#&gt; 3 Arizona    AZ    West      6392017   232  3.63\n#&gt; 4 Arkansas   AR    South     2915918    93  3.19\n#&gt; 5 California CA    West     37253956  1257  3.37\n#&gt; # ℹ 46 more rows\n\nNotice that there are no columns with this information. But, if you look closely at the output above, you see the line A tibble followd by dimensions. We can learn the class of the returned object using:\n\nmurders |&gt; group_by(region) |&gt; class()\n#&gt; [1] \"grouped_df\" \"tbl_df\"     \"tbl\"        \"data.frame\"\n\nThe tbl, pronounced tibble, is a special kind of data frame. The functions group_by and summarize always return this type of data frame. The group_by function returns a special kind of tbl, the grouped_df. We will say more about these later. For consistency, the dplyr manipulation verbs (select, filter, mutate, and arrange) preserve the class of the input: if they receive a regular data frame they return a regular data frame, while if they receive a tibble they return a tibble. But tibbles are the preferred format in the tidyverse and as a result tidyverse functions that produce a data frame from scratch return a tibble. For example, in Chapter 6 we will see that tidyverse functions used to import data create tibbles.\nTibbles are very similar to data frames. In fact, you can think of them as a modern version of data frames. Nonetheless there are some important differences which we describe next.\n\nTibbles display better\n\nThe print method for tibbles is more readable than that of a data frame. To see this, compare the outputs of typing murders and the output of murders if we convert it to a tibble. We can do this using as_tibble(murders). If using RStudio, output for a tibble adjusts to your window size. To see this, change the width of your R console and notice how more/less columns are shown.\nIf you subset the columns of a data frame, you may get back an object that is not a data frame, such as a vector or scalar. For example:\n\nclass(murders[,4])\n#&gt; [1] \"numeric\"\n\nis not a data frame. With tibbles this does not happen:\n\nclass(as_tibble(murders)[,4])\n#&gt; [1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\nThis is useful in the tidyverse since functions require data frames as input.\nWith tibbles, if you want to access the vector that defines a column, and not get back a data frame, you need to use the accessor $:\n\nclass(as_tibble(murders)$population)\n#&gt; [1] \"numeric\"\n\nA related feature is that tibbles will give you a warning if you try to access a column that does not exist. If we accidentally write Population instead of population this:\n\nmurders$Population\n#&gt; NULL\n\nreturns a NULL with no warning, which can make it harder to debug. In contrast, if we try this with a tibble we get an informative warning:\n\nas_tibble(murders)$Population\n#&gt; Warning: Unknown or uninitialised column: `Population`.\n#&gt; NULL\n\n\nTibbles can have complex entries\n\nWhile data frame columns need to be vectors of numbers, strings, or logical values, tibbles can have more complex objects, such as lists or functions. Also, we can create tibbles with functions:\n\ntibble(id = c(1, 2, 3), func = c(mean, median, sd))\n#&gt; # A tibble: 3 × 2\n#&gt;      id func  \n#&gt;   &lt;dbl&gt; &lt;list&gt;\n#&gt; 1     1 &lt;fn&gt;  \n#&gt; 2     2 &lt;fn&gt;  \n#&gt; 3     3 &lt;fn&gt;\n\n\nTibbles can be grouped\n\nThe function group_by returns a special kind of tibble: a grouped tibble. This class stores information that lets you know which rows are in which groups. The tidyverse functions, in particular the summarize function, are aware of the group information.\n\n4.6.1 Creating tibbles\nIt is sometimes useful for us to create our own data frames. To create a data frame in the tibble format, you can do this by using the tibble function.\n\ngrades &lt;- tibble(names = c(\"John\", \"Juan\", \"Jean\", \"Yao\"), \n                     exam_1 = c(95, 80, 90, 85), \n                     exam_2 = c(90, 85, 85, 90))\n\nNote that base R (without packages loaded) has a function with a very similar name, data.frame, that can be used to create a regular data frame rather than a tibble.\n\ngrades &lt;- data.frame(names = c(\"John\", \"Juan\", \"Jean\", \"Yao\"), \n                     exam_1 = c(95, 80, 90, 85), \n                     exam_2 = c(90, 85, 85, 90))\n\nTo convert a regular data frame to a tibble, you can use the as_tibble function.\n\nas_tibble(grades) |&gt; class()\n#&gt; [1] \"tbl_df\"     \"tbl\"        \"data.frame\""
  },
  {
    "objectID": "R/tidyverse.html#the-placeholder",
    "href": "R/tidyverse.html#the-placeholder",
    "title": "4  The tidyverse",
    "section": "\n4.7 The placeholder",
    "text": "4.7 The placeholder\nOne of the advantages of using the pipe |&gt; is that we do not have to keep naming new objects as we manipulate the data frame. The object on the left-hand side of the pipe is used as the first argument of the function on the right-hand side of the pipe. But what if we want to pass it as argument to the right-hand side function that is not the first? The answer is the placeholder operator _ (for the %&gt;% pipe the placeholder is .). Below is a simple example that passes the base argument to the log function. The following three are equivalent:\n\nlog(8, base = 2)\n2 |&gt; log(8, base = _)\n2 %&gt;% log(8, base = .)"
  },
  {
    "objectID": "R/tidyverse.html#the-purrr-package",
    "href": "R/tidyverse.html#the-purrr-package",
    "title": "4  The tidyverse",
    "section": "\n4.8 The purrr package",
    "text": "4.8 The purrr package\nIn Section Section 3.5 we learned about the sapply function, which permitted us to apply the same function to each element of a vector. We constructed a function and used sapply to compute the sum of the first n integers for several values of n like this:\n\ncompute_s_n &lt;- function(n) {\n  sum(1:n)\n}\nn &lt;- 1:25\ns_n &lt;- sapply(n, compute_s_n)\n\nThis type of operation, applying the same function or procedure to elements of an object, is quite common in data analysis. The purrr package includes functions similar to sapply but that better interact with other tidyverse functions. The main advantage is that we can better control the output type of functions. In contrast, sapply can return several different object types; for example, we might expect a numeric result from a line of code, but sapply might convert our result to character under some circumstances. purrr functions will never do this: they will return objects of a specified type or return an error if this is not possible.\nThe first purrr function we will learn is map, which works very similar to sapply but always, without exception, returns a list:\n\nlibrary(purrr)\ns_n &lt;- map(n, compute_s_n)\nclass(s_n)\n#&gt; [1] \"list\"\n\nIf we want a numeric vector, we can instead use map_dbl which always returns a vector of numeric values.\n\ns_n &lt;- map_dbl(n, compute_s_n)\nclass(s_n)\n#&gt; [1] \"numeric\"\n\nThis produces the same results as the sapply call shown above.\nA particularly useful purrr function for interacting with the rest of the tidyverse is map_df, which always returns a tibble data frame. However, the function being called needs to return a vector or a list with names. For this reason, the following code would result in a Argument 1 must have names error:\n\ns_n &lt;- map_df(n, compute_s_n)\n\nThe function needs to return a data frame to make this work:\n\ncompute_s_n &lt;- function(n) {\n  tibble(sum = sum(1:n))\n}\ns_n &lt;- map_df(n, compute_s_n)\n\nThe purrr package provides much more functionality not covered here. For more details you can consult this online resource."
  },
  {
    "objectID": "R/tidyverse.html#tidyverse-conditionals",
    "href": "R/tidyverse.html#tidyverse-conditionals",
    "title": "4  The tidyverse",
    "section": "\n4.9 Tidyverse conditionals",
    "text": "4.9 Tidyverse conditionals\nA typical data analysis will often involve one or more conditional operations. In Section Section 3.1 we described the ifelse function, which we will use extensively in this book. In this section we present two dplyr functions that provide further functionality for performing conditional operations.\n\n4.9.1 case_when\n\nThe case_when function is useful for vectorizing conditional statements. It is similar to ifelse but can output any number of values, as opposed to just TRUE or FALSE. Here is an example splitting numbers into negative, positive, and 0:\n\nx &lt;- c(-2, -1, 0, 1, 2)\ncase_when(x &lt; 0 ~ \"Negative\", \n          x &gt; 0 ~ \"Positive\", \n          TRUE  ~ \"Zero\")\n#&gt; [1] \"Negative\" \"Negative\" \"Zero\"     \"Positive\" \"Positive\"\n\nA common use for this function is to define categorical variables based on existing variables. For example, suppose we want to compare the murder rates in four groups of states: New England, West Coast, South, and other. For each state, we need to ask if it is in New England, if it is not we ask if it is in the West Coast, if not we ask if it is in the South, and if not we assign other. Here is how we use case_when to do this:\n\nmurders |&gt; \n  mutate(group = case_when(\n    abb %in% c(\"ME\", \"NH\", \"VT\", \"MA\", \"RI\", \"CT\") ~ \"New England\",\n    abb %in% c(\"WA\", \"OR\", \"CA\") ~ \"West Coast\",\n    region == \"South\" ~ \"South\",\n    TRUE ~ \"Other\")) |&gt;\n  group_by(group) |&gt;\n  summarize(rate = sum(total)/sum(population)*10^5) \n#&gt; # A tibble: 4 × 2\n#&gt;   group        rate\n#&gt;   &lt;chr&gt;       &lt;dbl&gt;\n#&gt; 1 New England  1.72\n#&gt; 2 Other        2.71\n#&gt; 3 South        3.63\n#&gt; 4 West Coast   2.90\n\n\n4.9.2 between\n\nA common operation in data analysis is to determine if a value falls inside an interval. We can check this using conditionals. For example, to check if the elements of a vector x are between a and b we can type\n\nx &gt;= a & x &lt;= b\n\nHowever, this can become cumbersome, especially within the tidyverse approach. The between function performs the same operation.\n\nbetween(x, a, b)"
  },
  {
    "objectID": "R/tidyverse.html#exercises",
    "href": "R/tidyverse.html#exercises",
    "title": "4  The tidyverse",
    "section": "\n4.10 Exercises",
    "text": "4.10 Exercises\n1. Examine the built-in dataset co2. Which of the following is true:\n\n\nco2 is tidy data: it has one year for each row.\n\nco2 is not tidy: we need at least one column with a character vector.\n\nco2 is not tidy: it is a matrix instead of a data frame.\n\nco2 is not tidy: to be tidy we would have to wrangle it to have three columns (year, month and value), then each co2 observation would have a row.\n\n2. Examine the built-in dataset ChickWeight. Which of the following is true:\n\n\nChickWeight is not tidy: each chick has more than one row.\n\nChickWeight is tidy: each observation (a weight) is represented by one row. The chick from which this measurement came is one of the variables.\n\nChickWeight is not tidy: we are missing the year column.\n\nChickWeight is tidy: it is stored in a data frame.\n\n3. Examine the built-in dataset BOD. Which of the following is true:\n\n\nBOD is not tidy: it only has six rows.\n\nBOD is not tidy: the first column is just an index.\n\nBOD is tidy: each row is an observation with two values (time and demand)\n\nBOD is tidy: all small datasets are tidy by definition.\n\n4. Which of the following built-in datasets is tidy (you can pick more than one):\n\nBJsales\nEuStockMarkets\nDNase\nFormaldehyde\nOrange\nUCBAdmissions\n\n5. Load the dplyr package and the murders dataset.\n\nlibrary(dplyr)\nlibrary(dslabs)\n\nYou can add columns using the dplyr function mutate. This function is aware of the column names and inside the function you can call them unquoted:\n\nmurders &lt;- mutate(murders, population_in_millions = population/10^6)\n\nWe can write population rather than murders$population. The function mutate knows we are grabbing columns from murders.\nUse the function mutate to add a murders column named rate with the per 100,000 murder rate as in the example code above. Make sure you redefine murders as done in the example code above ( murders &lt;- [your code]) so we can keep using this variable.\n6. If rank(x) gives you the ranks of x from lowest to highest, rank(-x) gives you the ranks from highest to lowest. Use the function mutate to add a column rank containing the rank, from highest to lowest murder rate. Make sure you redefine murders so we can keep using this variable.\n7. With dplyr, we can use select to show only certain columns. For example, with this code we would only show the states and population sizes:\n\nselect(murders, state, population) |&gt; head()\n\nUse select to show the state names and abbreviations in murders. Do not redefine murders, just show the results.\n8. The dplyr function filter is used to choose specific rows of the data frame to keep. Unlike select which is for columns, filter is for rows. For example, you can show just the New York row like this:\n\nfilter(murders, state == \"New York\")\n\nYou can use other logical vectors to filter rows.\nUse filter to show the top 5 states with the highest murder rates. After we add murder rate and rank, do not change the murders dataset, just show the result. Remember that you can filter based on the rank column.\n9. We can remove rows using the != operator. For example, to remove Florida, we would do this:\n\nno_florida &lt;- filter(murders, state != \"Florida\")\n\nCreate a new data frame called no_south that removes states from the South region. How many states are in this category? You can use the function nrow for this.\n10. We can also use %in% to filter with dplyr. You can therefore see the data from New York and Texas like this:\n\nfilter(murders, state %in% c(\"New York\", \"Texas\"))\n\nCreate a new data frame called murders_nw with only the states from the Northeast and the West. How many states are in this category?\n11. Suppose you want to live in the Northeast or West and want the murder rate to be less than 1. We want to see the data for the states satisfying these options. Note that you can use logical operators with filter. Here is an example in which we filter to keep only small states in the Northeast region.\n\nfilter(murders, population &lt; 5000000 & region == \"Northeast\")\n\nMake sure murders has been defined with rate and rank and still has all states. Create a table called my_states that contains rows for states satisfying both the conditions: it is in the Northeast or West and the murder rate is less than 1. Use select to show only the state name, the rate, and the rank.\n12. The pipe |&gt; can be used to perform operations sequentially without having to define intermediate objects. Start by redefining murder to include rate and rank.\n\nmurders &lt;- mutate(murders, rate =  total/population*100000, \n                  rank = rank(-rate))\n\nIn the solution to the previous exercise, we did the following:\n\nmy_states &lt;- filter(murders, region %in% c(\"Northeast\", \"West\") & \n                      rate &lt; 1)\n\nselect(my_states, state, rate, rank)\n\nThe pipe |&gt; permits us to perform both operations sequentially without having to define an intermediate variable my_states. We therefore could have mutated and selected in the same line like this:\n\nmutate(murders, rate =  total/population*100000, \n       rank = rank(-rate)) |&gt;\n  select(state, rate, rank)\n\nNotice that select no longer has a data frame as the first argument. The first argument is assumed to be the result of the operation conducted right before the |&gt;.\nRepeat the previous exercise, but now instead of creating a new object, show the result and only include the state, rate, and rank columns. Use a pipe |&gt; to do this in just one line.\n\nmy_states &lt;- murders |&gt;\n  mutate SOMETHING |&gt; \n  filter SOMETHING |&gt; \n  select SOMETHING\n\n13. For exercises 13-19, we will be using the data from the survey collected by the United States National Center for Health Statistics (NCHS). This center has conducted a series of health and nutrition surveys since the 1960’s. Starting in 1999, about 5,000 individuals of all ages have been interviewed every year and they complete the health examination component of the survey. Part of the data is made available via the NHANES package. Once you install the NHANES package, you can load the data like this:\n\nlibrary(NHANES)\n\nThe NHANES data has many missing values. The mean and sd functions in R will return NA if any of the entries of the input vector is an NA. Here is an example:\n\nlibrary(dslabs)\nmean(na_example)\n#&gt; [1] NA\nsd(na_example)\n#&gt; [1] NA\n\nTo ignore the NAs we can use the na.rm argument:\n\nmean(na_example, na.rm = TRUE)\n#&gt; [1] 2.3\nsd(na_example, na.rm = TRUE)\n#&gt; [1] 1.22\n\nLet’s now explore the NHANES data.\nWe will provide some basic facts about blood pressure. First let’s select a group to set the standard. We will use 20-to-29-year-old females. AgeDecade is a categorical variable with these ages. Note that the category is coded like ” 20-29”, with a space in front! What is the average and standard deviation of systolic blood pressure as saved in the BPSysAve variable? Save it to a variable called ref.\nHint: Use filter and summarize and use the na.rm = TRUE argument when computing the average and standard deviation. You can also filter the NA values using filter.\n14. Using a pipe, assign the average to a numeric variable ref_avg. Hint: Use the code similar to above and then pull.\n15. Now report the min and max values for the same group.\n16. Compute the average and standard deviation for females, but for each age group separately rather than a selected decade as in question 1. Note that the age groups are defined by AgeDecade. Hint: rather than filtering by age and gender, filter by Gender and then use group_by.\n17. Repeat exercise 16 for males.\n19. For males between the ages of 40-49, compare systolic blood pressure across race as reported in the Race1 variable. Order the resulting table from lowest to highest average systolic blood pressure.\n20. Load the murders dataset. Which of the following is true?\n\n\nmurders is in tidy format and is stored in a tibble.\n\nmurders is in tidy format and is stored in a data frame.\n\nmurders is not in tidy format and is stored in a tibble.\n\nmurders is not in tidy format and is stored in a data frame.\n\n21. Use as_tibble to convert the murders data table into a tibble and save it in an object called murders_tibble.\n22. Use the group_by function to convert murders into a tibble that is grouped by region.\n23. Write tidyverse code that is equivalent to this code:\n\nexp(mean(log(murders$population)))\n\nWrite it using the pipe so that each function is called without arguments. Use the dot operator to access the population. Hint: The code should start with murders |&gt;.\n24. Use the map_df to create a data frame with three columns named n, s_n, and s_n_2. The first column should contain the numbers 1 through 100. The second and third columns should each contain the sum of 1 through \\(n\\) with \\(n\\) the row number."
  },
  {
    "objectID": "R/data-table.html#refining-data-tables",
    "href": "R/data-table.html#refining-data-tables",
    "title": "5  data.table",
    "section": "\n5.1 Refining data tables",
    "text": "5.1 Refining data tables\ndata.table is a separate package that needs to be installed. Once installed, we then need to load it along with the other packages we will use:\n\nlibrary(dplyr)\nlibrary(dslabs)\nlibrary(data.table)\n\nWe will provide example code showing the data.table approaches to dplyr’s mutate, filter, select, group_by, and summarize shown in Chapter Chapter 4. As in that chapter, we will use the murders dataset:\nThe first step when using data.table is to convert the data frame into a data.table object using the as.data.table function:\n\nmurders_dt &lt;- as.data.table(murders)\n\nWithout this initial step, most of the approaches shown below will not work.\n\n5.1.1 Column-wise subsetting\nSelecting with data.table is done in a similar way to subsetting matrices. While with dplyr we write\n\nselect(murders, state, region)\n\nin data.table we use\n\nmurders_dt[, c(\"state\", \"region\")] \n\nWe can also use the .() data.table notation to alert R that variables inside the parenthesis are column names, not objects in the R environment. So the above can also be written like this:\n\nmurders_dt[, .(state, region)] \n\n\n5.1.2 Adding or transformin variables\nWe learned to use the dplyr mutate function with this example:\n\nmurders &lt;- mutate(murders, rate = total / population * 100000)\n\ndata.table uses an approach that avoids a new assignment (update by reference). This can help with large datasets that take up most of your computer’s memory. The data.table :=` function permits us to do this:\n\nmurders_dt[, rate := total / population * 100000]\n\nThis adds a new column, rate, to the table. Notice that, as in dplyr, we used total and population without quotes.\nTo define new multiple columns, we can use the := function with multiple arguments:\n\nmurders_dt[, \":=\"(rate = total / population * 100000, rank = rank(population))]\n\n\n5.1.3 Reference versus copy\nThe data.table package is designed to avoid wasting memory. So if you make a copy of a table, like this:\n\nx &lt;- data.table(a = 1)\ny &lt;- x\n\ny is actually referencing x, it is not an new opject: y just another name for x. Until you change y, a new object will not be made. However, the := function changes by reference so if you change x, a new object is not made and y continues to be just another name for x:\n\nx[,a := 2]\ny\n#&gt;    a\n#&gt; 1: 2\n\nYou can also change x like this:\n\ny[,a := 1]\nx\n#&gt;    a\n#&gt; 1: 1\n\nTo avoid this, you can use the copy function which forces the creation of an actual copy:\n\nx &lt;- data.table(a = 1)\ny &lt;- copy(x)\nx[,a := 2]\ny\n#&gt;    a\n#&gt; 1: 1\n\nNote that the function as.data.table creates a copy of the data frame being converted. However, if working with a large data frames it is helpful to avoid this by using setDT:\n\nx &lt;- data.frame(a = 1)\nsetDT(x)\n\nNote that because no copy is being made the following code does not create a new object:\n\nx &lt;- data.frame(a = 1)\ny &lt;- setDT(x)\n\nThe objects x and y are referencing the same data table:\n\nx[,a := 2]\ny\n#&gt;    a\n#&gt; 1: 2\n\n\n5.1.4 Row-wise subsetting\nWith dplyr, we filtered like this:\n\nfilter(murders, rate &lt;= 0.7)\n\nWith data.table, we again use an approach similar to subsetting matrices, except data.table knows that rate refers to a column name and not an object in the R environment:\n\nmurders_dt[rate &lt;= 0.7]\n\nNotice that we can combine the filter and select into one succint command. Here are the state names and rates for those with rates below 0.7.\n\nmurders_dt[rate &lt;= 0.7, .(state, rate)]\n#&gt;            state  rate\n#&gt; 1:        Hawaii 0.515\n#&gt; 2:          Iowa 0.689\n#&gt; 3: New Hampshire 0.380\n#&gt; 4:  North Dakota 0.595\n#&gt; 5:       Vermont 0.320\n\nwhich is more compact than the dplyr approach:\n\nmurders |&gt; filter(rate &lt;= 0.7) |&gt; select(state, rate)\n\n\n\n\n\n\n\nYou are ready to do exercises 1-7."
  },
  {
    "objectID": "R/data-table.html#summarizing-data",
    "href": "R/data-table.html#summarizing-data",
    "title": "5  data.table",
    "section": "\n5.2 Summarizing data",
    "text": "5.2 Summarizing data\nAs an example, we will use the heights dataset:\n\nheights_dt &lt;- as.data.table(heights)\n\nIn data.table, we can call functions inside .() and they will be applied to rows. So the equivalent of:\n\ns &lt;- heights |&gt; summarize(avg = mean(height), sd = sd(height))\n\nin dplyr is the following in data.table:\n\ns &lt;- heights_dt[, .(avg = mean(height), sd = sd(height))]\n\nNote that this permits a compact way of subsetting and then summarizing. Instead of:\n\ns &lt;- heights |&gt; \n  filter(sex == \"Female\") |&gt;\n  summarize(avg = mean(height), sd = sd(height))\n\nwe can write:\n\ns &lt;- heights_dt[sex == \"Female\", .(avg = mean(height), sd = sd(height))]\n\n\n5.2.1 Multiple summaries\nIn Chapter 4, we defined the follwing function to permit multiple column summaries in dplyer:\n\nmedian_min_max &lt;- function(x){\n  qs &lt;- quantile(x, c(0.5, 0, 1))\n  data.frame(median = qs[1], minimum = qs[2], maximum = qs[3])\n}\n\nIn data.table we place a function call within .() to obtain the three number summary:\n\nheights_dt[, .(median_min_max(height))]\n\n\n5.2.2 Group then summarize\nThe group_by followed by summarize in dplyr is performed in one line in data.table. We simply add the by argument to split the data into groups based on the values in categorical variable:\n\nheights_dt[, .(avg = mean(height), sd = sd(height)), by = sex]\n#&gt;       sex  avg   sd\n#&gt; 1:   Male 69.3 3.61\n#&gt; 2: Female 64.9 3.76"
  },
  {
    "objectID": "R/data-table.html#sorting",
    "href": "R/data-table.html#sorting",
    "title": "5  data.table",
    "section": "\n5.3 Sorting",
    "text": "5.3 Sorting\nWe can order rows using the same approach we use for filter. Here are the states ordered by murder rate:\n\nmurders_dt[order(population)]\n\nN To sort the table in descending order, we can order by the negative of population or use the decreasing argument:\n\nmurders_dt[order(population, decreasing = TRUE)] \n\n\n5.3.1 Nested sorting\nSimilarly, we can perform nested ordering by including more than one variable in order\n\nmurders_dt[order(region, rate)]"
  },
  {
    "objectID": "R/data-table.html#exercises",
    "href": "R/data-table.html#exercises",
    "title": "5  data.table",
    "section": "\n5.4 Exercises",
    "text": "5.4 Exercises\n1. Load the data.table package and the murders dataset and convert it to data.table object:\n\nlibrary(data.table)\nlibrary(dslabs)\nmurders_dt &lt;- as.data.table(murders)\n\nRemember you can add columns like this:\n\nmurders_dt[, population_in_millions := population / 10^6]\n\nAdd a murders column named rate with the per 100,000 murder rate as in the example code above.\n2. Add a column rank containing the rank, from highest to lowest murder rate.\n3. If we want to only show the states and population sizes, we can use:\n\nmurders_dt[, .(state, population)] \n\nShow the state names and abbreviations in murders.\n4. You can show just the New York row like this:\n\nmurders_dt[state == \"New York\"]\n\nYou can use other logical vectors to filter rows.\nShow the top 5 states with the highest murder rates. After we add murder rate and rank, do not change the murders dataset, just show the result. Remember that you can filter based on the rank column.\n5. We can remove rows using the != operator. For example, to remove Florida, we would do this:\n\nno_florida &lt;- murders_dt[state != \"Florida\"]\n\nCreate a new data frame called no_south that removes states from the South region. How many states are in this category? You can use the function nrow for this.\n6. We can also use %in% to filter. You can therefore see the data from New York and Texas as follows:\n\nmurders_dt[state %in% c(\"New York\", \"Texas\")]\n\nCreate a new data frame called murders_nw with only the states from the Northeast and the West. How many states are in this category?\n7. Suppose you want to live in the Northeast or West and want the murder rate to be less than 1. We want to see the data for the states satisfying these options. Note that you can use logical operators with filter. Here is an example in which we filter to keep only small states in the Northeast region.\n\nmurders_dt[population &lt; 5000000 & region == \"Northeast\"]\n\nMake sure murders has been defined with rate and rank and still has all states. Create a table called my_states that contains rows for states satisfying both the conditions: they are in the Northeast or West and the murder rate is less than 1. Show only the state name, the rate, and the rank.\nFor exercises 8-12, we will be using the NHANES data.\n\nlibrary(NHANES)\n\n8. We will provide some basic facts about blood pressure. First let’s select a group to set the standard. We will use 20-to-29-year-old females. AgeDecade is a categorical variable with these ages. Note that the category is coded like ” 20-29”, with a space in front! Use the data.table package to compute the average and standard deviation of systolic blood pressure as saved in the BPSysAve variable. Save it to a variable called ref.\n9. Report the min and max values for the same group.\n10. Compute the average and standard deviation for females, but for each age group separately rather than a selected decade as in question 1. Note that the age groups are defined by AgeDecade.\n11. Repeat exercise 3 for males.\n12. For males between the ages of 40-49, compare systolic blood pressure across race as reported in the Race1 variable. Order the resulting table from lowest to highest average systolic blood pressure."
  },
  {
    "objectID": "R/data-table.html#footnotes",
    "href": "R/data-table.html#footnotes",
    "title": "5  data.table",
    "section": "",
    "text": "https://cran.r-project.org/web/packages/data.table/vignettes/datatable-intro.html↩︎"
  },
  {
    "objectID": "R/importing-data.html#paths-and-the-working-directory",
    "href": "R/importing-data.html#paths-and-the-working-directory",
    "title": "6  Importing data",
    "section": "\n6.1 Paths and the working directory",
    "text": "6.1 Paths and the working directory\nThe first step when importing data from a spreadsheet is to locate the file containing the data. Although we do not recommend it, you can use an approach similar to what you do to open files in Microsoft Excel by clicking on the RStudio “File” menu, clicking “Import Dataset”, then clicking through folders until you find the file. However, we rather write code rather than use the point-and-click approach. The key concepts we need to learn to do this are described in detail in Chapter 20. Here we provide an overview of the very basics.\n\n6.1.1 The filesystem\nYou can think of your computer’s filesystem as a series of nested folders, each containing other folders and files. We refer to folders as directories. We refer to the folder that contains all other folders as the root directory. We refer to the directory in which we are currently located as the working directory. The working directory therefore changes as you move through folders: think of it as your current location.\n\n6.1.2 Relative and full paths\nThe path of a file is a list of directory names that can be thought of as instructions on what folders to click on, and in what order, to find the file. If these instructions are for finding the file from the root directory we refer to it as the full path. If the instructions are for finding the file starting in the working directory we refer to it as a relative path. Section 20.3 provides more details on this topic.\nTo see an example of a full path on your system type the following:\n\nsystem.file(package = \"dslabs\")\n#&gt; [1] \"/Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/library/dslabs\"\n\nNote that the output will be different across different computers. The system.file function finds the full path to the files that were added to your system when you installed the dslabs package. The strings separated by slashes are the directory names. The first slash represents the root directory and we know this is a full path because it starts with a slash.\nWe can use the function list.files to show the names of files and directories in any directory. For example, here are the files in the dslabs package directory:\n\ndir &lt;- system.file(package = \"dslabs\")\nlist.files(dir)\n#&gt;  [1] \"data\"        \"DESCRIPTION\" \"extdata\"     \"help\"       \n#&gt;  [5] \"html\"        \"INDEX\"       \"Meta\"        \"NAMESPACE\"  \n#&gt;  [9] \"R\"           \"script\"\n\nNote that these do not start with slash which implies they are relative paths. These relative paths give us the location of the files or directories if the path stored in dir is our working directory.\n\n\n\n\n\n\nYou will not make much use of the system.file function in your day-to-day data analysis work. We introduce it in this section because it facilitates the sharing of spreadsheets that can be used to practice. The spreadsheets are in the extdata directory.\n\n\n\n\n6.1.3 The working directory\nWe highly recommend only using relative paths in your code. The reason is that full paths are unique to your computer and you want your code to be portable. If you want to know the full path of your working directory using the getwd function. If you need to change your working directory, you can use the function setwd or you can change it through RStudio by clicking on “Session”.\nWhen you start a project you want to pick a directory to store all the files related to that project and make this is your working directory when ruining your analysis. This will facilitate th because if you provide a relative path to an importing functions, it will assume you want R to search for this file in the working directory. Chapter 22 provides details on how to organize projects with RStudio.\n\n6.1.4 Generating path names\nThe file.path function combines characters to form a complete path, ensuring compatibility with the respective operating system. Linux and Mac use forward slashes /, while Windows uses backslashes \\, to separate directories. This function is useful because often you want to define paths using a variable. Here is an example that constructs the full path for a spreadsheet containing the spreadsheet that with the murders data. Here the variable dir contains the full path for the dslabs package and extdata/murders.csv is the relative path of the spreadsheet if dir is considered the working directory.\n\ndir &lt;- system.file(package = \"dslabs\")\nfile_path &lt;- file.path(dir, \"extdata/murders.csv\")\n\nYou can copy the file with full path file_path to your working directory using the function file.copy:\n\nfile.copy(file_path, \"murders.csv\")\n#&gt; [1] TRUE\n\nIf the file is copied successfully, this function will return TRUE. Note that we used the same filename for the destination file, but we can give it whatever name we want. If a file with that name already exists in your destination directory, the copy will be unsuccessful. You can change this behavior with the overwrite argument."
  },
  {
    "objectID": "R/importing-data.html#file-types",
    "href": "R/importing-data.html#file-types",
    "title": "6  Importing data",
    "section": "\n6.2 File types",
    "text": "6.2 File types\nFor most data analysis applications, files can generally be classified into two categories: text files and binary files. In this section we describe the most widely used format for both these types and the best way to identify them. In the last subsection we describe the importance of knowing the file encoding.\n\n\n\n\n\n\nFor this and the following section we assume you have copied the murders.csv file into your working directory. You can use the code at the end of the previous section to do this.\n\n\n\n\n6.2.1 Text files\nYou have already worked with text files. All your R scripts and Quarto files, for example, are text files and so are the Quarto files used to create this book. The murders.csv file mentioned above is also text files. One big advantage of these files is that we can easily “look” at them without having to purchase any kind of special software or follow complicated instructions. Any text editor can be used to examine a text file, including freely available editors such as RStudio or nano. To see this, try opening a csv file using the “Open file” RStudio tool. You should be able to see the content right on your editor.\nWhen text files are used to store a spreadsheet, line breaks are used to separate rows and a predefined character, referred to as the delimiter, is used to separate columns within a row. The most common delimiters are comma (,), semicolon (;), space (), and tab (a preset number of spaces or \\t). Slightly different approaches are used to read these files into R, so we need to know what delimiter was used. In some cases, the delimiter can be inferred from file suffix. For example, files ending in csv or tsv are expected to be comma and tab delimited, respectively. However, it is harder to infer the delimiter for files ending in txt. As a result we recommend looking at the file rather than inferring from the suffix. You can look at any number of lines from within R using the readLines function:\n\nreadLines(\"murders.csv\", n = 3)\n#&gt; [1] \"state,abb,region,population,total\"\n#&gt; [2] \"Alabama,AL,South,4779736,135\"     \n#&gt; [3] \"Alaska,AK,West,710231,19\"\n\nThis immediately reveals that the file is indeed comma delimited. It also reveals that the file has a header: the first row contains column names rather than data. This is also important to know. Most parsers assume the file starts with a header, but not all files have one.\n\n6.2.2 Binary files\nOpening image files such as jpg or png in a text editor or using readLines in R will not show comprehensible content because these are binary files. Unlike text files, which are designed for human readability and have standardized conventions, binary files can adopt numerous formats specific to their data type. While R’s readBin function can process any binary file, interpreting the output necessitates a thorough understanding of the file’s structure. This intricate topic isn’t covered in this book. Instead, we concentrate on the prevalent binary formats for spreadsheets: Microsoft Excel’s xls and xlsx.\n\n6.2.3 Encoding\nA frequent issue when importing data, whether text or binary, is incorrectly identifying the file’s encoding. At its core, a computer translates everything into sequences of 0s and 1s. ASCII is an encoding system that assigns specific numbers to characters. Using 7 bits, ASCII can represent \\(2^7 = 128\\) unique symbols, sufficient for all English keyboard characters. However, many global languages contain characters outside ASCII’s range. For instance, the é in “México” isn’t in ASCII’s catalog. To address this, broader encodings, such as Unicode, emerged. Unicode offers variations using 8, 16, or 32 bits, known as UTF-8, UTF-16, and UTF-32. RStudio typically uses UTF-8 as its default. Notably, ASCII is a subset of UTF-8, meaning that if a file is ASCII-encoded, presuming it’s UTF-8 encoded won’t cause issues. However, there other encodings, such as ISO-8859-1 (also known as Latin-1) developed for the western European languages, Big5 for Traditional Chinese, and ISO-8859-6 for Arabic.\nThe dslabs package includes a file that is not UTF-8 encoded to serve as an example. Notice the strange characters that appear you attempt to read inthe first line:\n\nfn &lt;- \"calificaciones.csv\"\nfile.copy(file.path(system.file(\"extdata\", package = \"dslabs\"), fn), fn)\n#&gt; [1] TRUE\nreadLines(fn, n = 1)\n#&gt; [1] \"\\\"nombre\\\",\\\"f.n.\\\",\\\"estampa\\\",\\\"puntuaci\\xf3n\\\"\"\n\nIn the following section, we’ll introduce several helpful import functions, some of which allow you to specify the file encoding."
  },
  {
    "objectID": "R/importing-data.html#parsers",
    "href": "R/importing-data.html#parsers",
    "title": "6  Importing data",
    "section": "\n6.3 Parsers",
    "text": "6.3 Parsers\nImporting fuctions, or parsers, are avaible from base R. However, more powerful and often faster functions are available in the readr, readxl, and data.table pacakges. In this section we review some examples. We also describe how data can be downlaoded or read directly from the internet.\n\n6.3.1 Base R\nBase R base provides several file parsers for example read.csv, read.table and read.delim. The first argument takes a full or relative path. If a relative path is provided, the parser assumes you want to search in the working directory. Therfore, to read the murders.csv file previously copied to our working directory, we can simply type:\n\ndat &lt;- read.csv(\"murders.csv\")\n\nAn often useful R-base importing function is scan, as it provides much flexibility. When reading in spreadsheets many things can go wrong. The file might have multiline headers or be missing cells. With experience you will learn how to deal with different challenges. Carefully reading the help files for the functions discussed here will be useful. With scan you can read-in each cell of a file. Here is an example:\n\nx &lt;- scan(\"murders.csv\", sep = \",\", what = \"c\")\nx[1:10]\n#&gt;  [1] \"state\"      \"abb\"        \"region\"     \"population\" \"total\"     \n#&gt;  [6] \"Alabama\"    \"AL\"         \"South\"      \"4779736\"    \"135\"\n\n\n6.3.2 readr\nThe readr package includes parsers for reading text file spreadsheets into R. readr is part of the tidyverse, but you can load it directly using:\n\nlibrary(readr)\n\nThe following functions are available to read-in spreadsheets:\n\n\n\n\n\n\n\nFunction\nFormat\nTypical suffix\n\n\n\nread_table\nwhite space separated values\ntxt\n\n\nread_csv\ncomma separated values\ncsv\n\n\nread_csv2\nsemicolon separated values\ncsv\n\n\nread_tsv\ntab delimited separated values\ntsv\n\n\nread_delim\ngeneral text file format, must define delimiter\ntxt\n\n\n\nIt also includes read_lines with similar functionality to readLines.\nWe can read in the murders.csv file using\n\ndat &lt;- read_csv(\"murders.csv\")\n#&gt; Rows: 51 Columns: 5\n#&gt; ── Column specification ────────────────────────────────────────────────\n#&gt; Delimiter: \",\"\n#&gt; chr (3): state, abb, region\n#&gt; dbl (2): population, total\n#&gt; \n#&gt; ℹ Use `spec()` to retrieve the full column specification for this data.\n#&gt; ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nNote that we receive a message letting us know what data types were used for each column. Also note that dat is a tibble, not just a data frame. We can supress this message using the argument show_col_types = FALSE.\nThe readr parse permit us to specify an encoding. It also includes a function that tries to guess the encoding:\n\nguess_encoding(\"murders.csv\")\n#&gt; # A tibble: 1 × 2\n#&gt;   encoding confidence\n#&gt;   &lt;chr&gt;         &lt;dbl&gt;\n#&gt; 1 ASCII             1\n\nThis function can help us read the file we previously noted was showing strange characters:\n\nguess_encoding(\"calificaciones.csv\")\n#&gt; # A tibble: 3 × 2\n#&gt;   encoding   confidence\n#&gt;   &lt;chr&gt;           &lt;dbl&gt;\n#&gt; 1 ISO-8859-1       0.92\n#&gt; 2 ISO-8859-2       0.72\n#&gt; 3 ISO-8859-9       0.53\n\nOnce we know the encoding we can specify it:\n\ndat &lt;- read_csv(\"calificaciones.csv\", show_col_types = FALSE,\n                locale = locale(encoding = \"ISO-8859-1\"))\n\nWe can now see that the characters in the header were read in correctly:\n\nnames(dat)\n#&gt; [1] \"nombre\"     \"f.n.\"       \"estampa\"    \"puntuación\"\n\n\n6.3.3 readxl\nThe readxl package provides functions to read-in Microsoft Excel formats.\n\nlibrary(readxl)\n\nThe main functions are:\n\n\n\n\n\n\n\nFunction\nFormat\nTypical suffix\n\n\n\nread_excel\nauto detect the format\nxls, xlsx\n\n\nread_xls\noriginal format\nxls\n\n\nread_xlsx\nnew format\nxlsx\n\n\n\nThe Microsoft Excel formats permit you to have more than one spreadsheet in one file. These are referred to as sheets. The functions listed above read the first sheet by default, but we can also read the others. The excel_sheets function gives us the names of all the sheets in an Excel file. These names can then be passed to the sheet argument in the three functions above to read sheets other than the first.\n\n6.3.4 data.table\nThe data.table package provides the fread function, a powerful and fast utility designed for reading large datasets. fread automatically detects the format of the input, whether it’s delimited text or even files compressed in formats like gzip or zip. It offers a significant speed advantage over the other parsers described here, especially for large files.\n\nlibrary(data.table)\ndat &lt;- fread(\"murders.csv\")\n\nNote fread returns a data.table object.\n\n6.3.5 Downloading files\nA common place for data to reside is on the internet. When these data are in files, we can download them and then import them or even read them directly from the web. For example, we note that because our dslabs package is on GitHub, the file we downloaded with the package has a url:\n\nurl &lt;- \"https://raw.githubusercontent.com/rafalab/dslabs/master/inst/extdata/murders.csv\"\n\nMost parsers can read these files directly:\n\ndat &lt;- read.csv(url)\n\nIf you want to have a local copy of the file, you can use the download.file function:\n\ndownload.file(url, \"murders.csv\")\n\nThis will download the file and save it on your system with the name murders.csv. You can use any name here, not necessarily murders.csv.\n\n\n\n\n\n\nThe function download.file overwrites existing files without warning.\n\n\n\nTwo functions that are sometimes useful when downloading data from the internet are tempdir and tempfile. The first creates a directory with a random name that is very likely to be unique. Similarly, tempfile creates a character string, not a file, that is likely to be a unique filename. So you can run a command like this which erases the temporary file once it imports the data:\n\ntmp_filename &lt;- tempfile()\ndownload.file(url, tmp_filename)\ndat &lt;- read_csv(tmp_filename)\nfile.remove(tmp_filename)"
  },
  {
    "objectID": "R/importing-data.html#organizing-data-with-spreadsheets",
    "href": "R/importing-data.html#organizing-data-with-spreadsheets",
    "title": "6  Importing data",
    "section": "\n6.4 Organizing data with spreadsheets",
    "text": "6.4 Organizing data with spreadsheets\nAlthough this book focuses almost exclusively on data analysis, data management is also an important part of data science operations. As explained in the introduction, we do not cover this topic. However, quite often data analysts need to collect data, or work with others collecting data, in a way that is most conveniently stored in a spreadsheet. Although filling out a spreadsheet by hand is a practice we highly discourage, we instead recommend the process be automatized as much as possible, sometimes you just have to do it. Therefore, in this section, we provide recommendations on how to organize data in a spreadsheet. Although there are R packages designed to read Microsoft Excel spreadsheets, we generally want to avoid this format. Instead, we recommend Google Sheets as a free software tool. Below we summarize the recommendations made in paper by Karl Broman and Kara Woo1. Please read the paper for important details.\n\n\nBe Consistent - Before you commence entering data, have a plan. Once you have a plan, be consistent and stick to it.\n\nChoose Good Names for Things - You want the names you pick for objects, files, and directories to be memorable, easy to spell, and descriptive. This is actually a hard balance to achieve and it does require time and thought. One important rule to follow is do not use spaces, use underscores _ or dashes instead -. Also, avoid symbols; stick to letters and numbers.\n\nWrite Dates as YYYY-MM-DD - To avoid confusion, we strongly recommend using this global ISO 8601 standard.\n\nNo Empty Cells - Fill in all cells and use some common code for missing data.\n\nPut Just One Thing in a Cell - It is better to add columns to store the extra information rather than having more than one piece of information in one cell.\n\nMake It a Rectangle - The spreadsheet should be a rectangle.\n\nCreate a Data Dictionary - If you need to explain things, such as what the columns are or what the labels used for categorical variables are, do this in a separate file.\n\nNo Calculations in the Raw Data Files - Excel permits you to perform calculations. Do not make this part of your spreadsheet. Code for calculations should be in a script.\n\nDo Not Use Font Color or Highlighting as Data - Most import functions are not able to import this information. Encode this information as a variable instead.\n\nMake Backups - Make regular backups of your data.\n\nUse Data Validation to Avoid Errors - Leverage the tools in your spreadsheet software so that the process is as error-free and repetitive-stress-injury-free as possible.\n\nSave the Data as Text Files - Save files for sharing in comma or tab delimited format."
  },
  {
    "objectID": "R/importing-data.html#exercises",
    "href": "R/importing-data.html#exercises",
    "title": "6  Importing data",
    "section": "\n6.5 Exercises",
    "text": "6.5 Exercises\n1. Use the read_csv function to read each of the files that the following code saves in the files object:\n\npath &lt;- system.file(\"extdata\", package = \"dslabs\")\nfiles &lt;- list.files(path)\nfiles\n\n2. Note that the the olive file, gives us a warning. This is because the first line of the file is missing the header for the first column.\nRead the help file for read_csv to figure out how to read in the file without reading this header. If you skip the header, you should not get this warning. Save the result to an object called dat.\n3. A problem with the previous approach is that we don’t know what the columns represent. Type:\n\nnames(dat)\n\nto see that the names are not informative.\nUse the readLines function to read in just the first line (we later learn how to extract values from the output).\n4. Pick a measurement you can take on a regular basis. For example, your daily weight or how long it takes you to run 5 miles. Keep a spreadsheet that includes the date, the hour, the measurement, and any other informative variable you think is worth keeping. Do this for 2 weeks. Then make a plot."
  },
  {
    "objectID": "R/importing-data.html#footnotes",
    "href": "R/importing-data.html#footnotes",
    "title": "6  Importing data",
    "section": "",
    "text": "https://www.tandfonline.com/doi/abs/10.1080/00031305.2017.1375989↩︎"
  },
  {
    "objectID": "dataviz/intro-dataviz.html#footnotes",
    "href": "dataviz/intro-dataviz.html#footnotes",
    "title": "Data Visualization",
    "section": "",
    "text": "http://graphics.wsj.com/infectious-diseases-and-vaccines/?mc_cid=711ddeb86e↩︎\nhttp://graphics8.nytimes.com/images/2011/02/19/nyregion/19schoolsch/19schoolsch-popup.gif↩︎\nhttps://www.nytimes.com/2011/02/19/nyregion/19schools.html↩︎\nhttps://en.wikipedia.org/wiki/John_Tukey↩︎\nhttps://www.ted.com/talks/hans_rosling_reveals_new_insights_on_poverty?language=en↩︎\nhttps://www.ted.com/talks/hans_rosling_shows_the_best_stats_you_ve_ever_seen↩︎\nhttps://shiny.rstudio.com/↩︎\nhttps://d3js.org/↩︎"
  },
  {
    "objectID": "dataviz/distributions.html#variable-types",
    "href": "dataviz/distributions.html#variable-types",
    "title": "7  Visualizing data distributions",
    "section": "\n7.1 Variable types",
    "text": "7.1 Variable types\nThe two main variables types are categorical and numeric. Each can be divided into two other groups: categorical can be ordinal or not, whereas numerical variables can be discrete or continuous. When each entry in a dataset comes from one of a small number of groups, we refer to the data as categorical data. Two simple examples are sex (male or female) and US regions (Northeast, South, North Central, West). Some categorical data can be ordered even if they are not numbers per se, such as spiciness (mild, medium, hot). In statistics textbooks, ordered categorical data are referred to as ordinal data. Examples of numerical data are population sizes, murder rates, and heights. Some numerical data can be treated as ordered categorical. We can further divide numerical data into continuous and discrete. Continuous variables are those that can take any value, such as heights, if measured with enough precision. For example, a pair of twins may be 68.12 and 68.11 inches, respectively. Counts, such as population sizes, are discrete because they have to be round numbers.\nKeep in mind that discrete numeric data can be considered ordinal. Although this is technically true, we usually reserve the term ordinal data for variables belonging to a small number of different groups, with each group having many members. In contrast, when we have many groups with few cases in each group, we typically refer to them as discrete numerical variables. So, for example, the number of packs of cigarettes a person smokes a day, rounded to the closest pack, would be considered ordinal, while the actual number of cigarettes would be considered a numerical variable. But, indeed, there are examples that can be considered both numerical and ordinal when it comes to visualizing data.\nHere we focus on numeric variables because visualizing this data type is substantially more complex. However, we start by describing data visualization and summarization approaches for categorical data."
  },
  {
    "objectID": "dataviz/distributions.html#case-study-describing-student-heights",
    "href": "dataviz/distributions.html#case-study-describing-student-heights",
    "title": "7  Visualizing data distributions",
    "section": "\n7.2 Case study: describing student heights",
    "text": "7.2 Case study: describing student heights\nWe introduce a new motivating problem. It is an artificial one, but it will help us illustrate the concepts needed to understand distributions.\nPretend that we have to describe the heights of our classmates to ET, an extraterrestrial that has never seen humans. As a first step, we need to collect data. To do this, we ask students to report their heights in inches. We ask them to provide sex information because we know there are two different distributions by sex. We collect the data and save it in the heights data frame:\n\nlibrary(tidyverse)\nlibrary(dslabs)\nhead(heights)\n#&gt;      sex height\n#&gt; 1   Male     75\n#&gt; 2   Male     70\n#&gt; 3   Male     68\n#&gt; 4   Male     74\n#&gt; 5   Male     61\n#&gt; 6 Female     65\n\nOne way to convey the heights to ET is to simply send him this list of 1,050 heights. But there are much more effective ways to convey this information, and understanding the concept of a distribution will be key. To simplify the explanation, we first focus on male heights. We examine the female height data in Section 7.6."
  },
  {
    "objectID": "dataviz/distributions.html#distributions",
    "href": "dataviz/distributions.html#distributions",
    "title": "7  Visualizing data distributions",
    "section": "\n7.3 Distributions",
    "text": "7.3 Distributions\nThe most basic statistical summary of a list of objects or numbers is its distribution. The simplest way to think of a distribution is as a compact description of a list with many entries. This concept should not be new for readers of this book. For example, with categorical data, the distribution simply describes the proportion of each unique category. The sex represented in the heights dataset is:\n\n#&gt; \n#&gt; Female   Male \n#&gt;  0.227  0.773\n\nThis two-category frequency table is the simplest form of a distribution. We don’t really need to visualize it since one number describes everything we need to know: 23% are females and the rest are males. When there are more categories, then a simple barplot describes the distribution. Here is an example with US state regions:\n\n\n\n\n\n\n\n\nThis particular plot simply shows us four numbers, one for each category. We usually use barplots to display a few numbers. Although this particular plot does not provide much more insight than a frequency table itself, it is a first example of how we convert a vector into a plot that succinctly summarizes all the information in the vector. When the data is numerical, the task of displaying distributions is more challenging.\n\n7.3.1 Histograms\nNumerical data that are not categorical also have distributions. However, in general, when data is not categorical, reporting the frequency of each entry, as we did for categorical data, is not an effective summary since most entries are unique. For example, in our case study, while several students reported a height of 68 inches, only one student reported a height of 68.503937007874 inches and only one student reported a height 68.8976377952756 inches. We assume that they converted from 174 and 175 centimeters, respectively.\nStatistics textbooks teach us that a more useful way to define a distribution for numeric data is to define a function that reports the proportion of the data below \\(a\\) for all possible values of \\(a\\). This function is called the emipirical cumulative distribution function (eCDF), it can be plotted, and it provides a full description of the distribution of our data. Here is the eCDF for male student heights:\n\n\n\n\n\n\n\n\nHowever, summarizing data by plotting the eCDF is actually not very popular in practice. The main reason is that it does not easily convey characteristics of interest such as: at what value is the distribution centered? Is the distribution symmetric? What ranges contain 95% of the values?\nHistograms, are much preferred because they greatly facilitate answering such questions. Histograms sacrifice just a bit of information to produce plots that are much easier to interpret. The simplest way to make a histogram is to divide the span of our data into non-overlapping bins of the same size. Then, for each bin, we count the number of values that fall in that interval. The histogram plots these counts as bars with the base of the bar defined by the intervals. Here is the histogram for the height data splitting the range of values into one inch intervals: \\((49.5, 50.5],(50.5, 51.5],(51.5,52.5],(52.5,53.5],...,(82.5,83.5]\\)\n\n\n\n\n\n\n\n\nAs you can see in the figure above, a histogram is similar to a barplot, but it differs in that the x-axis is numerical, not categorical.\nIf we send this plot to ET, he will immediately learn some important properties about our data. First, the range of the data is from 50 to 84 with the majority (more than 95%) between 63 and 75 inches. Second, the heights are close to symmetric around 69 inches. Also, by adding up counts, ET could obtain a very good approximation of the proportion of the data in any interval. Therefore, the histogram above is not only easy to interpret, but also provides almost all the information contained in the raw list of 812 male heights with about 30 bin counts.\nWhat information do we lose? Note that all values in each interval are treated the same when computing bin heights. So, for example, the histogram does not distinguish between 64, 64.1, and 64.2 inches. Given that these differences are almost unnoticeable to the eye, the practical implications are negligible and we were able to summarize the data to just 23 numbers.\nWe discuss how to code histograms in Section Section 8.14.\n\n7.3.2 Smoothed density\nSmooth density plots relay the same information as a histogram but are aesthetically more appealing. Here is what a smooth density plot looks like for our heights data:\n\n\n\n\n\n\n\n\nIn this plot, we no longer have sharp edges at the interval boundaries and many of the local peaks have been removed. Also, the scale of the y-axis changed from counts to density. To fully understand smooth densities, we have to understand estimates, a topic covered in statistics our advanced data science textbooks. Here we simply describe them as making the histograms prettier by drawing a curve that goes through the top of the histogream bars and then removing the bars. The values shown y-axis are chosen so that the area under the curve at up to 1. This implies that for any interval, the area under the curve for that interval gives us an approximation of how what proportion of the data is in the interval.\nAn advantage of smooth densities over histograms for visualization purposes is that densities make it easier to compare two distributions. This is in large part because the jagged edges of the histogram add clutter. Here is an example comparing male and female heights:\n\n\n\n\n\n\n\n\nWith the right argument, ggplot automatically shades the intersecting region with a different color. We will show examples of ggplot2 code for densities in Section Chapter 10) as well as Section Section 8.14.\n\n7.3.3 The normal distribution\nHistograms and density plots provide excellent summaries of a distribution. But can we summarize even further? We often see the average and standard deviation used as summary statistics: a two-number summary! To understand what these summaries are and why they are so widely used, we need to understand the normal distribution.\nThe normal distribution, also known as the bell curve and as the Gaussian distribution. Here is what the normal distribution looks like:\n\n\n\n\n\n\n\n\nThe normal distribution is one of the most famous mathematical concepts in history. A reason for this is that the distribution of many datasets can be approximated with normal distributions. These include gambling winnings, heights, weights, blood pressure, standardized test scores, and experimental measurement errors. Statistical textbooks offer explanations for why this is the case. But how can the same distribution approximate datasets with completely different ranges for values, for example heights and weights? A second important characteristic of the normal distribution is that it can be adapted to different datasets by just adjusting two numbers, refered to as the average or mean and the standard deviation (SD). The normal distribution is symmetric, centered at what we refer to as the average, and most values (about 95%) are within 2 SDs from the average. The plot above shows a normal distribution with average 0 and SD 1, often referred to as a standard normal. Note that the fact that vecause only two numbers are needed to adapt the normal distribution to a dataset, implies that if our data distribution is approximated by a normal distribution, all the information needed to describe the distribution can be encoded in just two numbers. We now define these values for an arbitrary list of numbers.\nOnce we are convinced that our data, say it is stored in the vector x, has a distribution that is approximately normal we can find the specifc one that matches our data by matching the average and SD of the data to the average and SD of the normal distribution, respectively. For a list of numbers contained in a vector x:\n\nindex &lt;- heights$sex == \"Male\"\nx &lt;- heights$height[index]\n\nthe average is defined as\n\nm &lt;- sum(x) / length(x)\n\nand the SD is defined as\n\ns &lt;- sqrt(sum((x - mu)^2) / length(x))\n\nwhich can be interpreted as the average distance between values and their average.\nThe pre-built functions mean and sd (note that, for reasons explained in statistics textbooks,sd divides by length(x)-1 rather than length(x)) can be used here:\n\nm &lt;- mean(x)\ns &lt;- sd(x)\nc(average = m, sd = s)\n#&gt; average      sd \n#&gt;   69.31    3.61\n\nHere is a plot of the smooth density and the normal distribution with mean = 69.3 and SD = 3.6 plotted as a black line with our student height smooth density in blue:"
  },
  {
    "objectID": "dataviz/distributions.html#boxplots",
    "href": "dataviz/distributions.html#boxplots",
    "title": "7  Visualizing data distributions",
    "section": "\n7.4 Boxplots",
    "text": "7.4 Boxplots\nTo understand boxplots we need to define some terms that are commonly used in exploratory data analysis.\nThe percentiles are the values for which \\(p = 0.01, 0.02, ..., 0.99\\) of the data are less then or equal to that value, respectively. We call, for example, the case of \\(p=0.10\\) the 10th percentile, which gives us a number for which 10% of the data is below. The most famous percentile is the 50th, also known as the median. Another special case that receives a name are the quartiles, which are obtained when setting \\(p=0.25,0.50\\), and \\(0.75\\), which are used by the boxplot.\nTo motivate boxplots we will go back to the US murder data. Suppose we want to summarize the murder rate distribution. Using the data visualization technique we have learned, we can quickly see that the normal approximation does not apply here:\n\n\n\n\n\n\n\n\nIn this case, the histogram above or a smooth density plot would serve as a relatively succinct summary.\nNow suppose those used to receiving just two numbers as summaries ask us for a more compact numerical summary.\nThe boxplot provides a five-number summary composed of the range along with the quartiles (the 25th, 50th, and 75th percentiles). The R implementation of boxplots ignore outliers when computing the range and instead plot these as independent points. The help file provides a specific definition of outliers. The boxplot shows these numbers as a “box” with “whiskers”\n\n\n\n\n\n\n\n\nwith the box defined by the 25% and 75% percentile and the whiskers showing the range. The distance between these two is called the interquartile range. The two points are outliers according to the R implementation. The median is shown with a horizontal line.\nFrom just this simple plot, we know that the median is about 2.5, that the distribution is not symmetric, and that the range is 0 to 5 for the great majority of states with two exceptions.\nWe discuss how to make boxplots in Section Section 8.14."
  },
  {
    "objectID": "dataviz/distributions.html#sec-dataviz-stratification",
    "href": "dataviz/distributions.html#sec-dataviz-stratification",
    "title": "7  Visualizing data distributions",
    "section": "\n7.5 Stratification",
    "text": "7.5 Stratification\nIn data analysis we often divide observations into groups based on the values of one or more variables associated with those observations. For example in the next section we divide the height values into groups based on a sex variable: females and males. We call this procedure stratification and refer to the resulting groups as strata.\nStratification is common in data visualization because we are often interested in how the distribution of variables differs across different subgroups. We will see several examples throughout this part of the book, starting with the next section."
  },
  {
    "objectID": "dataviz/distributions.html#sec-student-height-cont",
    "href": "dataviz/distributions.html#sec-student-height-cont",
    "title": "7  Visualizing data distributions",
    "section": "\n7.6 Case study: describing student heights (continued)",
    "text": "7.6 Case study: describing student heights (continued)\nIf we are convinved that the male height data is well approximated with a normal distribution we can report back to ET a very succinct summary: male heights follow a normal distribution with an average of 69.3 inches and a SD of 3.6 inches. With this information, ET will have a good idea of what to expect when he meets our male students. However, to provide a complete picture we need to also provide a summary of the female heights.\nBoxplots are useful when we want to quickly compare two or more distributions. Here are the heights for men and women:\n\n\n\n\n\n\n\n\nThe plot immediately reveals that males are, on average, taller than females. The standard deviations appear to be similar. But does the normal approximation also work for the female height data collected by the survey? We expect that they will follow a normal distribution, just like males. However, exploratory plots reveal that the approximation is not as useful:\n\n\n\n\n\n\n\n\nWe see something we did not see for the males: the density plot has a second “bump”. Also, the highest points tend to be taller than expected by the normal than expected heights for a normal distribution. When reporting back to ET, we might need to provide a histogram rather than just the average and standard deviation for the female heights.\nHowever, go back and read Tukey’s quote. We have noticed what we didn’t expect to see. If we look at other female height distributions, we do find that they are well approximated with a normal distribution. So why are our female students different? Is our class a requirement for the female basketball team? Are small proportions of females claiming to be taller than they are? Another, perhaps more likely, explanation is that in the form students used to enter their heights, FEMALE was the default sex and some males entered their heights, but forgot to change the sex variable. In any case, data visualization has helped discover a potential flaw in our data.\nRegarding the five smallest values, note that these values are:\n\nheights |&gt; filter(sex == \"Female\") |&gt; \n  top_n(5, desc(height)) |&gt;\n  pull(height)\n#&gt; [1] 51 53 55 52 52\n\nBecause these are reported heights, a possibility is that the student meant to enter 5'1\", 5'2\", 5'3\" or 5'5\"."
  },
  {
    "objectID": "dataviz/distributions.html#exercises",
    "href": "dataviz/distributions.html#exercises",
    "title": "7  Visualizing data distributions",
    "section": "\n7.7 Exercises",
    "text": "7.7 Exercises\n1. Define variables containing the heights of males and females like this:\n\nlibrary(dslabs)\nmale &lt;- heights$height[heights$sex == \"Male\"]\nfemale &lt;- heights$height[heights$sex == \"Female\"]\n\nHow many measurements do we have for each?\n2. Suppose we can’t make a plot and want to compare the distributions side by side. We can’t just list all the numbers. Instead, we will look at the percentiles. Create a five row table showing female_percentiles and male_percentiles with the 10th, 30th, 50th, 70th, & 90th percentiles for each sex. Then create a data frame with these two as columns.\n3. Study the following boxplots showing population sizes by country:\n\n\n\n\n\n\n\n\nWhich continent has the country with the biggest population size?\n4. What continent has the largest median population size?\n5. What is median population size for Africa to the nearest million?\n6. What proportion of countries in Europe have populations below 14 million?\n\n0.99\n0.75\n0.50\n0.25\n\n7. If we use a log transformation, which continent shown above has the largest interquartile range?\n8. Load the height data set and create a vector x with just the male heights:\n\nlibrary(dslabs)\nx &lt;- heights$height[heights$sex==\"Male\"]\n\nWhat proportion of the data is between 69 and 72 inches (taller than 69, but shorter or equal to 72)? Hint: use a logical operator and mean."
  },
  {
    "objectID": "dataviz/ggplot2.html#the-components-of-a-graph",
    "href": "dataviz/ggplot2.html#the-components-of-a-graph",
    "title": "8  ggplot2",
    "section": "\n8.1 The components of a graph",
    "text": "8.1 The components of a graph\nWe will construct a graph that summarizes the US murders dataset that looks like this:\n\n\n\n\n\n\n\n\nWe can clearly see how much states vary across population size and the total number of murders. Not surprisingly, we also see a clear relationship between murder totals and population size. A state falling on the dashed grey line has the same murder rate as the US average. The four geographic regions are denoted with color, which depicts how most southern states have murder rates above the average.\nThis data visualization shows us pretty much all the information in the data table. The code needed to make this plot is relatively simple. We will learn to create the plot part by part.\nThe first step in learning ggplot2 is to be able to break a graph apart into components. Let’s break down the plot above and introduce some of the ggplot2 terminology. The main three components to note are:\n\nData: The US murders data table is being summarized. We refer to this as the data component.\nGeometry: The plot above is a scatterplot. This is referred to as the geometry component. Other possible geometries are barplot, histogram, smooth densities, qqplot, and boxplot. We will learn more about these in Section 8.14.\nAesthetic mapping: The plot uses several visual cues to represent the information provided by the dataset. The two most important cues in this plot are the point positions on the x-axis and y-axis, which represent population size and the total number of murders, respectively. Each point represents a different observation, and we map data about these observations to visual cues like x- and y-scale. Color is another visual cue that we map to region. We refer to this as the aesthetic mapping component. How we define the mapping depends on what geometry we are using.\n\nWe also note that:\n\nThe points are labeled with the state abbreviations.\nThe range of the x-axis and y-axis appears to be defined by the range of the data. They are both on log-scales.\nThere are labels, a title, a legend, and we use the style of The Economist magazine.\n\nThe general approach in ggplot2 is to construct the plot part by part by adding layers to a ggplot object, created by the ggplot function. Layers can define geometries, compute summary statistics, define what scales to use, or change styles. To add layers, we use the symbol +. In general, a line of code will look like this:\n\n\nDATA |&gt; ggplot() + LAYER 1 + LAYER 2 + … + LAYER N\n\n\nWe will now illustrate the basics of ggplot2 by dissecting how we construct the plot above."
  },
  {
    "objectID": "dataviz/ggplot2.html#initializing-an-object-with-data",
    "href": "dataviz/ggplot2.html#initializing-an-object-with-data",
    "title": "8  ggplot2",
    "section": "\n8.2 Initializing an object with data",
    "text": "8.2 Initializing an object with data\nWe start by loading the relevant dataset which is in the dslabs package:\n\nlibrary(dslabs)\n\nThe first step in creating a ggplot2 graph is to define a ggplot object. Typically most or all the layers will be mapping variables from the same dataset, so we associate this object with the relevant data frame\n\nggplot(data = murders)\n\nor equivalently\n\nmurders |&gt; ggplot()\n\n\n\n\n\n\n\nBoth these lines of code render a plot, in this case a blank slate since no geometry has been defined. The only style choice we see is a grey background, the default. We see a plot because an object was created and not assigned to a variable, it was automatically evaluated and printed. But we can assign our plot to an object in the usual way:\n\np &lt;- ggplot(data = murders)\n\nTo render the plot associated with this object, we simply print the object p. The following two lines of code each produce the same plot we see above:\n\nprint(p)\np\n\nTo summarize, p is a ggplot object with the murders data frame as its data component."
  },
  {
    "objectID": "dataviz/ggplot2.html#adding-a-geometry",
    "href": "dataviz/ggplot2.html#adding-a-geometry",
    "title": "8  ggplot2",
    "section": "\n8.3 Adding a geometry",
    "text": "8.3 Adding a geometry\nA common frist step is to let ggplot2 know what geometry to use. We often add multiple geometries, but we at least need one. In our example, we want to make a scatterplot. Geometries are added using functions. Taking a quick look at the cheat sheet, we see that we should use the function geom_point.\n\n\n\n\n\n\n\n\n\n\n(Image courtesy of RStudio3. CC-BY-4.0 license4.)\nNote that geometry function names follow the pattern: geom_X where X is the name of the geometry. Some examples include geom_histogram, geom_boxplot, and geom_col. We will discuss these further in Section 8.14.\nFor geom_point to run properly we need to provide data and a mapping. We have already assigned with the murders data table to the object p. Next we need to add the layer geom_point to define the geometry. To find out what mappings are expected by this function, we read the Aesthetics section of the help file:\n&gt; Aesthetics\n&gt; \n&gt; geom_point understands the following aesthetics (required aesthetics are in bold):\n&gt;\n&gt; x\n&gt;\n&gt; y\n&gt; \n&gt; alpha\n&gt;\n&gt; colour\nWe see that at least two arguments are required x and y. Next explain how to map values from the dataset to the plot."
  },
  {
    "objectID": "dataviz/ggplot2.html#aesthetic-mappings",
    "href": "dataviz/ggplot2.html#aesthetic-mappings",
    "title": "8  ggplot2",
    "section": "\n8.4 Aesthetic mappings",
    "text": "8.4 Aesthetic mappings\nAesthetic mappings describe how properties of the data connect with features of the graph, such as distance along an axis, size, or color. The aes function connects data with what we see on the graph by defining aesthetic mappings and will be one of the functions you use most often when plotting. This example produces a scatterplot of total murders versus population in millions:\n\nmurders |&gt; ggplot() + geom_point(aes(population/10^6, total))\n\n\n\n\n\n\n\nWe didn’t use the x and y to define the arguments because the help file showed these are the first and second expected arguments.\nThe scale and labels are defined by default when adding this layer. Like dplyr functions, aes also uses the variable names from the data component: we can use population and total without having to call them as murders$population and murders$total. The behavior of recognizing the variables from the data component is specific to aes. With ggplot2 functionsother thanaes, if you try to access the values of population or total outside of aes you receive an error."
  },
  {
    "objectID": "dataviz/ggplot2.html#other-layers",
    "href": "dataviz/ggplot2.html#other-layers",
    "title": "8  ggplot2",
    "section": "\n8.5 Other layers",
    "text": "8.5 Other layers\nTo shape the plot into its final form, we continue to add layers. A second layer in the plot we wish to make involves adding a label to each point to identify the state. The geom_label and geom_text functions permit us to add text to the plot with and without a rectangle behind the text, respectively.\nBecause each point (each state in this case) has a label, we need an aesthetic mapping to make the connection between points and labels. By reading the help file, we learn that we supply the mapping between point and label through the label argument of aes. So the code looks like this:\n\nmurders |&gt; ggplot() + \n  geom_point(aes(population/10^6, total)) +\n  geom_text(aes(population/10^6, total, label = abb))\n\n\n\n\n\n\n\nAs an example of the unique behavior of aes related to variable names, note that if the added layer was geom_text(aes(population/10^6, total), label = abb), we would result in an error since abb is now outside the call to aes and it is not object in out workspace, it is a variable name in the data component of the ggplot object."
  },
  {
    "objectID": "dataviz/ggplot2.html#global-aesthetic-mappings",
    "href": "dataviz/ggplot2.html#global-aesthetic-mappings",
    "title": "8  ggplot2",
    "section": "\n8.6 Global aesthetic mappings",
    "text": "8.6 Global aesthetic mappings\nIn the previous lines of code, we define the mapping aes(population/10^6, total) twice, once in each layer. We can avoid this by using a global aesthetic mapping. We can do this when we define the blank slate ggplot object. Remember that the mapping argument of ggplot function permits us to define aesthetic mappings. If we define a mapping in ggplot, all the geometries that are added as layers will default to this mapping. So we can simply write the following code to produce the previous plot:\n\nmurders |&gt; ggplot(aes(population/10^6, total)) +\n  geom_point() +\n  geom_text(aes(label = abb))\n\nNote that the mapping for label is only defined in geom_text because geom_point does not use this argument.\nIf necessary, we can override the global mapping by defining a new mapping within each layer. These local definitions override the global. Here is an example:\n\nmurders |&gt; ggplot(aes(population/10^6, total)) +\n  geom_point() +\n  geom_text(aes(x = 10, y = 800, label = \"Hello there!\"))\n\n\n\n\n\n\n\nClearly, the second call to geom_text does not use population and total."
  },
  {
    "objectID": "dataviz/ggplot2.html#non-aesthetic-arguments",
    "href": "dataviz/ggplot2.html#non-aesthetic-arguments",
    "title": "8  ggplot2",
    "section": "\n8.7 Non-aesthetic arguments",
    "text": "8.7 Non-aesthetic arguments\nEach geometry function has arguments other than aes and data. They tend to be specific to the function and are not mapped to variables in the data. For example, in the plot we wish to make, the points are larger than the default size. As another example, to avoid putting the text on top of the point, we can use the nudge_x argument in geom_text. The code, with the arguments, looks like this:\n\nmurders |&gt; ggplot(aes(population/10^6, total)) +\n  geom_point(size = 3) +\n  geom_text(aes(label = abb), nudge_x = 1.5)\n\n\n\n\n\n\n\nIn Section 8.12 we learn a better way of assuring we can see the points and the labels."
  },
  {
    "objectID": "dataviz/ggplot2.html#categories-as-colors",
    "href": "dataviz/ggplot2.html#categories-as-colors",
    "title": "8  ggplot2",
    "section": "\n8.8 Categories as colors",
    "text": "8.8 Categories as colors\nFor the final plot, we want each region to have a different color. Because this information comes from the data, it is a aesthetic mapping. For our example, we can map color to categories using the col mapping in the geom_point function as follows:\n\nmurders |&gt; ggplot(aes(population/10^6, total)) +\n  geom_point(aes(color = region), size = 3) \n\n\n\n\n\n\n\nNote the geom_point automatically assigns a different color to each category and also adds a legend! Legends are usually desired, but to avoid adding a legend we can set the geom_point argument show.legend = FALSE.\nNote that color is also a non-aesthetic argument in several ggplot2 functions, including geom_point. This argument is not used to map colors to categories, but to change the color of all the points. For example, if we wanted all the points to be blue we would change the layer to geom_point(col = \"blue\", size = 3).\n\nmurders |&gt; ggplot(aes(population/10^6, total)) +\n  geom_point(color = \"blue\", size = 3)"
  },
  {
    "objectID": "dataviz/ggplot2.html#updating-ggplot-objects",
    "href": "dataviz/ggplot2.html#updating-ggplot-objects",
    "title": "8  ggplot2",
    "section": "\n8.9 Updating ggplot objects",
    "text": "8.9 Updating ggplot objects\nIn ggplot2 we build plots by parts. A useful feature of the package is that we can update existing ggplot objects by adding layers. For example, we can start by initializing an object with a dataset and an global aesthetic\n\np0 &lt;- murders |&gt; ggplot(aes(population/10^6, total))\n\nand then start adding layers. For example, we start by adding the scatter plot\n\np1 &lt;- p0 +  geom_point(aes(color = region), size = 3)\n\nand labels:\n\np2 &lt;- p1 + geom_text(aes(label = abb), nudge_x = 0.1)\n\nIn the next few sections, we will be building on objects created in previous sections using this approach. This facilitates improving plots as well as testing options. Note that we changed the nudge_x from 1.5 to 0.1 because in the next section we will apply a log transformation and a smaller value is more appropriate."
  },
  {
    "objectID": "dataviz/ggplot2.html#scales",
    "href": "dataviz/ggplot2.html#scales",
    "title": "8  ggplot2",
    "section": "\n8.10 Scales",
    "text": "8.10 Scales\nOne of the strengths of ggplot2 is that the default behavior often is good enough to achieve our visualization goals. However, it also offers ways in which we can change these defaults. Many of these are changed through the scales functions.\nTwo examples, are the scale_x_continuous and scale_y_continuous functions which let’s us make adjustments to the x-axis and y-axis, respectively. In the final plot we are trying to produce scales in log-scale and this can be achieved by assigning the argument trans = \"log10\" in these functions. However, because this operation is so common, ggplot2 includes scale_x_log10 and scale_y_log10 functions. We can achieve the desired transformation by adding these layers:\n\np3 &lt;- p2 + scale_x_log10() + scale_y_log10() \np3\n\n\n\n\n\n\n\nBe aware that ggplot2 offers immense flexibility, particularly through the scales functions. We’ve introduced just one of the many available. In subsequent chapters of this book, we’ll provide examples as they become pertinent to our visualizations. However, to familiarize yourself with these functions, we recommend consulting the ggplot2 cheat sheet or conducting internet searches as specific needs arise."
  },
  {
    "objectID": "dataviz/ggplot2.html#annotations",
    "href": "dataviz/ggplot2.html#annotations",
    "title": "8  ggplot2",
    "section": "\n8.11 Annotations",
    "text": "8.11 Annotations\nWe often want to add annotations to figures that are not derived directly from the aesthetic mapping. Examples of annotation functions are labs, annotate, and geom_abline. The labs function permits adding a title, subtitle, caption, and other labels. Note these can also be defined individually using the functions such as xlab, ylab and ggtitle.\nThe labs function also allows another change needed for our desired plot: changing the legend title. Because the legend for the color mapping, this is achieved with the color = \"NEW_TITLE\" argument:\n\np4 &lt;- p3 + labs(title = \"US Gun Murders in 2010\",\n                x = \"Populations in millions (log scale)\", \n                y = \"Total number of murders (log scale)\",\n                color = \"Region\")\np4\n\n\n\n\n\n\n\nOur desired final plot includes a line that represents the average murder rate for the entire country. Once we determine the per million rate to be \\(r\\), the desired line is defined by the formula: \\(y = r x\\), with \\(y\\) and \\(x\\) our axes: total murders and population in millions, respectively. In the log-scale this line turns into: \\(\\log(y) = \\log(r) + \\log(x)\\), a line with slope 1 and intercept \\(\\log(r)\\). We can compute r using:\n\nr &lt;- murders |&gt; summarize(rate = sum(total)/sum(population)*10^6) |&gt; pull(rate)\n\nTo add a line we use the geom_abline function. The ab in the name reminds us we are supplying the intercept (a) and slope (b). The default line has slope 1 and intercept 0 so we only have to define the intercept. Note that he final plot has a dashed line type and is grey and these can be changed throught the lty (line type) and color non aesthetic arguments. We add the layer like this:\n\np5 &lt;- p4 + geom_abline(intercept = log10(r), lty = 2, color = \"darkgrey\") \np5\n\n\n\n\n\n\n\nNote that geom_abline does not use any mappings from the data object, once we have the slope.\nWe are almost there! All we have to do is add line and optional changes to the style."
  },
  {
    "objectID": "dataviz/ggplot2.html#sec-add-on-packages",
    "href": "dataviz/ggplot2.html#sec-add-on-packages",
    "title": "8  ggplot2",
    "section": "\n8.12 Add-on packages",
    "text": "8.12 Add-on packages\nThe power of ggplot2 is augmented further due to the availability of add-on packages. The remaining changes needed to put the finishing touches on our plot require the ggthemes and ggrepel packages.\nThe style of a ggplot2 graph can be changed using the theme functions. Several themes are included as part of the ggplot2 package. In fact, for most of the plots in this book, we use a function in the dslabs package that automatically sets a default theme:\n\nds_theme_set()\n\nMany other themes are added by the package ggthemes. Among those are the theme_economist theme that we used. After installing the package, you can change the style by adding a layer like this:\n\nlibrary(ggthemes)\np6 &lt;- p5 + theme_economist()\n\nYou can see how some of the other themes look by simply changing the function. For instance, you might try the theme_fivethirtyeight() theme instead.\nThe final chage is to better position of the labels to avoid crowding: currently. some of the labels fall on top of each other. The add-on package ggrepel includes a geometry that adds labels while ensuring that they don’t fall on top of each other. We simply change geom_text with geom_text_repel."
  },
  {
    "objectID": "dataviz/ggplot2.html#sec-putting-it-all-together",
    "href": "dataviz/ggplot2.html#sec-putting-it-all-together",
    "title": "8  ggplot2",
    "section": "\n8.13 Putting it all together",
    "text": "8.13 Putting it all together\nNow that we are done testing, we can write one line of code that produces our desired plot from scratch.\n\nlibrary(ggthemes)\nlibrary(ggrepel)\n\nr &lt;- murders |&gt; \n  summarize(rate = sum(total) /  sum(population) * 10^6) |&gt;\n  pull(rate)\n\nmurders |&gt; \n  ggplot(aes(population/10^6, total)) +   \n  geom_abline(intercept = log10(r), lty = 2, color = \"darkgrey\") +\n  geom_point(aes(col = region), size = 3) +\n  geom_text_repel(aes(label = abb)) + \n  scale_x_log10() +\n  scale_y_log10() +\n  labs(title = \"US Gun Murders in 2010\",\n                x = \"Populations in millions (log scale)\", \n                y = \"Total number of murders (log scale)\",\n                color = \"Region\") +\n  theme_economist()"
  },
  {
    "objectID": "dataviz/ggplot2.html#sec-other-geometries",
    "href": "dataviz/ggplot2.html#sec-other-geometries",
    "title": "8  ggplot2",
    "section": "\n8.14 Geometries",
    "text": "8.14 Geometries\nIn our illustrative example we introduced the scatterplot geometry geom_point. However, ggplot2 has many others and here we demonstrate how to generate plots related to distributions, specifically the plots shown Chapter 7.\n\n8.14.1 Barplots\nTo generate a barplot we can use the geom_bar geometry. The default is to count the number of each category and draw a bar. Here is the plot for the regions of the US.\n\nmurders |&gt; ggplot(aes(region)) + geom_bar()\n\nHowever, we often already have a table with the numbers we want to present as a barplot. Here is an example of such a table:\n\ntab &lt;- murders |&gt; \n  count(region) |&gt; \n  mutate(proportion = n/sum(n))\n\nIn this case, we use geom_col instead of geom_bar:\n\ntab |&gt; ggplot(aes(region, proportion)) + geom_col()\n\n\n8.14.2 Histograms\nTo generate histograms we use geom_histogram. By looking at the help file for this function, we learn that the only required argument is x, the variable for which we will construct a histogram. We dropped the x because we know it is the first argument. The code looks like this:\n\nheights |&gt; filter(sex == \"Female\") |&gt; \n  ggplot(aes(height)) + \n  geom_histogram(binwidth = 1, fill = \"blue\", col = \"black\")\n\nNote that we use the optional arguments bandwidth = 1 to change the bin size to 1 inch. The default is to create 30 bins. We also use the optional arguments fill = \"blue\" and col = \"black\" to fill the bars with colors and use a different color to distinguish the bars.\n\n8.14.3 Density plots\nTo create a smooth density, we use the geom_density. To make a smooth density plot with the data previously shown as a histogram we can use this code:\n\nheights |&gt; \n  filter(sex == \"Female\") |&gt;\n  ggplot(aes(height)) +\n  geom_density(fill = \"blue\")\n\nNote that we use the optional argument fill to change the color. To change the smoothness of the density, we use the adjust argument to multiply the default value by that adjust. For example, if we want the bandwidth to be twice as big we use:\n\nheights |&gt; \n  filter(sex == \"Female\") |&gt;\n  ggplot(aes(height)) +\n  geom_density(fill=\"blue\", adjust = 2)\n\n\n8.14.4 Boxplots\nThe geometry for boxplot is geom_boxplot. As discussed, boxplots are useful for comparing distributions. For example, below are the previously shown heights for women, but compared to men. For this geometry, we need arguments x as the categories, and y as the values.\n\nheights |&gt; ggplot(aes(sex, height)) +\n  geom_boxplot()\n\n\n8.14.5 Images\nImages were not needed for the concepts described in this chapter, but we will use images in Section 10.9, so we introduce the two geometries used to create images: geom_tile and geom_raster. They behave similarly; to see how they differ, please consult the help file. To create an image in ggplot2 we need a data frame with the x and y coordinates as well as the values associated with each of these. Here is a data frame.\n\nx &lt;- expand.grid(x = 1:12, y = 1:10) |&gt; mutate(z = 1:120) \n\nNote that this is the tidy version of a matrix, matrix(1:120, 12, 10). To plot the image we use the following code:\n\nx |&gt; ggplot(aes(x, y, fill = z)) + geom_raster()\n\nWith these images you will often want to change the color scale. This can be done through the scale_fill_gradientn layer.\n\nx |&gt; ggplot(aes(x, y, fill = z)) + \n  geom_raster() + \n  scale_fill_gradientn(colors =  terrain.colors(10, 1))"
  },
  {
    "objectID": "dataviz/ggplot2.html#grids-of-plots",
    "href": "dataviz/ggplot2.html#grids-of-plots",
    "title": "8  ggplot2",
    "section": "\n8.15 Grids of plots",
    "text": "8.15 Grids of plots\nThere are often reasons to graph plots next to each other. The gridExtra package permits us to do that. Here are the graphs p5 and p6 created in the previous sections:\n\nlibrary(gridExtra)\ngrid.arrange(p5, p6, ncol = 2)"
  },
  {
    "objectID": "dataviz/ggplot2.html#exercises",
    "href": "dataviz/ggplot2.html#exercises",
    "title": "8  ggplot2",
    "section": "\n8.16 Exercises",
    "text": "8.16 Exercises\nStart by loading the dplyr and ggplot2 library as well as the murders and heights data.\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(dslabs)\n\n1. With ggplot2 plots can be saved as objects. For example we can associate a dataset with a plot object like this\n\np &lt;- ggplot(data = murders)\n\nBecause data is the first argument we don’t need to spell it out\n\np &lt;- ggplot(murders)\n\nand we can also use the pipe:\n\np &lt;- murders |&gt; ggplot()\n\nWhat is class of the object p?\n2. Remember that to print an object you can use the command print or simply type the object. Print the object p defined in exercise one and describe what you see.\n\nNothing happens.\nA blank slate plot.\nA scatterplot.\nA histogram.\n\n3. Using the pipe |&gt;, create an object p but this time associated with the heights dataset instead of the murders dataset.\n4. What is the class of the object p you have just created?\n5. Now we are going to add a layer and the corresponding aesthetic mappings. For the murders data we plotted total murders versus population sizes. Explore the murders data frame to remind yourself what are the names for these two variables and select the correct answer. Hint: Look at ?murders.\n\n\nstate and abb.\n\ntotal_murders and population_size.\n\ntotal and population.\n\nmurders and size.\n\n6. To create the scatterplot we add a layer with geom_point. The aesthetic mappings require us to define the x-axis and y-axis variables, respectively. So the code looks like this:\n\nmurders |&gt; ggplot(aes(x = , y = )) +\n  geom_point()\n\nexcept we have to define the two variables x and y. Fill this out with the correct variable names.\n7. Note that if we don’t use argument names, we can obtain the same plot by making sure we enter the variable names in the right order like this:\n\nmurders |&gt; ggplot(aes(population, total)) +\n  geom_point()\n\nRemake the plot but now with total in the x-axis and population in the y-axis.\n8. If instead of points we want to add text, we can use the geom_text() or geom_label() geometries. The following code\n\nmurders |&gt; ggplot(aes(population, total)) + geom_label()\n\nwill give us the error message: Error: geom_label requires the following missing aesthetics: label\nWhy is this?\n\nWe need to map a character to each point through the label argument in aes.\nWe need to let geom_label know what character to use in the plot.\nThe geom_label geometry does not require x-axis and y-axis values.\n\ngeom_label is not a ggplot2 command.\n\n9. Rewrite the code above to use abbreviation as the label through aes\n10. Change the color of the labels to blue. How will we do this?\n\nAdding a column called blue to murders.\nBecause each label needs a different color we map the colors through aes.\nUse the color argument in ggplot.\nBecause we want all colors to be blue, we do not need to map colors, just use the color argument in geom_label.\n\n11. Rewrite the code above to make the labels blue.\n12. Now suppose we want to use color to represent the different regions. In this case which of the following is most appropriate:\n\nAdding a column called color to murders with the color we want to use.\nBecause each label needs a different color we map the colors through the color argument of aes .\nUse the color argument in ggplot.\nBecause we want all colors to be blue, we do not need to map colors, just use the color argument in geom_label.\n\n13. Rewrite the code above to make the labels’ color be determined by the state’s region.\n14. Now we are going to change the x-axis to a log scale to account for the fact the distribution of population is skewed. Let’s start by defining an object p holding the plot we have made up to now\n\np &lt;- murders |&gt; \n  ggplot(aes(population, total, label = abb, color = region)) +\n  geom_label() \n\nTo change the y-axis to a log scale we learned about the scale_x_log10() function. Add this layer to the object p to change the scale and render the plot.\n15. Repeat the previous exercise but now change both axes to be in the log scale.\n16. Now edit the code above to add the title “Gun murder data” to the plot. Hint: use the ggtitle function.\n17. Now we are going to use the geom_histogram function to make a histogram of the heights in the height data frame. When reading the documentation for this function we see that it requires just one mapping, the values to be used for the histogram. Make a histogram of all the plots.\nWhat is the variable containing the heights?\n\nsex\nheights\nheight\nheights$height\n\n18. Now create a ggplot object using the pipe to assign the heights data to a ggplot object. Assign height to the x values through the aes function.\n19. Now we are ready to add a layer to actually make the histogram. Use the object created in the previous exercise and the geom_histogram function to make the histogram.\n20. Note that when we run the code in the previous exercise we get the warning: stat_bin() using bins = 30. Pick better value with binwidth.`\nUse the binwidth argument to change the histogram made in the previous exercise to use bins of size 1 inch.\n21. Instead of a histogram, we are going to make a smooth density plot. In this case we will not make an object, but instead render the plot with one line of code. Change the geometry in the code previously used to make a smooth density instead of a histogram.\n22. Now we are going to make a density plot for males and females separately. We can do this using the group argument. We assign groups via the aesthetic mapping as each point needs to a group before making the calculations needed to estimate a density.\n23. We can also assign groups through the color argument. This has the added benefit that it uses color to distinguish the groups. Change the code above to use color.\n24. We can also assign groups through the fill argument. This has the added benefit that it uses colors to distinguish the groups, like this:\n\nheights |&gt; \n  ggplot(aes(height, fill = sex)) + \n  geom_density() \n\nHowever, here the second density is drawn over the other. We can make the curves more visible by using alpha blending to add transparency. Set the alpha parameter to 0.2 in the geom_density function to make this change."
  },
  {
    "objectID": "dataviz/ggplot2.html#footnotes",
    "href": "dataviz/ggplot2.html#footnotes",
    "title": "8  ggplot2",
    "section": "",
    "text": "https://ggplot2.tidyverse.org/↩︎\nhttp://www.springer.com/us/book/9780387245447↩︎\nhttps://github.com/rstudio/cheatsheets↩︎\nhttps://github.com/rstudio/cheatsheets/blob/master/LICENSE↩︎"
  },
  {
    "objectID": "dataviz/dataviz-principles.html#encoding-data-using-visual-cues",
    "href": "dataviz/dataviz-principles.html#encoding-data-using-visual-cues",
    "title": "9  Data visualization principles",
    "section": "\n9.1 Encoding data using visual cues",
    "text": "9.1 Encoding data using visual cues\nWe start by describing some principles for encoding data. There are several visual cues at our disposal including position, aligned lengths, angles, area, brightness, and color hue.\nTo illustrate how some of these visual cues compare, let’s suppose we want to report the results from two hypothetical polls regarding browser preference taken in 2000 and then 2015. For each year, we are simply comparing five quantities – the five percentages. A widely used graphical representation of percentages, popularized by Microsoft Excel, is the pie chart:\n\n\n\n\n\n\n\n\nHere we are representing quantities with both areas and angles, since both the angle and area of each pie slice are proportional to the quantity the slice represents. This turns out to be a sub-optimal choice since, as demonstrated by perception studies, humans are not good at precisely quantifying angles and are even worse when area is the only available visual cue. The donut chart is an example of a plot that uses only area:\n\n\n\n\n\n\n\n\nTo see how hard it is to quantify angles and area, note that the rankings and all the percentages in the plots above changed from 2000 to 2015. Can you determine the actual percentages and rank the browsers’ popularity? Can you see how the percentages changed from 2000 to 2015? It is not easy to tell from the plot.\nIn this case, simply showing the numbers is not only clearer, but would also saves on printing costs if printing a paper copy:\n\n\n\n\nBrowser\n2000\n2015\n\n\n\nOpera\n3\n2\n\n\nSafari\n21\n22\n\n\nFirefox\n23\n21\n\n\nChrome\n26\n29\n\n\nIE\n28\n27\n\n\n\n\n\nThe preferred way to plot these quantities is to use length and position as visual cues, since humans are much better at judging linear measures. The barplot uses this approach by using bars of length proportional to the quantities of interest. By adding horizontal lines at strategically chosen values, in this case at every multiple of 10, we ease the visual burden of quantifying through the position of the top of the bars. Compare and contrast the information we can extract from the two figures.\n\n\n\n\n\n\n\n\nNotice how much easier it is to see the differences in the barplot. In fact, we can now determine the actual percentages by following a horizontal line to the x-axis.\nIf for some reason you need to make a pie chart, label each pie slice with its respective percentage so viewers do not have to infer them from the angles or area:\n\n\n\n\n\n\n\n\nIn general, when displaying quantities, position and length are preferred over angles and/or area. Brightness and color are even harder to quantify than angles. But, as we will see later, they are sometimes useful when more than two dimensions must be displayed at once."
  },
  {
    "objectID": "dataviz/dataviz-principles.html#know-when-to-include-0",
    "href": "dataviz/dataviz-principles.html#know-when-to-include-0",
    "title": "9  Data visualization principles",
    "section": "\n9.2 Know when to include 0",
    "text": "9.2 Know when to include 0\nWhen using length as a visual cue, it is misinformative not to start the bars at 0. This is because, by using length as a visual cue, say with a barplot, we are implying the length is proportional to the quantities being displayed. By avoiding 0, relatively small differences can be made to look much bigger than they actually are. This approach is often used by politicians or media organizations trying to exaggerate a difference. Below is an illustrative example6:\n\n(Source: Fox News, via Media Matters7.)\nFrom the plot above, it appears that apprehensions have almost tripled when, in fact, they have only increased by about 16%. Starting the graph at 0 illustrates this clearly:\n\n\n\n\n\n\n\n\nHere is another example:\n\n\n(Source: Fox News, via Flowing Data8.)\nThis plot makes a 13% increase look like a five fold change. Here is the appropriate plot:\n\n\n\n\n\n\n\n\nFinally, here is an extreme example that makes a very small difference of under 2% look like a 10-100 fold change:\n\n\n(Source: Venezolana de Televisión via Pakistan Today9 and Diego Mariano.)\nHere is the appropriate plot:\n\n\n\n\n\n\n\n\nWhen using position rather than length, it is then not necessary to include 0. This is particularly the case when we want to compare differences between groups relative to the within-group variability. Here is an illustrative example showing country average life expectancy stratified across continents in 2012:\n\n\n\n\n\n\n\n\nNote that in the plot on the left, which includes 0, the space between 0 and 43 adds no information and makes it harder to compare the between and within group variability."
  },
  {
    "objectID": "dataviz/dataviz-principles.html#do-not-distort-quantities",
    "href": "dataviz/dataviz-principles.html#do-not-distort-quantities",
    "title": "9  Data visualization principles",
    "section": "\n9.3 Do not distort quantities",
    "text": "9.3 Do not distort quantities\nDuring President Barack Obama’s 2011 State of the Union Address, the following chart was used to compare the US GDP to the GDP of four competing nations:\n\n\n\n(Source: The 2011 State of the Union Address10)\nJudging by the area of the circles, the US appears to have an economy over five times larger than China’s and over 30 times larger than France’s. However, if we look at the actual numbers, we see that this is not the case. The actual ratios are 2.6 and 5.8 times bigger than China and France, respectively. The reason for this distortion is that the radius, rather than the area, was made to be proportional to the quantity, which implies that the proportion between the areas is squared: 2.6 turns into 6.5 and 5.8 turns into 34.1. Here is a comparison of the circles we get if we make the value proportional to the radius and to the area:\n\n\n\n\n\n\n\n\nNot surprisingly, ggplot2 defaults to using area rather than radius. Of course, in this case, we really should not be using area at all since we can use position and length:"
  },
  {
    "objectID": "dataviz/dataviz-principles.html#order-categories-by-a-meaningful-value",
    "href": "dataviz/dataviz-principles.html#order-categories-by-a-meaningful-value",
    "title": "9  Data visualization principles",
    "section": "\n9.4 Order categories by a meaningful value",
    "text": "9.4 Order categories by a meaningful value\nWhen one of the axes is used to show categories, as is done in barplots and boxplots, the default ggplot2 behavior is to order the categories alphabetically when they are defined by character strings. If they are defined by factors, they are ordered by the factor levels. We rarely want to use alphabetical order. Instead, we should order by a meaningful quantity. In all the cases above, the barplots were ordered by the values being displayed. The exception was the graph showing barplots comparing browsers. In this case, we kept the order the same across the barplots to ease the comparison. Specifically, instead of ordering the browsers separately in the two years, we ordered both years by the average value of 2000 and 2015.\nTo appreciate how the right order can help convey a message, suppose we want to create a plot to compare the murder rate across states. We are particularly interested in the most dangerous and safest states. Note the difference when we order alphabetically (the default) versus when we order by the actual rate:\n\n\n\n\n\n\n\n\nHere is an example showing boxplots of income distributions across regions. Here are the two versions plotted against each other:\n\n\n\n\n\n\n\n\nThe first orders the regions alphabetically, while the second orders them by the group’s median."
  },
  {
    "objectID": "dataviz/dataviz-principles.html#show-the-data",
    "href": "dataviz/dataviz-principles.html#show-the-data",
    "title": "9  Data visualization principles",
    "section": "\n9.5 Show the data",
    "text": "9.5 Show the data\nWe have focused on displaying single quantities across categories. We now shift our attention to displaying data, with a focus on comparing groups.\nTo motivate the principle, “show the data”, we go back to our artificial example of describing heights to ET, an extraterrestrial. This time let’s assume ET is interested in the difference in heights between males and females. A commonly seen plot used for comparisons between groups, popularized by software such as Microsoft Excel, is the dynamite plot, which shows the average and standard errors (standard errors are defined in a later chapter, but do not confuse them with the standard deviation of the data). The plot looks like this:\n\n\n\n\n\n\n\n\nThe average of each group is represented by the top of each bar and the antennae extend out from the average to the average plus two standard errors. If all ET receives is this plot, he will have little information on what to expect if he meets a group of human males and females. The bars go to 0: does this mean there are tiny humans measuring less than one foot? Are all males taller than the tallest females? Is there a range of heights? ET can’t answer these questions since we have provided almost no information on the height distribution.\nThis brings us the “show the data” principle. This simple ggplot2 code already generates a more informative plot than the barplot by simply showing all the data points:\n\n\n\n\n\n\n\n\nFor example, this plot gives us an idea of the range of the data. However, this plot has limitations as well, since we can’t really see all the 238 and 812 points plotted for females and males, respectively, and many points are plotted on top of each other. As we have previously described, visualizing the distribution is much more informative. But before doing this, we point out two ways we can improve a plot showing all the points.\nThe first is to add jitter, which adds a small random shift to each point. In this case, adding horizontal jitter does not alter the interpretation, since the point heights do not change, but we minimize the number of points that fall on top of each other and, therefore, get a better visual sense of how the data is distributed. A second improvement comes from using alpha blending: making the points somewhat transparent. The more points fall on top of each other, the darker the plot, which also helps us get a sense of how the points are distributed. Here is the same plot with jitter and alpha blending:\n\nheights |&gt; \n  ggplot(aes(sex, height)) +\n  geom_jitter(width = 0.05, alpha = 0.2) \n\n\n\n\n\n\n\nNow we start getting a sense that, on average, males are taller than females. We also note dark horizontal bands of points, demonstrating that many report values that are rounded to the nearest integer."
  },
  {
    "objectID": "dataviz/dataviz-principles.html#ease-comparisons",
    "href": "dataviz/dataviz-principles.html#ease-comparisons",
    "title": "9  Data visualization principles",
    "section": "\n9.6 Ease comparisons",
    "text": "9.6 Ease comparisons\n\n9.6.1 Use common axes\nSince there are so many points, it is more effective to show distributions rather than individual points. We therefore show histograms for each group:\n\n\n\n\n\n\n\n\nHowever, from this plot it is not immediately obvious that males are, on average, taller than females. We have to look carefully to notice that the x-axis has a higher range of values in the male histogram. An important principle here is to keep the axes the same when comparing data across two plots. Below we see how the comparison becomes easier:\n\n\n\n\n\n\n\n\n\n9.6.2 Align plots vertically to see horizontal changes and horizontally to see vertical changes\nIn these histograms, the visual cue related to decreases or increases in height are shifts to the left or right, respectively: horizontal changes. Aligning the plots vertically helps us see this change when the axes are fixed:\n\n\n\n\n\n\n\n\nThis plot makes it much easier to notice that men are, on average, taller.\nIf we want the more compact summary provided by boxplots, we then align them horizontally since, by default, boxplots move up and down with changes in height. Following our show the data principle, we then overlay all the data points:\n\n\n\n\n\n\n\n\nNow contrast and compare these three plots, based on exactly the same data:\n\n\n\n\n\n\n\n\nNotice how much more we learn from the two plots on the right. Barplots are useful for showing one number, but not very useful when we want to describe distributions."
  },
  {
    "objectID": "dataviz/dataviz-principles.html#consider-transformations",
    "href": "dataviz/dataviz-principles.html#consider-transformations",
    "title": "9  Data visualization principles",
    "section": "\n9.7 Consider transformations",
    "text": "9.7 Consider transformations\nWe have motivated the use of the log transformation in cases where the changes are multiplicative. Population size was an example in which we found a log transformation to yield a more informative transformation.\nThe combination of an incorrectly chosen barplot and a failure to use a log transformation when one is merited can be particularly distorting. As an example, consider this barplot showing the average population sizes for each continent in 2015:\n\n\n\n\n\n\n\n\nFrom this plot, one would conclude that countries in Asia are much more populous than in other continents. Following the show the data principle, we quickly notice that this is due to two very large countries, which we assume are India and China:\n\n\n\n\n\n\n\n\nUsing a log transformation here provides a much more informative plot. We compare the original barplot to a boxplot using the log scale transformation for the y-axis:\n\n\n\n\n\n\n\n\nWith the new plot, we realize that countries in Africa actually have a larger median population size than those in Asia.\nOther transformations you should consider are the logistic transformation (logit), useful to better see fold changes in odds, and the square root transformation (sqrt), useful for count data."
  },
  {
    "objectID": "dataviz/dataviz-principles.html#visual-cues-to-be-compared-should-be-adjacent",
    "href": "dataviz/dataviz-principles.html#visual-cues-to-be-compared-should-be-adjacent",
    "title": "9  Data visualization principles",
    "section": "\n9.8 Visual cues to be compared should be adjacent",
    "text": "9.8 Visual cues to be compared should be adjacent\nFor each continent, let’s compare income in 1970 versus 2010. When comparing income data across regions between 1970 and 2010, we made a figure similar to the one below, but this time we investigate continents rather than regions.\n\n\n\n\n\n\n\n\nThe default in ggplot2 is to order labels alphabetically so the labels with 1970 come before the labels with 2010, making the comparisons challenging because a continent’s distribution in 1970 is visually far from its distribution in 2010. It is much easier to make the comparison between 1970 and 2010 for each continent when the boxplots for that continent are next to each other:\n\n\n\n\n\n\n\n\nThe comparison becomes even easier to make if we use color to denote the two things we want to compare:"
  },
  {
    "objectID": "dataviz/dataviz-principles.html#think-of-the-color-blind",
    "href": "dataviz/dataviz-principles.html#think-of-the-color-blind",
    "title": "9  Data visualization principles",
    "section": "\n9.9 Think of the color blind",
    "text": "9.9 Think of the color blind\nAbout 10% of the population is color blind. Unfortunately, the default colors used in ggplot2 are not optimal for this group. However, ggplot2 does make it easy to change the color palette used in the plots. An example of how we can use a color blind friendly palette is described here: http://www.cookbook-r.com/Graphs/Colors_(ggplot2)/#a-colorblind-friendly-palette:\n\ncolor_blind_friendly_cols &lt;- \n  c(\"#999999\", \"#E69F00\", \"#56B4E9\", \"#009E73\", \n    \"#F0E442\", \"#0072B2\", \"#D55E00\", \"#CC79A7\")\n\nHere are the colors\n\n\n\n\n\n\n\n\nThere are several resources that can help you select colors, for example this one: http://bconnelly.net/2013/10/creating-colorblind-friendly-figures/."
  },
  {
    "objectID": "dataviz/dataviz-principles.html#plots-for-two-variables",
    "href": "dataviz/dataviz-principles.html#plots-for-two-variables",
    "title": "9  Data visualization principles",
    "section": "\n9.10 Plots for two variables",
    "text": "9.10 Plots for two variables\nIn general, you should use scatterplots to visualize the relationship between two variables. In every single instance in which we have examined the relationship between two variables, including total murders versus population size, life expectancy versus fertility rates, and infant mortality versus income, we have used scatterplots. This is the plot we generally recommend. However, there are some exceptions and we describe two alternative plots here: the slope chart and the Bland-Altman plot.\n\n9.10.1 Slope charts\nOne exception where another type of plot may be more informative is when you are comparing variables of the same type, but at different time points and for a relatively small number of comparisons. For example, comparing life expectancy between 2010 and 2015. In this case, we might recommend a slope chart.\nThere is no geometry for slope charts in ggplot2, but we can construct one using geom_line. We need to do some tinkering to add labels. Below is an example comparing 2010 to 2015 for large western countries:\n\n\n\n\n\n\n\n\nAn advantage of the slope chart is that it permits us to quickly get an idea of changes based on the slope of the lines. Although we are using angle as the visual cue, we also have position to determine the exact values. Comparing the improvements is a bit harder with a scatterplot:\n\nlibrary(ggrepel)\nwest &lt;- c(\"Western Europe\",\"Northern Europe\",\"Southern Europe\",\n          \"Northern America\",\"Australia and New Zealand\")\n\ndat &lt;- gapminder |&gt; \n  filter(year%in% c(2010, 2015) & region %in% west & \n           !is.na(life_expectancy) & population &gt; 10^7) \n\ndat |&gt; \n  mutate(year = paste0(\"life_expectancy_\", year)) |&gt;\n  select(country, year, life_expectancy) |&gt;\n  spread(year, life_expectancy) |&gt; \n  ggplot(aes(x=life_expectancy_2010,y=life_expectancy_2015, label = country)) + \n  geom_point() + geom_text_repel() +\n  scale_x_continuous(limits=c(78.5, 83)) +\n  scale_y_continuous(limits=c(78.5, 83)) +\n  geom_abline(lty = 2) +\n  xlab(\"2010\") + \n  ylab(\"2015\")\n\n\n\n\n\n\n\nIn the scatterplot, we have followed the principle use common axes since we are comparing these before and after. However, if we have many points, slope charts stop being useful as it becomes hard to see all the lines.\n\n9.10.2 Bland-Altman plot\nSince we are primarily interested in the difference, it makes sense to dedicate one of our axes to it. The Bland-Altman plot, also known as the Tukey mean-difference plot and the MA-plot, shows the difference versus the average:\n\nlibrary(ggrepel)\ndat |&gt; \n  mutate(year = paste0(\"life_expectancy_\", year)) |&gt;\n  select(country, year, life_expectancy) |&gt; \n  pivot_wider(names_from = \"year\", values_from=\"life_expectancy\") |&gt; \n  mutate(average = (life_expectancy_2015 + life_expectancy_2010)/2,\n         difference = life_expectancy_2015 - life_expectancy_2010) |&gt;\n  ggplot(aes(average, difference, label = country)) + \n  geom_point() +\n  geom_text_repel() +\n  geom_abline(lty = 2) +\n  xlab(\"Average of 2010 and 2015\") + \n  ylab(\"Difference between 2015 and 2010\")\n\n\n\n\n\n\n\nHere, by simply looking at the y-axis, we quickly see which countries have shown the most improvement. We also get an idea of the overall value from the x-axis."
  },
  {
    "objectID": "dataviz/dataviz-principles.html#encoding-a-third-variable",
    "href": "dataviz/dataviz-principles.html#encoding-a-third-variable",
    "title": "9  Data visualization principles",
    "section": "\n9.11 Encoding a third variable",
    "text": "9.11 Encoding a third variable\nAn earlier scatterplot showed the relationship between infant survival and average income. Below is a version of this plot that encodes three variables: OPEC membership, region, and population.\n\n\n\n\n\n\n\n\nWe encode categorical variables with color and shape. These shapes can be controlled with shape argument. Below are the shapes available for use in R. For the last five, the color goes inside.\n\n\n\n\n\n\n\n\nFor continuous variables, we can use color, intensity, or size. We now show an example of how we do this with a case study.\nWhen selecting colors to quantify a numeric variable, we choose between two options: sequential and diverging. Sequential colors are suited for data that goes from high to low. High values are clearly distinguished from low values. Here are some examples offered by the package RColorBrewer:\n\n\n\n\n\n\n\n\nDiverging colors are used to represent values that diverge from a center. We put equal emphasis on both ends of the data range: higher than the center and lower than the center. An example of when we would use a divergent pattern would be if we were to show height in standard deviations away from the average. Here are some examples of divergent patterns:"
  },
  {
    "objectID": "dataviz/dataviz-principles.html#avoid-pseudo-three-dimensional-plots",
    "href": "dataviz/dataviz-principles.html#avoid-pseudo-three-dimensional-plots",
    "title": "9  Data visualization principles",
    "section": "\n9.12 Avoid pseudo-three-dimensional plots",
    "text": "9.12 Avoid pseudo-three-dimensional plots\nThe figure below, taken from the scientific literature11, shows three variables: dose, drug type and survival. Although your screen/book page is flat and two-dimensional, the plot tries to imitate three dimensions and assigned a dimension to each variable.\n\n\n(Image courtesy of Karl Broman)\nHumans are not good at seeing in three dimensions (which explains why it is hard to parallel park) and our limitation is even worse with regard to pseudo-three-dimensions. To see this, try to determine the values of the survival variable in the plot above. Can you tell when the purple ribbon intersects the red one? This is an example in which we can easily use color to represent the categorical variable instead of using a pseudo-3D:\n\n\n\n\n\n\n\n\nNotice how much easier it is to determine the survival values.\nPseudo-3D is sometimes used completely gratuitously: plots are made to look 3D even when the 3rd dimension does not represent a quantity. This only adds confusion and makes it harder to relay your message. Here are two examples:\n\n\n\n\n\n\n\n\n\n\n(Images courtesy of Karl Broman)"
  },
  {
    "objectID": "dataviz/dataviz-principles.html#avoid-too-many-significant-digits",
    "href": "dataviz/dataviz-principles.html#avoid-too-many-significant-digits",
    "title": "9  Data visualization principles",
    "section": "\n9.13 Avoid too many significant digits",
    "text": "9.13 Avoid too many significant digits\nBy default, statistical software like R returns many significant digits. The default behavior in R is to show 7 significant digits. That many digits often adds no information and the added visual clutter can make it hard for the viewer to understand the message. As an example, here are the per 10,000 disease rates, computed from totals and population in R, for California across the five decades:\n\n\n\n\nstate\nyear\nMeasles\nPertussis\nPolio\n\n\n\nCalifornia\n1940\n37.8826320\n18.3397861\n0.8266512\n\n\nCalifornia\n1950\n13.9124205\n4.7467350\n1.9742639\n\n\nCalifornia\n1960\n14.1386471\nNA\n0.2640419\n\n\nCalifornia\n1970\n0.9767889\nNA\nNA\n\n\nCalifornia\n1980\n0.3743467\n0.0515466\nNA\n\n\n\n\n\nWe are reporting precision up to 0.00001 cases per 10,000, a very small value in the context of the changes that are occurring across the dates. In this case, two significant figures is more than enough and clearly makes the point that rates are decreasing:\n\n\n\n\nstate\nyear\nMeasles\nPertussis\nPolio\n\n\n\nCalifornia\n1940\n37.9\n18.3\n0.8\n\n\nCalifornia\n1950\n13.9\n4.7\n2.0\n\n\nCalifornia\n1960\n14.1\nNA\n0.3\n\n\nCalifornia\n1970\n1.0\nNA\nNA\n\n\nCalifornia\n1980\n0.4\n0.1\nNA\n\n\n\n\n\nUseful ways to change the number of significant digits or to round numbers are signif and round. You can define the number of significant digits globally by setting options like this: options(digits = 3).\nAnother principle related to displaying tables is to place values being compared on columns rather than rows. Note that our table above is easier to read than this one:\n\n\n\n\nstate\ndisease\n1940\n1950\n1960\n1970\n1980\n\n\n\nCalifornia\nMeasles\n37.9\n13.9\n14.1\n1\n0.4\n\n\nCalifornia\nPertussis\n18.3\n4.7\nNA\nNA\n0.1\n\n\nCalifornia\nPolio\n0.8\n2.0\n0.3\nNA\nNA"
  },
  {
    "objectID": "dataviz/dataviz-principles.html#know-your-audience",
    "href": "dataviz/dataviz-principles.html#know-your-audience",
    "title": "9  Data visualization principles",
    "section": "\n9.14 Know your audience",
    "text": "9.14 Know your audience\nGraphs can be used for 1) our own exploratory data analysis, 2) to convey a message to experts, or 3) to help convery a message to a general audience. Make sure that the intended audience understands each element of the plot.\nAs a simple example, consider that for your own exploration it may be more useful to log-transform data and then plot it. However, for a general audience that is unfamiliar with converting logged values back to the original measurements, using a log-scale for the axis instead of log-transformed values will be much easier to digest."
  },
  {
    "objectID": "dataviz/dataviz-principles.html#exercises",
    "href": "dataviz/dataviz-principles.html#exercises",
    "title": "9  Data visualization principles",
    "section": "\n9.15 Exercises",
    "text": "9.15 Exercises\nFor these exercises, we will be using the vaccines data in the dslabs package:\n\nlibrary(dslabs)\n\n1. Pie charts are appropriate:\n\nWhen we want to display percentages.\nWhen ggplot2 is not available.\nWhen I am in a bakery.\nNever. Barplots and tables are always better.\n\n2. What is the problem with the plot below:\n\n\n\n\n\n\n\n\n\nThe values are wrong. The final vote was 306 to 232.\nThe axis does not start at 0. Judging by the length, it appears Trump received 3 times as many votes when, in fact, it was about 30% more.\nThe colors should be the same.\nPercentages should be shown as a pie chart.\n\n3. Take a look at the following two plots. They show the same information: 1928 rates of measles across the 50 states.\n\n\n\n\n\n\n\n\nWhich plot is easier to read if you are interested in determining which are the best and worst states in terms of rates, and why?\n\nThey provide the same information, so they are both equally as good.\nThe plot on the right is better because it orders the states alphabetically.\nThe plot on the right is better because alphabetical order has nothing to do with the disease and by ordering according to actual rate, we quickly see the states with most and least rates.\nBoth plots should be a pie chart.\n\n4. To make the plot on the left, we have to reorder the levels of the states’ variables.\n\ndat &lt;- us_contagious_diseases |&gt;  \n  filter(year == 1967 & disease==\"Measles\" & !is.na(population)) |&gt;\n  mutate(rate = count / population * 10000 * 52 / weeks_reporting)\n\nNote what happens when we make a barplot:\n\ndat |&gt; ggplot(aes(state, rate)) +\n  geom_col() +\n  coord_flip() \n\n\n\n\n\n\n\nDefine these objects:\n\nstate &lt;- dat$state\nrate &lt;- dat$count/dat$population*10000*52/dat$weeks_reporting\n\nRedefine the state object so that the levels are re-ordered. Print the new object state and its levels so you can see that the vector is not re-ordered by the levels.\n5. Now with one line of code, define the dat table as done above, but change the use mutate to create a rate variable and re-order the state variable so that the levels are re-ordered by this variable. Then make a barplot using the code above, but for this new dat.\n6. Say we are interested in comparing gun homicide rates across regions of the US. We see this plot:\n\nlibrary(dslabs)\nmurders |&gt; mutate(rate = total/population*100000) |&gt;\ngroup_by(region) |&gt;\nsummarize(avg = mean(rate)) |&gt;\nmutate(region = factor(region)) |&gt;\nggplot(aes(region, avg)) +\ngeom_col() + \nylab(\"Murder Rate Average\")\n\n\n\n\n\n\n\nand decide to move to a state in the western region. What is the main problem with this interpretation?\n\nThe categories are ordered alphabetically.\nThe graph does not show standarad errors.\nIt does not show all the data. We do not see the variability within a region and it’s possible that the safest states are not in the West.\nThe Northeast has the lowest average.\n\n7. Make a boxplot of the murder rates defined as\n\nmurders |&gt; mutate(rate = total/population*100000)\n\nby region, showing all the points and ordering the regions by their median rate.\n8. The plots below show three continuous variables.\n\n\n\n\n\n\n\n\nThe line \\(x=2\\) appears to separate the points. But it is actually not the case, which we can see by plotting the data in a couple of two-dimensional points.\n\n\n\n\n\n\n\n\nWhy is this happening?\n\nHumans are not good at reading pseudo-3D plots.\nThere must be an error in the code.\nThe colors confuse us.\nScatterplots should not be used to compare two variables when we have access to 3."
  },
  {
    "objectID": "dataviz/dataviz-principles.html#footnotes",
    "href": "dataviz/dataviz-principles.html#footnotes",
    "title": "9  Data visualization principles",
    "section": "",
    "text": "http://kbroman.org/↩︎\nhttps://www.biostat.wisc.edu/~kbroman/presentations/graphs2017.pdf↩︎\nhttps://github.com/kbroman/Talk_Graphs↩︎\nhttp://paldhous.github.io/ucb/2016/dataviz/index.html↩︎\nhttps://github.com/rafalab/dsbook-part-1/blob/main/dataviz/dataviz-principles.qmd↩︎\nhttp://paldhous.github.io/ucb/2016/dataviz/week2.html↩︎\nhttp://mediamatters.org/blog/2013/04/05/fox-news-newest-dishonest-chart-immigration-enf/193507↩︎\nhttp://flowingdata.com/2012/08/06/fox-news-continues-charting-excellence/↩︎\nhttps://www.pakistantoday.com.pk/2018/05/18/whats-at-stake-in-venezuelan-presidential-vote↩︎\nhttps://www.youtube.com/watch?v=kl2g40GoRxg↩︎\nhttps://projecteuclid.org/download/pdf_1/euclid.ss/1177010488↩︎"
  },
  {
    "objectID": "dataviz/dataviz-in-practice.html#case-study-1-new-insights-on-poverty",
    "href": "dataviz/dataviz-in-practice.html#case-study-1-new-insights-on-poverty",
    "title": "10  Data visualization in practice",
    "section": "\n10.1 Case study 1: new insights on poverty",
    "text": "10.1 Case study 1: new insights on poverty\nHans Rosling1 was the co-founder of the Gapminder Foundation2, an organization dedicated to educating the public by using data to dispel common myths about the so-called developing world. The organization uses data to show how actual trends in health and economics contradict the narratives that emanate from sensationalist media coverage of catastrophes, tragedies, and other unfortunate events. As stated in the Gapminder Foundation’s website:\n\n\n\nJournalists and lobbyists tell dramatic stories. That’s their job. They tell stories about extraordinary events and unusual people. The piles of dramatic stories pile up in peoples’ minds into an over-dramatic worldview and strong negative stress feelings: “The world is getting worse!”, “It’s we vs. them!”, “Other people are strange!”, “The population just keeps growing!” and “Nobody cares!”\n\n\n\nHans Rosling conveyed actual data-based trends in a dramatic way of his own, using effective data visualization. This section is based on two talks that exemplify this approach to education: [New Insights on Poverty]3 and The Best Stats You’ve Ever Seen4. Specifically, in this section, we use data to attempt to answer the following two questions:\n\nIs it a fair characterization of today’s world to say it is divided into western rich nations and the developing world in Africa, Asia, and Latin America?\nHas income inequality across countries worsened during the last 40 years?\n\nTo answer these questions, we will be using the gapminder dataset provided in dslabs. This dataset was created using a number of spreadsheets available from the Gapminder Foundation. You can access the table like this:\n\nlibrary(tidyverse)\nlibrary(dslabs)\ngapminder |&gt; as_tibble()\n#&gt; # A tibble: 10,545 × 9\n#&gt;   country     year infant_mortality life_expectancy fertility population\n#&gt;   &lt;fct&gt;      &lt;int&gt;            &lt;dbl&gt;           &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n#&gt; 1 Albania     1960            115.             62.9      6.19    1636054\n#&gt; 2 Algeria     1960            148.             47.5      7.65   11124892\n#&gt; 3 Angola      1960            208              36.0      7.32    5270844\n#&gt; 4 Antigua a…  1960             NA              63.0      4.43      54681\n#&gt; 5 Argentina   1960             59.9            65.4      3.11   20619075\n#&gt; # ℹ 10,540 more rows\n#&gt; # ℹ 3 more variables: gdp &lt;dbl&gt;, continent &lt;fct&gt;, region &lt;fct&gt;\n\n\n10.1.1 Hans Rosling’s quiz\nAs done in the New Insights on Poverty video, we start by testing our knowledge regarding differences in child mortality across different countries. For each of the six pairs of countries below, which country do you think had the highest child mortality rates in 2015? Which pairs do you think are most similar?\n\nSri Lanka or Turkey\nPoland or South Korea\nMalaysia or Russia\nPakistan or Vietnam\nThailand or South Africa\n\nWhen answering these questions without data, the non-European countries are typically picked as having higher child mortality rates: Sri Lanka over Turkey, South Korea over Poland, and Malaysia over Russia. It is also common to assume that countries considered to be part of the developing world: Pakistan, Vietnam, Thailand, and South Africa, have similarly high mortality rates.\nTo answer these questions with data, we can use dplyr. For example, for the first comparison we see that:\n\ngapminder |&gt; \n  filter(year == 2015 & country %in% c(\"Sri Lanka\",\"Turkey\")) |&gt; \n  select(country, infant_mortality)\n#&gt;     country infant_mortality\n#&gt; 1 Sri Lanka              8.4\n#&gt; 2    Turkey             11.6\n\nTurkey has the higher infant mortality rate.\nWe can use this code on all comparisons and find the following:\n\n\n\n\ncountry\ninfant_mortality\ncountry\ninfant_mortality\n\n\n\nSri Lanka\n8.4\nTurkey\n11.6\n\n\nPoland\n4.5\nSouth Korea\n2.9\n\n\nMalaysia\n6.0\nRussia\n8.2\n\n\nPakistan\n65.8\nVietnam\n17.3\n\n\nThailand\n10.5\nSouth Africa\n33.6\n\n\n\n\n\nWe see that the European countries on this list have higher child mortality rates: Poland has a higher rate than South Korea, and Russia has a higher rate than Malaysia. We also see that Pakistan has a much higher rate than Vietnam, and South Africa has a much higher rate than Thailand. It turns out that when Hans Rosling gave this quiz to educated groups of people, the average score was less than 2.5 out of 5, worse than what they would have obtained had they guessed randomly. This implies that more than ignorant, we are misinformed. In this chapter we see how data visualization helps inform us."
  },
  {
    "objectID": "dataviz/dataviz-in-practice.html#scatterplots",
    "href": "dataviz/dataviz-in-practice.html#scatterplots",
    "title": "10  Data visualization in practice",
    "section": "\n10.2 Scatterplots",
    "text": "10.2 Scatterplots\nThe reason for the misconception described in the previous section stems from the preconceived notion that the world is divided into two groups: the western world (Western Europe and North America), characterized by long life spans and small families, versus the developing world (Africa, Asia, and Latin America) characterized by short life spans and large families. But do the data support this dichotomous view?\nThe necessary data to answer this question is also available in our gapminder table. Using our newly learned data visualization skills, we will be able to tackle this challenge.\nIn order to analyze this world view, our first plot is a scatterplot of life expectancy versus fertility rates (average number of children per woman). We start by looking at data from about 50 years ago, when perhaps this view was first cemented in our minds.\n\nfilter(gapminder, year == 1962) |&gt;\n  ggplot(aes(fertility, life_expectancy)) +\n  geom_point()\n\n\n\n\n\n\n\nMost points fall into two distinct categories:\n\nLife expectancy around 70 years and 3 or fewer children per family.\nLife expectancy lower than 65 years and more than 5 children per family.\n\nTo confirm that indeed these countries are from the regions we expect, we can use color to represent continent.\n\nfilter(gapminder, year == 1962) |&gt;\n  ggplot( aes(fertility, life_expectancy, color = continent)) +\n  geom_point() \n\n\n\n\n\n\n\nIn 1962, “the West versus developing world” view was grounded in some reality. Is this still the case 50 years later?"
  },
  {
    "objectID": "dataviz/dataviz-in-practice.html#faceting",
    "href": "dataviz/dataviz-in-practice.html#faceting",
    "title": "10  Data visualization in practice",
    "section": "\n10.3 Faceting",
    "text": "10.3 Faceting\nWe could easily plot the 2012 data in the same way we did for 1962. To make comparisons, however, side by side plots are preferable. In ggplot2, we can achieve this by faceting variables: we stratify the data by some variable and make the same plot for each strata.\nTo achieve faceting, we add a layer with the function facet_grid, which automatically separates the plots. This function lets you facet by up to two variables using columns to represent one variable and rows to represent the other. The function expects the row and column variables to be separated by a ~. Here is an example of a scatterplot with facet_grid added as the last layer:\n\nfilter(gapminder, year %in% c(1962, 2012)) |&gt;\n  ggplot(aes(fertility, life_expectancy, col = continent)) +\n  geom_point() +\n  facet_grid(year~continent)\n\n\n\n\n\n\n\nWe see a plot for each continent/year pair. However, this is just an example and more than what we want, which is simply to compare 1962 and 2012. In this case, there is just one variable and we use . to let facet know that we are not using one of the variables:\n\nfilter(gapminder, year %in% c(1962, 2012)) |&gt;\n  ggplot(aes(fertility, life_expectancy, col = continent)) +\n  geom_point() +\n  facet_grid(. ~ year)\n\n\n\n\n\n\n\nThis plot clearly shows that the majority of countries have moved from the developing world cluster to the western world one. In 2012, the western versus developing world view no longer makes sense. This is particularly clear when comparing Europe to Asia, the latter of which includes several countries that have made great improvements.\n\n10.3.1 facet_wrap\n\nTo explore how this transformation happened through the years, we can make the plot for several years. For example, we can add 1970, 1980, 1990, and 2000. If we do this, we will not want all the plots on the same row, the default behavior of facet_grid, since they will become too thin to show the data. Instead, we will want to use multiple rows and columns. The function facet_wrap permits us to do this by automatically wrapping the series of plots so that each display has viewable dimensions:\n\nyears &lt;- c(1962, 1980, 1990, 2000, 2012)\ncontinents &lt;- c(\"Europe\", \"Asia\")\ngapminder |&gt; \n  filter(year %in% years & continent %in% continents) |&gt;\n  ggplot( aes(fertility, life_expectancy, col = continent)) +\n  geom_point() +\n  facet_wrap(~year) \n\n\n\n\n\n\n\nThis plot clearly shows how most Asian countries have improved at a much faster rate than European ones.\n\n10.3.2 Fixed scales for better comparisons\nThe default choice of the range of the axes is important. When not using facet, this range is determined by the data shown in the plot. When using facet, this range is determined by the data shown in all plots and therefore kept fixed across plots. This makes comparisons across plots much easier. For example, in the above plot, we can see that life expectancy has increased and the fertility has decreased across most countries. We see this because the cloud of points moves. This is not the case if we adjust the scales:\n\nfilter(gapminder, year %in% c(1962, 2012)) |&gt;\n  ggplot(aes(fertility, life_expectancy, col = continent)) +\n  geom_point() +\n  facet_wrap(. ~ year, scales = \"free\")\n\n\n\n\n\n\n\nIn the plot above, we have to pay special attention to the range to notice that the plot on the right has a larger life expectancy."
  },
  {
    "objectID": "dataviz/dataviz-in-practice.html#time-series-plots",
    "href": "dataviz/dataviz-in-practice.html#time-series-plots",
    "title": "10  Data visualization in practice",
    "section": "\n10.4 Time series plots",
    "text": "10.4 Time series plots\nThe visualizations above effectively illustrate that data no longer supports the western versus developing world view. Once we see these plots, new questions emerge. For example, which countries are improving more and which ones less? Was the improvement constant during the last 50 years or was it more accelerated during certain periods? For a closer look that may help answer these questions, we introduce time series plots.\nTime series plots have time in the x-axis and an outcome or measurement of interest on the y-axis. For example, here is a trend plot of United States fertility rates:\n\ngapminder |&gt; \n  filter(country == \"United States\") |&gt; \n  ggplot(aes(year, fertility)) +\n  geom_point()\n\n\n\n\n\n\n\nWe see that the trend is not linear at all. Instead there is sharp drop during the 1960s and 1970s to below 2. Then the trend comes back to 2 and stabilizes during the 1990s.\nWhen the points are regularly and densely spaced, as they are here, we create curves by joining the points with lines, to convey that these data are from a single series, here a country. To do this, we use the geom_line function instead of geom_point.\n\ngapminder |&gt; \n  filter(country == \"United States\") |&gt; \n  ggplot(aes(year, fertility)) +\n  geom_line()\n\n\n\n\n\n\n\nThis is particularly helpful when we look at two countries. If we subset the data to include two countries, one from Europe and one from Asia, then adapt the code above:\n\ncountries &lt;- c(\"South Korea\", \"Germany\")\n\ngapminder |&gt; filter(country %in% countries) |&gt; \n  ggplot(aes(year,fertility)) +\n  geom_line()\n\n\n\n\n\n\n\nUnfortunately, this is not the plot that we want. Rather than a line for each country, the points for both countries are joined. This is actually expected since we have not told ggplot anything about wanting two separate lines. To let ggplot know that there are two curves that need to be made separately, we assign each point to a group, one for each country:\n\ncountries &lt;- c(\"South Korea\",\"Germany\")\n\ngapminder |&gt; filter(country %in% countries & !is.na(fertility)) |&gt; \n  ggplot(aes(year, fertility, group = country)) +\n  geom_line()\n\n\n\n\n\n\n\nBut which line goes with which country? We can assign colors to make this distinction. A useful side-effect of using the color argument to assign different colors to the different countries is that the data is automatically grouped:\n\ncountries &lt;- c(\"South Korea\",\"Germany\")\ngapminder |&gt; filter(country %in% countries & !is.na(fertility)) |&gt; \n  ggplot(aes(year,fertility, col = country)) +\n  geom_line()\n\n\n\n\n\n\n\nThe plot clearly shows how South Korea’s fertility rate dropped drastically during the 1960s and 1970s, and by 1990 had a similar rate to that of Germany.\n\n10.4.1 Labels instead of legends\nFor trend plots we recommend labeling the lines rather than using legends since the viewer can quickly see which line is which country. This suggestion actually applies to most plots: labeling is usually preferred over legends.\nWe demonstrate how we can do this using the geomtextpath package. We define a data table with the label locations and then use a second mapping just for these labels:\n\nlibrary(geomtextpath)\ngapminder |&gt; \n  filter(country %in% countries) |&gt; \n  ggplot(aes(year, life_expectancy, col = country, label = country)) +\n  geom_textpath() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nThe plot clearly shows how an improvement in life expectancy followed the drops in fertility rates. In 1960, Germans lived 15 years longer than South Koreans, although by 2010 the gap is completely closed. It exemplifies the improvement that many non-western countries have achieved in the last 40 years."
  },
  {
    "objectID": "dataviz/dataviz-in-practice.html#data-transformations",
    "href": "dataviz/dataviz-in-practice.html#data-transformations",
    "title": "10  Data visualization in practice",
    "section": "\n10.5 Data transformations",
    "text": "10.5 Data transformations\nWe now shift our attention to the second question related to the commonly held notion that wealth distribution across the world has become worse during the last decades. When general audiences are asked if poor countries have become poorer and rich countries become richer, the majority answers yes. By using stratification, histograms, smooth densities, and boxplots, we will be able to understand if this is in fact the case. First we learn how transformations can sometimes help provide more informative summaries and plots.\nThe gapminder data table includes a column with the countries’ gross domestic product (GDP). GDP measures the market value of goods and services produced by a country in a year. The GDP per person is often used as a rough summary of a country’s wealth. Here we divide this quantity by 365 to obtain the more interpretable measure dollars per day. Using current US dollars as a unit, a person surviving on an income of less than $2 a day is defined to be living in absolute poverty. We add this variable to the data table:\n\ngapminder &lt;- gapminder |&gt;  mutate(dollars_per_day = gdp/population/365)\n\nThe GDP values are adjusted for inflation and represent current US dollars, so these values are meant to be comparable across the years. Of course, these are country averages and within each country there is much variability. All the graphs and insights described below relate to country averages and not to individuals.\n\n10.5.1 Log transformation\nHere is a histogram of per day incomes from 1970:\n\npast_year &lt;- 1970\ngapminder |&gt; \n  filter(year == past_year & !is.na(gdp)) |&gt;\n  ggplot(aes(dollars_per_day)) + \n  geom_histogram(binwidth = 1, color = \"black\")\n\n\n\n\n\n\n\nWe use the color = \"black\" argument to draw a boundary and clearly distinguish the bins.\nIn this plot, we see that for the majority of countries, averages are below $10 a day. However, the majority of the x-axis is dedicated to the 35 countries with averages above $10. So the plot is not very informative about countries with values below $10 a day.\nIt might be more informative to quickly be able to see how many countries have average daily incomes of about $1 (extremely poor), $2 (very poor), $4 (poor), $8 (middle), $16 (well off), $32 (rich), $64 (very rich) per day. These changes are multiplicative and log transformations convert multiplicative changes into additive ones: when using base 2, a doubling of a value turns into an increase by 1.\nHere is the distribution if we apply a log base 2 transform:\n\ngapminder |&gt; \n  filter(year == past_year & !is.na(gdp)) |&gt;\n  ggplot(aes(log2(dollars_per_day))) + \n  geom_histogram(binwidth = 1, color = \"black\")\n\n\n\n\n\n\n\nIn a way, this provides a close-up of the mid to lower income countries.\n\n10.5.2 Which base?\nIn the case above, we used base 2 in the log transformations. Other common choices are base \\(\\mathrm{e}\\) (the natural log) and base 10.\nIn general, we do not recommend using the natural log for data exploration and visualization. This is because while \\(2^2, 2^3, 2^4, \\dots\\) or \\(10^2, 10^3, \\dots\\) are easy to mentally compute, but the same is not true for \\(\\mathrm{e}^2, \\mathrm{e}^3, \\dots\\). So natural log scale is not intuitive or easy to interpret.\nIn the dollars per day example, we used base 2 instead of base 10 because the resulting range is easier to interpret. The range of the values being plotted is 0.3269426, 48.8852142.\nIn base 10, this turns into a range that includes very few integers: just 0 and 1. With base two, our range includes -2, -1, 0, 1, 2, 3, 4, and 5. It is easier to compute \\(2^x\\) and \\(10^x\\) when \\(x\\) is an integer and between -10 and 10, so we prefer to have smaller integers in the scale. Another consequence of a limited range is that choosing the binwidth is more challenging. With log base 2, we know that a binwidth of 1 will translate to a bin with range \\(x\\) to \\(2x\\).\nFor an example in which base 10 makes more sense, consider population sizes. A log base 10 is preferable since the range for these is:\n\nfilter(gapminder, year == past_year) |&gt;\n  summarize(min = min(population), max = max(population))\n#&gt;     min      max\n#&gt; 1 46075 8.09e+08\n\nHere is the histogram of the transformed values:\n\ngapminder |&gt; \n  filter(year == past_year) |&gt;\n  ggplot(aes(log10(population))) +\n  geom_histogram(binwidth = 0.5, color = \"black\")\n\n\n\n\n\n\n\nIn the above, we quickly see that country populations range between ten thousand and ten billion.\n\n10.5.3 Transform the values or the scale?\nThere are two ways we can use log transformations in plots. We can log the values before plotting them or use log scales in the axes. The plot will look the same, except for the numbers in the axes. Both approaches are useful and have different strengths. If we log the data, we can more easily interpret intermediate values in the scale. For example, if we see:\n----1----x----2--------3----\nfor log transformed data, we know that the value of \\(x\\) is about 1.5. If the scales are logged:\n----10---x---100------1000---\nthen, to determine x, we need to compute \\(10^{1.5}\\), which is not easy to do in our heads. The advantage of using logged scales is that we see the original values on the axes. However, the advantage of showing logged scales is that the original values are displayed in the plot, which are easier to interpret. For example, we would see “32 dollars a day” instead of “5 log base 2 dollars a day”.\nAs we learned earlier, if we want to scale the axis with logs, we can use the scale_x_continuous function. Instead of logging the values first, we apply this layer:\n\ngapminder |&gt; \n  filter(year == past_year & !is.na(gdp)) |&gt;\n  ggplot(aes(dollars_per_day)) + \n  geom_histogram(binwidth = 1, color = \"black\") +\n  scale_x_continuous(trans = \"log2\")\n\n\n\n\n\n\n\nNote that the log base 10 transformation has its own function: scale_x_log10(), but currently base 2 does not, although we could easily define our own.\nThere are other transformations available through the trans argument. As we learn later on, the square root (sqrt) transformation is useful when considering counts. The logistic transformation (logit) is useful when plotting proportions between 0 and 1. The reverse transformation is useful when we want smaller values to be on the right or on top."
  },
  {
    "objectID": "dataviz/dataviz-in-practice.html#multimodal-distributions",
    "href": "dataviz/dataviz-in-practice.html#multimodal-distributions",
    "title": "10  Data visualization in practice",
    "section": "\n10.6 Multimodal distributions",
    "text": "10.6 Multimodal distributions\nIn the histogram above we see two bumps: one at about 4 and another at about 32. In statistics these bumps are sometimes referred to as modes. The mode of a distribution is the value with the highest frequency. The mode of the normal distribution is the average. When a distribution, like the one above, doesn’t monotonically decrease from the mode, we call the locations where it goes up and down again local modes and say that the distribution has multiple modes.\nThe histogram above suggests that the 1970 country income distribution has two modes: one at about 2 dollars per day (1 in the log 2 scale) and another at about 32 dollars per day (5 in the log 2 scale). This bimodality is consistent with a dichotomous world made up of countries with average incomes less than $8 (3 in the log 2 scale) a day and countries above that."
  },
  {
    "objectID": "dataviz/dataviz-in-practice.html#comparing-distributions",
    "href": "dataviz/dataviz-in-practice.html#comparing-distributions",
    "title": "10  Data visualization in practice",
    "section": "\n10.7 Comparing distributions",
    "text": "10.7 Comparing distributions\nA histogram showed us that the 1970 income distribution values show a dichotomy. However, the histogram does not show us if the two groups of countries are west versus the developing world.\nLet’s start by quickly examining the data by region. We reorder the regions by the median value and use a log scale.\n\ngapminder |&gt; \n  filter(year == past_year & !is.na(gdp)) |&gt;\n  mutate(region = reorder(region, dollars_per_day, FUN = median)) |&gt;\n  ggplot(aes(dollars_per_day, region)) +\n  geom_point() +\n  scale_x_continuous(trans = \"log2\")  \n\n\n\n\n\n\n\nWe can already see that there is indeed a “west versus the rest” dichotomy: we see two clear groups, with the rich group composed of North America, Northern and Western Europe, New Zealand and Australia. We define groups based on this observation:\n\ngapminder &lt;- gapminder |&gt; \n  mutate(group = case_when(\n    region %in% c(\"Western Europe\", \"Northern Europe\",\"Southern Europe\", \n                    \"Northern America\", \n                  \"Australia and New Zealand\") ~ \"West\",\n    region %in% c(\"Eastern Asia\", \"South-Eastern Asia\") ~ \"East Asia\",\n    region %in% c(\"Caribbean\", \"Central America\", \n                  \"South America\") ~ \"Latin America\",\n    continent == \"Africa\" & \n      region != \"Northern Africa\" ~ \"Sub-Saharan\",\n    TRUE ~ \"Others\"))\n\nWe turn this group variable into a factor to control the order of the levels:\n\ngapminder &lt;- gapminder |&gt; \n  mutate(group = factor(group, levels = c(\"Others\", \"Latin America\", \n                                          \"East Asia\", \"Sub-Saharan\",\n                                          \"West\")))\n\nIn the next section we demonstrate how to visualize and compare distributions across groups.\n\n10.7.1 Boxplots\nThe exploratory data analysis above has revealed two characteristics about average income distribution in 1970. Using a histogram, we found a bimodal distribution with the modes relating to poor and rich countries. We now want to compare the distribution across these five groups to confirm the “west versus the rest” dichotomy. The number of points in each category is large enough that a summary plot may be useful. We could generate five histograms or five density plots, but it may be more practical to have all the visual summaries in one plot. We therefore start by stacking boxplots next to each other. Note that we add the layer theme(axis.text.x = element_text(angle = 90, hjust = 1)) to turn the group labels vertical, since they do not fit if we show them horizontally, and remove the axis label to make space.\n\np &lt;- gapminder |&gt; \n  filter(year == past_year & !is.na(gdp)) |&gt;\n  ggplot(aes(group, dollars_per_day)) +\n  geom_boxplot() +\n  scale_y_continuous(trans = \"log2\") +\n  xlab(\"\") +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1)) \np\n\n\n\n\n\n\n\nBoxplots have the limitation that by summarizing the data into five numbers, we might miss important characteristics of the data. One way to avoid this is by showing the data.\n\np + geom_point(alpha = 0.5)\n\n\n\n\n\n\n\n\n10.7.2 Ridge plots\nShowing each individual point does not always reveal important characteristics of the distribution. Although not the case here, when the number of data points is so large that there is over-plotting, showing the data can be counterproductive. Boxplots help with this by providing a five-number summary, but this has limitations too. For example, boxplots will not permit us to discover bimodal distributions. To see this, note that the two plots below are summarizing the same dataset:\n\n\n\n\n\n\n\n\nIn cases in which we are concerned that the boxplot summary is too simplistic, we can show stacked smooth densities or histograms. We refer to these as ridge plots. Because we are used to visualizing densities with values in the x-axis, we stack them vertically. Also, because more space is needed in this approach, it is convenient to overlay them. The package ggridges provides a convenient function for doing this. Here is the income data shown above with boxplots but with a ridge plot.\n\nlibrary(ggridges)\np &lt;- gapminder |&gt; \n  filter(year == past_year & !is.na(dollars_per_day)) |&gt;\n  ggplot(aes(dollars_per_day, group)) + \n  scale_x_continuous(trans = \"log2\") \np  + geom_density_ridges() \n\n\n\n\n\n\n\nNote that we have to invert the x and y used for the boxplot. A useful geom_density_ridges parameter is scale, which lets you determine the amount of overlap, with scale = 1 meaning no overlap and larger values resulting in more overlap.\nIf the number of data points is small enough, we can add them to the ridge plot using the following code:\n\np + geom_density_ridges(jittered_points = TRUE)\n\n\n\n\n\n\n\nBy default, the height of the points is jittered and should not be interpreted in any way. To show data points, but without using jitter we can use the following code to add what is referred to as a rug representation of the data.\n\np + geom_density_ridges(jittered_points = TRUE, \n                        position = position_points_jitter(height = 0),\n                        point_shape = '|', point_size = 3, \n                        point_alpha = 1, alpha = 0.7)\n\n\n\n\n\n\n\n\n10.7.3 Example: 1970 versus 2010 income distributions\nData exploration clearly shows that in 1970 there was a “west versus the rest” dichotomy. But does this dichotomy persist? Let’s use facet_grid and see how the distributions have changed. To start, we will focus on two groups: the west and the rest. We make four histograms. We make this plot only for countries with data in both 1970 and 2010. Note that several countries were founded after 1970, for example, the Soviet Union divided into several countries during the 1990s. We also note tat that data was available for more countries in 2010.\nWe therefore make the plot only for countries with data in both years:\n\npast_year &lt;- 1970\npresent_year &lt;- 2010\nyears &lt;- c(past_year, present_year)\ncountry_list &lt;- gapminder |&gt; \n  filter(year %in% c(present_year, past_year)) |&gt;\n  group_by(country) |&gt;\n  summarize(n = sum(!is.na(dollars_per_day)), .groups = \"drop\") |&gt;\n  filter(n == 2) |&gt;\n  pull(country)\n\nThese 108 account for 86% of the world population, so this subset should be representative. We can compare the distributions using this code:\n\ngapminder |&gt; \n  filter(year %in% years & country %in% country_list) |&gt;\n  mutate(west = ifelse(group == \"West\", \"West\", \"Developing\")) |&gt;\n  ggplot(aes(dollars_per_day)) +\n  geom_histogram(binwidth = 1, color = \"black\") +\n  scale_x_continuous(trans = \"log2\") + \n  facet_grid(year ~ west)\n\n\n\n\n\n\n\nWe now see that the rich countries have become a bit richer, but percentage-wise, the poor countries appear to have improved more. In particular, we see that the proportion of developing countries earning more than $16 a day increased substantially.\nTo see which specific regions improved the most, we can remake the boxplots we made above, but now adding the year 2010 and then using facet to compare the two years.\n\ngapminder |&gt; \n  filter(year %in% years & country %in% country_list) |&gt;\n  ggplot(aes(group, dollars_per_day)) +\n  geom_boxplot() +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +\n  scale_y_continuous(trans = \"log2\") +\n  xlab(\"\") +\n  facet_grid(. ~ year)\n\n\n\n\n\n\n\nHere, we pause to introduce another powerful ggplot2 feature. Because we want to compare each region before and after, it would be convenient to have the 1970 boxplot next to the 2010 boxplot for each region. In general, comparisons are easier when data are plotted next to each other.\nSo instead of faceting, we keep the data from each year together and ask to color (or fill) them depending on the year. Note that groups are automatically separated by year and each pair of boxplots drawn next to each other. Because year is a number, we turn it into a factor since ggplot2 automatically assigns a color to each category of a factor. Note that we have to convert the year columns from numeric to factor.\n\ngapminder |&gt; \n  filter(year %in% years & country %in% country_list) |&gt;\n  mutate(year = factor(year)) |&gt;\n  ggplot(aes(group, dollars_per_day, fill = year)) +\n  geom_boxplot() +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +\n  scale_y_continuous(trans = \"log2\") +\n  xlab(\"\") \n\n\n\n\n\n\n\nThe previous data exploration suggested that the income gap between rich and poor countries has narrowed considerably during the last 40 years. We used a series of histograms and boxplots to see this. We suggest a succinct way to convey this message with just one plot.\nLet’s start by noting that density plots for income distribution in 1970 and 2010 deliver the message that the gap is closing:\n\ngapminder |&gt; \n  filter(year %in% years & country %in% country_list) |&gt;\n  ggplot(aes(dollars_per_day)) +\n  geom_density(fill = \"grey\") + \n  scale_x_continuous(trans = \"log2\") + \n  facet_grid(. ~ year)\n\n\n\n\n\n\n\nIn the 1970 plot, we see two clear modes: poor and rich countries. In 2010, it appears that some of the poor countries have shifted towards the right, closing the gap.\nThe next message we need to convey is that the reason for this change in distribution is that several poor countries became richer, rather than some rich countries becoming poorer. To do this, we can assign a color to the groups we identified during data exploration.\nHowever, because when we overlay two densities, the default is to have the area under the distribution curve add up to 1 for each group, regardless of the size of each group, we first need to learn how to make these smooth densities in a way that preserves information on the number of countries in each group. To do this, we will need to learn to access computed variables with geom_density function.\n\n10.7.4 Accessing computed variables\nTo have the areas of these densities be proportional to the size of the groups, we can simply multiply the y-axis values by the size of the group. From the geom_density help file, we see that the functions compute a variable called count that does exactly this. We want this variable to be on the y-axis rather than the density.\nIn ggplot2, we access these variables using the function after_stat. We will therefore use the following mapping:\n\naes(x = dollars_per_day, y = after_stat(count))\n\nWe can now create the desired plot by simply changing the mapping in the previous code chunk. We will also expand the limits of the x-axis.\n\np &lt;- gapminder |&gt; \n  filter(year %in% years & country %in% country_list) |&gt;\n  mutate(group = ifelse(group == \"West\", \"West\", \"Developing\")) |&gt;\n  ggplot(aes(dollars_per_day, y = after_stat(count), fill = group)) +\n  scale_x_continuous(trans = \"log2\", limits = c(0.125, 300))\np + geom_density(alpha = 0.2) + facet_grid(year ~ .)\n\n\n\n\n\n\n\nIf we want the densities to be smoother, we use the bw argument so that the same bandwidth is used in each density. We selected 0.75 after trying out several values.\n\np + geom_density(alpha = 0.2, bw = 0.75) + facet_grid(year ~ .)\n\n\n\n\n\n\n\nThis plot now shows what is happening very clearly. The developing world distribution is changing. A third mode appears consisting of the countries that most narrowed the gap.\nTo visualize if any of the groups defined above are driving this we can quickly make a ridge plot:\n\ngapminder |&gt; \n  filter(year %in% years & !is.na(dollars_per_day)) |&gt;\n  ggplot(aes(dollars_per_day, group)) + \n  scale_x_continuous(trans = \"log2\") + \n  geom_density_ridges(bandwidth = 1.5) +\n  facet_grid(. ~ year)\n\n\n\n\n\n\n\nAnother way to achieve this is by stacking the densities on top of each other:\n\ngapminder |&gt; \n    filter(year %in% years & country %in% country_list) |&gt;\n  group_by(year) |&gt;\n  mutate(weight = population/sum(population)*2) |&gt;\n  ungroup() |&gt;\n  ggplot(aes(dollars_per_day, fill = group)) +\n  scale_x_continuous(trans = \"log2\", limits = c(0.125, 300)) + \n  geom_density(alpha = 0.2, bw = 0.75, position = \"stack\") + \n  facet_grid(year ~ .) \n\n\n\n\n\n\n\nHere we can clearly see how the distributions for East Asia, Latin America, and others shift markedly to the right. While Sub-Saharan Africa remains stagnant.\nNotice that we order the levels of the group so that the West’s density is plotted first, then Sub-Saharan Africa. Having the two extremes plotted first allows us to see the remaining bimodality better.\n\n10.7.5 Weighted densities\nAs a final point, we note that these distributions weigh every country the same. So if most of the population is improving, but living in a very large country, such as China, we might not appreciate this. We can actually weight the smooth densities using the weight mapping argument. The plot then looks like this:\n\n\n\n\n\n\n\n\nThis particular figure shows very clearly how the income distribution gap is closing with most of the poor remaining in Sub-Saharan Africa."
  },
  {
    "objectID": "dataviz/dataviz-in-practice.html#case-study-2-the-ecological-fallacy",
    "href": "dataviz/dataviz-in-practice.html#case-study-2-the-ecological-fallacy",
    "title": "10  Data visualization in practice",
    "section": "\n10.8 Case study 2: the ecological fallacy",
    "text": "10.8 Case study 2: the ecological fallacy\nThroughout this section, we have been comparing regions of the world. We have seen that, on average, some regions do better than others. In this section, we focus on describing the importance of variability within the groups when examining the relationship between a country’s infant mortality rates and average income.\nWe define a few more regions and compare the averages across regions:\n\n\n\n\n\n\n\n\nThe relationship between these two variables is almost perfectly linear and the graph shows a dramatic difference. While in the West less than 0.5% of infants die, in Sub-Saharan Africa the rate is higher than 6%!\nNote that the plot uses a new transformation, the logistic transformation.\n\n10.8.1 Logistic transformation\nThe logistic or logit transformation for a proportion or rate \\(p\\) is defined as:\n\\[f(p) = \\log \\left( \\frac{p}{1-p} \\right)\\]\nWhen \\(p\\) is a proportion or probability, the quantity that is being logged, \\(p/(1-p)\\), is called the odds. In this case \\(p\\) is the proportion of infants that survived. The odds tell us how many more infants are expected to survive than to die. The log transformation makes this symmetric. If the rates are the same, then the log odds is 0. Fold increases or decreases turn into positive and negative increments, respectively.\nThis scale is useful when we want to highlight differences near 0 or 1. For survival rates this is important because a survival rate of 90% is unacceptable, while a survival of 99% is relatively good. We would much prefer a survival rate closer to 99.9%. We want our scale to highlight these difference and the logit does this. Note that 99.9/0.1 is about 10 times bigger than 99/1 which is about 10 times larger than 90/10. By using the log, these fold changes turn into constant increases.\n\n10.8.2 Show the data\nNow, back to our plot. Based on the plot above, do we conclude that a country with a low income is destined to have low survival rate? Do we conclude that survival rates in Sub-Saharan Africa are all lower than in Southern Asia, which in turn are lower than in the Pacific Islands, and so on?\nJumping to this conclusion based on a plot showing averages is referred to as the ecological fallacy. The almost perfect relationship between survival rates and income is only observed for the averages at the region level. Once we show all the data, we see a somewhat more complicated story:\n\n\n\n\n\n\n\n\nSpecifically, we see that there is a large amount of variability. We see that countries from the same regions can be quite different and that countries with the same income can have different survival rates. For example, while on average Sub-Saharan Africa had the worse health and economic outcomes, there is wide variability within that group. Mauritius and Botswana are doing better than Angola and Sierra Leone, with Mauritius comparable to Western countries."
  },
  {
    "objectID": "dataviz/dataviz-in-practice.html#sec-vaccines",
    "href": "dataviz/dataviz-in-practice.html#sec-vaccines",
    "title": "10  Data visualization in practice",
    "section": "\n10.9 Case study 3: vaccines and infectious diseases",
    "text": "10.9 Case study 3: vaccines and infectious diseases\nVaccines have helped save millions of lives. In the 19th century, before herd immunization was achieved through vaccination programs, deaths from infectious diseases, such as smallpox and polio, were common. However, today vaccination programs have become somewhat controversial despite all the scientific evidence for their importance.\nThe controversy started with a paper5 published in 1988 and led by Andrew Wakefield claiming there was a link between the administration of the measles, mumps, and rubella (MMR) vaccine and the appearance of autism and bowel disease. Despite much scientific evidence contradicting this finding, sensationalist media reports and fear-mongering from conspiracy theorists led parts of the public into believing that vaccines were harmful. As a result, many parents ceased to vaccinate their children. This dangerous practice can be potentially disastrous given that the Centers for Disease Control (CDC) estimates that vaccinations will prevent more than 21 million hospitalizations and 732,000 deaths among children born in the last 20 years (see Benefits from Immunization during the Vaccines for Children Program Era — United States, 1994-2013, MMWR6). The 1988 paper has since been retracted and Andrew Wakefield was eventually “struck off the UK medical register, with a statement identifying deliberate falsification in the research published in The Lancet, and was thereby barred from practicing medicine in the UK.” (source: Wikipedia7). Yet misconceptions persist, in part due to self-proclaimed activists who continue to disseminate misinformation about vaccines.\nEffective communication of data is a strong antidote to misinformation and fear-mongering. Earlier we used an example provided by a Wall Street Journal article8 showing data related to the impact of vaccines on battling infectious diseases. Here we reconstruct that example.\n\n10.9.1 Data\nThe data used for these plots were collected, organized, and distributed by the Tycho Project9. They include weekly reported counts for seven diseases from 1928 to 2011, from all fifty states. We include the yearly totals in the dslabs package:\n\nlibrary(tidyverse)\nlibrary(RColorBrewer)\nlibrary(dslabs)\nnames(us_contagious_diseases)\n#&gt; [1] \"disease\"         \"state\"           \"year\"           \n#&gt; [4] \"weeks_reporting\" \"count\"           \"population\"\n\nWe create a temporary object dat that stores only the measles data, includes a per 100,000 rate, orders states by average value of disease and removes Alaska and Hawaii since they only became states in the late 1950s. Note that there is a weeks_reporting column that tells us for how many weeks of the year data was reported. We have to adjust for that value when computing the rate.\n\nthe_disease &lt;- \"Measles\"\ndat &lt;- us_contagious_diseases |&gt;\n  filter(!state %in% c(\"Hawaii\",\"Alaska\") & disease == the_disease) |&gt;\n  mutate(rate = count / population * 10000 * 52 / weeks_reporting) |&gt; \n  mutate(state = reorder(state, ifelse(year &lt;= 1963, rate, NA), \n                         median, na.rm = TRUE)) \n\n\n10.9.2 Trend plots and heatmaps\nWe can now easily plot disease rates per year. Here are the measles data from California:\n\ndat |&gt; filter(state == \"California\" & !is.na(rate)) |&gt;\n  ggplot(aes(year, rate)) +\n  geom_line() + \n  ylab(\"Cases per 10,000\")  + \n  geom_vline(xintercept = 1963, col = \"blue\")\n\n\n\n\n\n\n\nWe add a vertical line at 1963 since this is when the vaccine was introduced [Control, Centers for Disease; Prevention (2014). CDC health information for international travel 2014 (the yellow book). p. 250. ISBN 9780199948505].\nNow can we show data for all states in one plot? We have three variables to show: year, state, and rate. In the WSJ figure, they use the x-axis for year, the y-axis for state, and color hue to represent rates. However, the color scale they use, which goes from yellow to blue to green to orange to red, can be improved.\nIn our example, we want to use a sequential palette since there is no meaningful center, just low and high rates.\nWe use the geometry geom_tile to tile the region with colors representing disease rates. We use a square root transformation to avoid having the really high counts dominate the plot. Notice that missing values are shown in grey. Note that once a disease was pretty much eradicated, some states stopped reporting cases all together. This is why we see so much grey after 1980.\n\ndat |&gt; ggplot(aes(year, state, fill = rate)) +\n  geom_tile(color = \"grey50\") +\n  scale_x_continuous(expand = c(0,0)) +\n  scale_fill_gradientn(colors = brewer.pal(9, \"Reds\"), trans = \"sqrt\") +\n  geom_vline(xintercept = 1963, col = \"blue\") +\n  theme_minimal() +  \n  theme(panel.grid = element_blank(), \n        legend.position = \"bottom\", \n        text = element_text(size = 8)) +\n  labs(title = the_disease, x = \"\", y = \"\")\n\n\n\n\n\n\n\nThis plot makes a very striking argument for the contribution of vaccines. However, one limitation of this plot is that it uses color to represent quantity, which we earlier explained makes it harder to know exactly how high values are going. Position and lengths are better cues. If we are willing to lose state information, we can make a version of the plot that shows the values with position. We can also show the average for the US, which we compute like this:\n\navg &lt;- us_contagious_diseases |&gt;\n  filter(disease == the_disease) |&gt; group_by(year) |&gt;\n  summarize(us_rate = sum(count, na.rm = TRUE) / \n              sum(population, na.rm = TRUE) * 10000)\n\nNow to make the plot we simply use the geom_line geometry:\n\ndat |&gt; \n  filter(!is.na(rate)) |&gt;\n    ggplot() +\n  geom_line(aes(year, rate, group = state),  color = \"grey50\", \n            show.legend = FALSE, alpha = 0.2, size = 1) +\n  geom_line(mapping = aes(year, us_rate),  data = avg, size = 1) +\n  scale_y_continuous(trans = \"sqrt\", breaks = c(5, 25, 125, 300)) + \n  ggtitle(\"Cases per 10,000 by state\") + \n  xlab(\"\") + ylab(\"\") +\n  geom_text(data = data.frame(x = 1955, y = 50), \n            mapping = aes(x, y, label = \"US average\"), \n            color = \"black\") + \n  geom_vline(xintercept = 1963, col = \"blue\")\n#&gt; Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n#&gt; ℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\nIn theory, we could use color to represent the categorical value state, but it is hard to pick 50 distinct colors."
  },
  {
    "objectID": "dataviz/dataviz-in-practice.html#exercises",
    "href": "dataviz/dataviz-in-practice.html#exercises",
    "title": "10  Data visualization in practice",
    "section": "\n10.10 Exercises",
    "text": "10.10 Exercises\n\nReproduce the image plot we previously made but for smallpox. For this plot, do not include years in which cases were not reported in 10 or more weeks.\nNow reproduce the time series plot we previously made, but this time following the instructions of the previous question for smallpox.\nFor the state of California, make a time series plot showing rates for all diseases. Include only years with 10 or more weeks reporting. Use a different color for each disease.\nNow do the same for the rates for the US. Hint: compute the US rate by using summarize: the total divided by total population."
  },
  {
    "objectID": "dataviz/dataviz-in-practice.html#footnotes",
    "href": "dataviz/dataviz-in-practice.html#footnotes",
    "title": "10  Data visualization in practice",
    "section": "",
    "text": "https://en.wikipedia.org/wiki/Hans_Rosling↩︎\nhttp://www.gapminder.org/↩︎\nhttps://www.ted.com/talks/hans_rosling_reveals_new_insights_on_poverty?language=en↩︎\nhttps://www.ted.com/talks/hans_rosling_shows_the_best_stats_you_ve_ever_seen↩︎\nhttp://www.thelancet.com/journals/lancet/article/PIIS0140-6736(97)11096-0/abstract↩︎\nhttps://www.cdc.gov/mmwr/preview/mmwrhtml/mm6316a4.htm↩︎\nhttps://en.wikipedia.org/wiki/Andrew_Wakefield↩︎\nhttp://graphics.wsj.com/infectious-diseases-and-vaccines/↩︎\nhttp://www.tycho.pitt.edu/↩︎"
  },
  {
    "objectID": "wrangling/intro-to-wrangling.html",
    "href": "wrangling/intro-to-wrangling.html",
    "title": "Data Wrangling",
    "section": "",
    "text": "The datasets used in this book have been made available to you as R objects, specifically as data frames. The US murders data, the reported heights data, and the Gapminder data were all data frames. These datasets come included in the dslabs package and we loaded them using the data function. Furthermore, we have made the data available in what is referred to as tidy form. The tidyverse packages and functions assume that the data is tidy and this assumption is a big part of the reason these packages work so well together.\nHowever, very rarely in a data science project is data easily available as part of a package. We did quite a bit of work “behind the scenes” to get the original raw data into the tidy tables you worked with. Much more typical is for the data to be in a file, a database, or extracted from a document, including web pages, tweets, or PDFs. In these cases, the first step is to import the data into R and, when using the tidyverse, tidy the data. This initial step in the data analysis process usually involves several, often complicated, steps to convert data from its raw form to the tidy form that greatly facilitates the rest of the analysis. We refer to this process as data wrangling.\nHere we cover several common steps of the data wrangling process including tidying data, string processing, html parsing, working with dates and times, and text analysis. Rarely are all these wrangling steps necessary in a single analysis, but as a data analysts you will likely face them all at some point. Some of the examples we use to demonstrate data wrangling techniques are based on the work we did to convert raw data into the tidy datasets provided by the dslabs package and used in the book as examples."
  },
  {
    "objectID": "wrangling/reshaping-data.html#pivot_longer",
    "href": "wrangling/reshaping-data.html#pivot_longer",
    "title": "11  Reshaping data",
    "section": "\n11.1 pivot_longer\n",
    "text": "11.1 pivot_longer\n\nOne of the most used functions in the tidyr package is pivot_longer, which is useful for converting wide data into tidy data.\nAs with most tidyverse functions, the pivot_longer function’s first argument is the data frame that will be converted. Here we want to reshape the wide_data dataset so that each row represents a fertility observation, which implies we need three columns to store the year, country, and the observed value. In its current form, data from different years are in different columns with the year values stored in the column names. Through the names_to and values_to argument we will tell pivot_longer the column names we want to assign to the columns containing the current column names and observations, respectively. The default names are name and value, which are often usable choices. In this case a better choice for these two arguments would be year and fertility. Note that nowhere in the data file does it tell us this is fertility data. Instead, we deciphered this from the file name. Through cols, the second argument we specify the columns containing observed values; these are the columns that will be pivoted. The default is to pivot all columns so, in most cases, we have to specify the columns. In our example we want columns 1960, 1961 up to 2015.\nThe code to pivot the fertility data therefore looks like this:\n\nnew_tidy_data &lt;- wide_data |&gt;\n  pivot_longer(`1960`:`2015`, names_to = \"year\", values_to = \"fertility\")\n\nWe can see that the data have been converted to tidy format with columns year and fertility:\n\nhead(new_tidy_data)\n#&gt; # A tibble: 6 × 3\n#&gt;   country year  fertility\n#&gt;   &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt;\n#&gt; 1 Germany 1960       2.41\n#&gt; 2 Germany 1961       2.44\n#&gt; 3 Germany 1962       2.47\n#&gt; 4 Germany 1963       2.49\n#&gt; 5 Germany 1964       2.49\n#&gt; # ℹ 1 more row\n\nand that each year resulted in two rows since we have two countries and this column was not pivoted. A somewhat quicker way to write this code is to specify which column will not include in the pivot, rather than all the columns that will be pivoted:\n\nnew_tidy_data &lt;- wide_data |&gt;\n  pivot_longer(-country, names_to = \"year\", values_to = \"fertility\")\n\nThe new_tidy_data object looks like the original tidy_data we defined this way\n\ntidy_data &lt;- gapminder |&gt; \n  filter(country %in% c(\"South Korea\", \"Germany\") & !is.na(fertility)) |&gt;\n  select(country, year, fertility)\n\nwith just one minor difference. Can you spot it? Look at the data type of the year column. The pivot_longer function assumes that column names are characters. So we need a bit more wrangling before we are ready to make a plot. We need to convert the year column to be numbers:\n\nnew_tidy_data &lt;- wide_data |&gt;\n  pivot_longer(-country, names_to = \"year\", values_to = \"fertility\") |&gt;\n  mutate(year = as.integer(year))\n\nNow that the data is tidy, we can use this relatively simple ggplot code:\n\nnew_tidy_data |&gt; ggplot(aes(year, fertility, color = country)) + geom_point()"
  },
  {
    "objectID": "wrangling/reshaping-data.html#pivot_wider",
    "href": "wrangling/reshaping-data.html#pivot_wider",
    "title": "11  Reshaping data",
    "section": "\n11.2 pivot_wider\n",
    "text": "11.2 pivot_wider\n\nAs we will see in later examples, it is sometimes useful for data wrangling purposes to convert tidy data into wide data. We often use this as an intermediate step in tidying up data. The pivot_wider function is basically the inverse of pivot_longer. The first argument is for the data, but since we are using the pipe, we don’t show it. The names_from argument tells pivot_wider which variable will be used as the column names. The values_from argument specifies which variable to use to fill out the cells.\n\nnew_wide_data &lt;- new_tidy_data |&gt; \n  pivot_wider(names_from = year, values_from = fertility)\nselect(new_wide_data, country, `1960`:`1967`)\n#&gt; # A tibble: 2 × 9\n#&gt;   country     `1960` `1961` `1962` `1963` `1964` `1965` `1966` `1967`\n#&gt;   &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1 Germany       2.41   2.44   2.47   2.49   2.49   2.48   2.44   2.37\n#&gt; 2 South Korea   6.16   5.99   5.79   5.57   5.36   5.16   4.99   4.85\n\nSimilar to pivot_wider, names_from and values_from default to name and value."
  },
  {
    "objectID": "wrangling/reshaping-data.html#sec-separate",
    "href": "wrangling/reshaping-data.html#sec-separate",
    "title": "11  Reshaping data",
    "section": "\n11.3 Separating variables",
    "text": "11.3 Separating variables\nThe data wrangling shown above was simple compared to what is usually required. In our example spreadsheet files, we include an illustration that is slightly more complicated. It contains two variables: life expectancy and fertility. However, the way it is stored is not tidy and, as we will explain, not optimal.\n\npath &lt;- system.file(\"extdata\", package = \"dslabs\")\n\nfilename &lt;- \"life-expectancy-and-fertility-two-countries-example.csv\"\nfilename &lt;-  file.path(path, filename)\n\nraw_dat &lt;- read_csv(filename)\nselect(raw_dat, 1:5)\n#&gt; # A tibble: 2 × 5\n#&gt;   country     `1960_fertility` `1960_life_expectancy` `1961_fertility`\n#&gt;   &lt;chr&gt;                  &lt;dbl&gt;                  &lt;dbl&gt;            &lt;dbl&gt;\n#&gt; 1 Germany                 2.41                   69.3             2.44\n#&gt; 2 South Korea             6.16                   53.0             5.99\n#&gt; # ℹ 1 more variable: `1961_life_expectancy` &lt;dbl&gt;\n\nFirst, note that the data is in wide format. Second, notice that this table includes values for two variables, fertility and life expectancy, with the column name encoding which column represents which variable. Encoding information in the column names is not recommended but, unfortunately, it is quite common. We will put our wrangling skills to work to extract this information and store it in a tidy fashion.\nWe can start the data wrangling with the pivot_longer function, but we should no longer use the column name year for the new column since it also contains the variable type. We will call it name, the default, for now:\n\ndat &lt;- raw_dat |&gt; pivot_longer(-country)\nhead(dat)\n#&gt; # A tibble: 6 × 3\n#&gt;   country name                 value\n#&gt;   &lt;chr&gt;   &lt;chr&gt;                &lt;dbl&gt;\n#&gt; 1 Germany 1960_fertility        2.41\n#&gt; 2 Germany 1960_life_expectancy 69.3 \n#&gt; 3 Germany 1961_fertility        2.44\n#&gt; 4 Germany 1961_life_expectancy 69.8 \n#&gt; 5 Germany 1962_fertility        2.47\n#&gt; # ℹ 1 more row\n\nThe result is not exactly what we refer to as tidy since each observation is associated with two, not one, rows. We want to have the values from the two variables, fertility and life expectancy, in two separate columns. The first challenge to achieve this is to separate the name column into the year and the variable type. Notice that the entries in this column separate the year from the variable name with an underscore:\n\ndat$name[1:5]\n#&gt; [1] \"1960_fertility\"       \"1960_life_expectancy\" \"1961_fertility\"      \n#&gt; [4] \"1961_life_expectancy\" \"1962_fertility\"\n\nEncoding multiple variables in a column name is such a common problem that the tidyr package includes function to separate these columns into two or more. The separate_wider_delim function takes three arguments: the name of the column to be separated, the names to be used for the new columns, and the character that separates the variables. So, a first attempt at separating the variable name from the year might be:\n\ndat |&gt; separate_wider_delim(name, delim = \"_\", names = c(\"year\", \"name\"))\n\nHowever, this line of code will give an error. This is because the life expectancy names have three string separated by _ and the fertility names have two. This is a common problem so the separate_wider_delim function has arguments too_few and too_many to handle these situations. We see in the help file that the option too_many = merge will merge together any additional pieces. The following line does what we want:\n\ndat |&gt; separate_wider_delim(name, delim = \"_\", names = c(\"year\", \"name\"), too_many = \"merge\")\n#&gt; # A tibble: 224 × 4\n#&gt;   country year  name            value\n#&gt;   &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;           &lt;dbl&gt;\n#&gt; 1 Germany 1960  fertility        2.41\n#&gt; 2 Germany 1960  life_expectancy 69.3 \n#&gt; 3 Germany 1961  fertility        2.44\n#&gt; 4 Germany 1961  life_expectancy 69.8 \n#&gt; 5 Germany 1962  fertility        2.47\n#&gt; # ℹ 219 more rows\n\nBut we are not done yet. We need to create a column for each variable and change year to a number. As we learned, the pivot_wider function can do this:\n\ndat |&gt; \n  separate_wider_delim(name, delim = \"_\", names = c(\"year\", \"name\"), too_many = \"merge\") |&gt;\n  pivot_wider() |&gt;\n  mutate(year = as.integer(year))\n#&gt; # A tibble: 112 × 4\n#&gt;   country  year fertility life_expectancy\n#&gt;   &lt;chr&gt;   &lt;int&gt;     &lt;dbl&gt;           &lt;dbl&gt;\n#&gt; 1 Germany  1960      2.41            69.3\n#&gt; 2 Germany  1961      2.44            69.8\n#&gt; 3 Germany  1962      2.47            70.0\n#&gt; 4 Germany  1963      2.49            70.1\n#&gt; 5 Germany  1964      2.49            70.7\n#&gt; # ℹ 107 more rows\n\nThe data is now in tidy format with one row for each observation with three variables: year, fertility, and life expectancy.\nThree related function are separate_wider_position, separate_wider_regex, and unite. separate_wider_position takes a width instead of delimiter. separate_wider_regex, described in Section 16.4.13, provides much more control over how we separate and what we keep. The untie function can be tought of as the inverse of the separate function: it combines two columns into one."
  },
  {
    "objectID": "wrangling/reshaping-data.html#the-janitor-package",
    "href": "wrangling/reshaping-data.html#the-janitor-package",
    "title": "11  Reshaping data",
    "section": "\n11.4 The janitor package",
    "text": "11.4 The janitor package\nThe janitor package includes function for some of the most common steps needed to wrangle data. These are particularly useful as these tasks that are often repetitive and time-consuming. Key features include functions for examining and cleaning column names, removing empty or duplicate rows, and converting data types. It also offers capabilities to generate frequency tables and perform cross tabulations with ease. The package is designed to work seamlessly with the tidyverse. Here we show four examples.\nSpreadsheets often use names that are not compatible with programming. The most common problem is column names with spaces. The clean_names() function attempts to fix this and other common problems. By default it forces varaible names to be lower case and with underscore instead of space. In this example we change the variable names of the object dat created in the previous section and then demonstrate how this function works:\n\nlibrary(janitor)\n#&gt; \n#&gt; Attaching package: 'janitor'\n#&gt; The following objects are masked from 'package:stats':\n#&gt; \n#&gt;     chisq.test, fisher.test\nnames(dat) &lt;- c(\"Country\", \"Year\", \"Fertility\",  \"Life Expectancy\")\n#&gt; Warning: The `value` argument of `names&lt;-` must have the same length as `x` as\n#&gt; of tibble 3.0.0.\nclean_names(dat) |&gt; names()\n#&gt; [1] \"country\"   \"year\"      \"fertility\"\n\nAnother very common challenging reality is that numeric matrices are saved in spreadsheets and include a column with characters defining the row names. To fix this we have to remove the first column, but only after assigning them as vector that we will use to define rownames after converting the data frame to a matrix. The function column_to_rows does these operations for us and all we have to do is specify which column contains the rownames:\n\ndata.frame(ids = letters[1:3], x = 1:3, y = 4:6) |&gt; \n  column_to_rownames(\"ids\") |&gt;\n  as.matrix() \n#&gt;   x y\n#&gt; a 1 4\n#&gt; b 2 5\n#&gt; c 3 6\n\nAnother common challenge is that spreadsheets include the columnnames as a first row. To quickly fix this we can `row_to_names``:\n\nx &lt;- read.csv(file.path(path, \"murders.csv\"), header = FALSE) |&gt; row_to_names(1)\nnames(x)\n#&gt; [1] \"state\"      \"abb\"        \"region\"     \"population\" \"total\"\n\nOur final example relates to finding duplicates. A very common error in the creation of speadsheets is that rows are duplicated. The get_dups function finds and reports duplicate records. By default it considers all varaibles, but you can also specificy which ones.\n\nx &lt;- bind_rows(x, x[1,])\nget_dupes(x)\n#&gt; No variable names specified - using all columns.\n#&gt;     state abb region population total dupe_count\n#&gt; 1 Alabama  AL  South    4779736   135          2\n#&gt; 2 Alabama  AL  South    4779736   135          2"
  },
  {
    "objectID": "wrangling/reshaping-data.html#exercises",
    "href": "wrangling/reshaping-data.html#exercises",
    "title": "11  Reshaping data",
    "section": "\n11.5 Exercises",
    "text": "11.5 Exercises\n1. Run the following command to define the co2_wide object:\n\nco2_wide &lt;- data.frame(matrix(co2, ncol = 12, byrow = TRUE)) |&gt; \n  setNames(1:12) |&gt;\n  mutate(year = as.character(1959:1997))\n\nUse the pivot_longer function to wrangle this into a tidy dataset. Call the column with the CO2 measurements co2 and call the month column month. Call the resulting object co2_tidy.\n2. Plot CO2 versus month with a different curve for each year using this code:\n\nco2_tidy |&gt; ggplot(aes(month, co2, color = year)) + geom_line()\n\nIf the expected plot is not made, it is probably because co2_tidy$month is not numeric:\n\nclass(co2_tidy$month)\n\nRewrite your code to make sure the month column is numeric. Then make the plot.\n3. What do we learn from this plot?\n\nCO2 measures increase monotonically from 1959 to 1997.\nCO2 measures are higher in the summer and the yearly average increased from 1959 to 1997.\nCO2 measures appear constant and random variability explains the differences.\nCO2 measures do not have a seasonal trend.\n\n4. Now load the admissions data set, which contains admission information for men and women across six majors and keep only the admitted percentage column:\n\nload(admissions)\ndat &lt;- admissions |&gt; select(-applicants)\n\nIf we think of an observation as a major, and that each observation has two variables (men admitted percentage and women admitted percentage) then this is not tidy. Use the pivot_wider function to wrangle into tidy shape: one row for each major.\n5. Now we will try a more advanced wrangling challenge. We want to wrangle the admissions data so that for each major we have 4 observations: admitted_men, admitted_women, applicants_men and applicants_women. The trick we perform here is actually quite common: first use pivot_longer to generate an intermediate data frame and then pivot_wider to obtain the tidy data we want. We will go step by step in this and the next two exercises.\nUse the pivot_longer function to create a tmp data.frame with a column containing the type of observation admitted or applicants. Call the new columns name and value.\n6. Now you have an object tmp with columns major, gender, name and value. Note that if you combine the name and gender, we get the column names we want: admitted_men, admitted_women, applicants_men and applicants_women. Use the function unite to create a new column called column_name.\n7. Now use the pivot_wider function to generate the tidy data with four variables for each major.\n8. Now use the pipe to write a line of code that turns admissions to the table produced in the previous exercise."
  },
  {
    "objectID": "wrangling/joining-tables.html#sec-joins",
    "href": "wrangling/joining-tables.html#sec-joins",
    "title": "\n12  Joining tables\n",
    "section": "\n12.1 Joins",
    "text": "12.1 Joins\nThe join functions in the dplyr package make sure that the tables are combined so that matching rows are together. If you know SQL, you will see that the approach and syntax is very similar. The general idea is that one needs to identify one or more columns that will serve to match the two tables. Then a new table with the combined information is returned. Notice what happens if we join the two tables above by state using left_join (we will remove the others column and rename electoral_votes so that the tables fit on the page):\n\ntab &lt;- left_join(murders, results_us_election_2016, by = \"state\") |&gt;\n  select(-others) |&gt; rename(ev = electoral_votes)\nhead(tab)\n#&gt;        state abb region population total ev clinton trump\n#&gt; 1    Alabama  AL  South    4779736   135  9    34.4  62.1\n#&gt; 2     Alaska  AK   West     710231    19  3    36.6  51.3\n#&gt; 3    Arizona  AZ   West    6392017   232 11    45.1  48.7\n#&gt; 4   Arkansas  AR  South    2915918    93  6    33.7  60.6\n#&gt; 5 California  CA   West   37253956  1257 55    61.7  31.6\n#&gt; 6   Colorado  CO   West    5029196    65  9    48.2  43.3\n\nThe data has been successfully joined and we can now, for example, make a plot to explore the relationship:\n\n\n\n\n\n\n\n\nWe see the relationship is close to linear with about 2 electoral votes for every million persons, but with very small states getting higher ratios.\nIn practice, it is not always the case that each row in one table has a matching row in the other. For this reason, we have several versions of join. To illustrate this challenge, we will take subsets of the tables above. We create the tables tab1 and tab2 so that they have some states in common but not all:\n\ntab_1 &lt;- slice(murders, 1:6) |&gt; select(state, population)\ntab_2 &lt;- results_us_election_2016 |&gt; \n  filter(state %in% c(\"Alabama\", \"Alaska\", \"Arizona\", \n                    \"California\", \"Connecticut\", \"Delaware\")) |&gt; \n  select(state, electoral_votes) |&gt; rename(ev = electoral_votes)\n\nWe will use these two tables as examples in the next sections.\n\n12.1.1 Left join\nSuppose we want a table like tab_1, but adding electoral votes to whatever states we have available. For this, we use left_join with tab_1 as the first argument. We specify which column to use to match with the by argument.\n\nleft_join(tab_1, tab_2, by = \"state\")\n#&gt;        state population ev\n#&gt; 1    Alabama    4779736  9\n#&gt; 2     Alaska     710231  3\n#&gt; 3    Arizona    6392017 11\n#&gt; 4   Arkansas    2915918 NA\n#&gt; 5 California   37253956 55\n#&gt; 6   Colorado    5029196 NA\n\nNote that NAs are added to the two states not appearing in tab_2. Also, notice that this function, as well as all the other joins, can receive the first arguments through the pipe:\n\ntab_1 |&gt; left_join(tab_2, by = \"state\")\n\n\n12.1.2 Right join\nIf instead of a table with the same rows as first table, we want one with the same rows as second table, we can use right_join:\n\ntab_1 |&gt; right_join(tab_2, by = \"state\")\n#&gt;         state population ev\n#&gt; 1     Alabama    4779736  9\n#&gt; 2      Alaska     710231  3\n#&gt; 3     Arizona    6392017 11\n#&gt; 4  California   37253956 55\n#&gt; 5 Connecticut         NA  7\n#&gt; 6    Delaware         NA  3\n\nNow the NAs are in the column coming from tab_1.\n\n12.1.3 Inner join\nIf we want to keep only the rows that have information in both tables, we use inner_join. You can think of this as an intersection:\n\ninner_join(tab_1, tab_2, by = \"state\")\n#&gt;        state population ev\n#&gt; 1    Alabama    4779736  9\n#&gt; 2     Alaska     710231  3\n#&gt; 3    Arizona    6392017 11\n#&gt; 4 California   37253956 55\n\n\n12.1.4 Full join\nIf we want to keep all the rows and fill the missing parts with NAs, we can use full_join. You can think of this as a union:\n\nfull_join(tab_1, tab_2, by = \"state\")\n#&gt;         state population ev\n#&gt; 1     Alabama    4779736  9\n#&gt; 2      Alaska     710231  3\n#&gt; 3     Arizona    6392017 11\n#&gt; 4    Arkansas    2915918 NA\n#&gt; 5  California   37253956 55\n#&gt; 6    Colorado    5029196 NA\n#&gt; 7 Connecticut         NA  7\n#&gt; 8    Delaware         NA  3\n\n\n12.1.5 Semi join\nThe semi_join function lets us keep the part of first table for which we have information in the second. It does not add the columns of the second:\n\nsemi_join(tab_1, tab_2, by = \"state\")\n#&gt;        state population\n#&gt; 1    Alabama    4779736\n#&gt; 2     Alaska     710231\n#&gt; 3    Arizona    6392017\n#&gt; 4 California   37253956\n\n\n12.1.6 Anti join\nThe function anti_join is the opposite of semi_join. It keeps the elements of the first table for which there is no information in the second:\n\nanti_join(tab_1, tab_2, by = \"state\")\n#&gt;      state population\n#&gt; 1 Arkansas    2915918\n#&gt; 2 Colorado    5029196\n\nThe following diagram summarizes the above joins:\n\n(Image courtesy of RStudio1. CC-BY-4.0 license2. Cropped from original.)"
  },
  {
    "objectID": "wrangling/joining-tables.html#binding",
    "href": "wrangling/joining-tables.html#binding",
    "title": "\n12  Joining tables\n",
    "section": "\n12.2 Binding",
    "text": "12.2 Binding\nAlthough we have yet to use it in this book, another common way in which datasets are combined is by binding them. Unlike the join function, the binding functions do not try to match by a variable, but instead simply combine datasets. If the datasets don’t match by the appropriate dimensions, one obtains an error.\n\n12.2.1 Binding columns\nThe dplyr function bind_cols binds two objects by making them columns in a tibble. For example, we quickly want to make a data frame consisting of numbers we can use.\n\nbind_cols(a = 1:3, b = 4:6)\n#&gt; # A tibble: 3 × 2\n#&gt;       a     b\n#&gt;   &lt;int&gt; &lt;int&gt;\n#&gt; 1     1     4\n#&gt; 2     2     5\n#&gt; 3     3     6\n\nThis function requires that we assign names to the columns. Here we chose a and b.\nNote that there is an R-base function cbind with the exact same functionality. An important difference is that cbind can create different types of objects, while bind_cols always produces a data frame.\nbind_cols can also bind two different data frames. For example, here we break up the tab data frame and then bind them back together:\n\ntab_1 &lt;- tab[, 1:3]\ntab_2 &lt;- tab[, 4:6]\ntab_3 &lt;- tab[, 7:8]\nnew_tab &lt;- bind_cols(tab_1, tab_2, tab_3)\nhead(new_tab)\n#&gt;        state abb region population total ev clinton trump\n#&gt; 1    Alabama  AL  South    4779736   135  9    34.4  62.1\n#&gt; 2     Alaska  AK   West     710231    19  3    36.6  51.3\n#&gt; 3    Arizona  AZ   West    6392017   232 11    45.1  48.7\n#&gt; 4   Arkansas  AR  South    2915918    93  6    33.7  60.6\n#&gt; 5 California  CA   West   37253956  1257 55    61.7  31.6\n#&gt; 6   Colorado  CO   West    5029196    65  9    48.2  43.3\n\n\n12.2.2 Binding by rows\nThe bind_rows function is similar to bind_cols, but binds rows instead of columns:\n\ntab_1 &lt;- tab[1:2,]\ntab_2 &lt;- tab[3:4,]\nbind_rows(tab_1, tab_2)\n#&gt;      state abb region population total ev clinton trump\n#&gt; 1  Alabama  AL  South    4779736   135  9    34.4  62.1\n#&gt; 2   Alaska  AK   West     710231    19  3    36.6  51.3\n#&gt; 3  Arizona  AZ   West    6392017   232 11    45.1  48.7\n#&gt; 4 Arkansas  AR  South    2915918    93  6    33.7  60.6\n\nThis is based on an R-base function rbind."
  },
  {
    "objectID": "wrangling/joining-tables.html#set-operators",
    "href": "wrangling/joining-tables.html#set-operators",
    "title": "\n12  Joining tables\n",
    "section": "\n12.3 Set operators",
    "text": "12.3 Set operators\nAnother set of commands useful for combining datasets are the set operators. When applied to vectors, these behave as their names suggest. Examples are intersect, union, setdiff, and setequal. However, if the tidyverse, or more specifically dplyr, is loaded, these functions can be used on data frames as opposed to just on vectors.\n\n12.3.1 Intersect\nYou can take intersections of vectors of any type, such as numeric:\n\nintersect(1:10, 6:15)\n#&gt; [1]  6  7  8  9 10\n\nor characters:\n\nintersect(c(\"a\",\"b\",\"c\"), c(\"b\",\"c\",\"d\"))\n#&gt; [1] \"b\" \"c\"\n\nThe dplyr package includes an intersect function that can be applied to tables with the same column names. This function returns the rows in common between two tables. To make sure we use the dplyr version of intersect rather than the base package version, we can use dplyr::intersect like this:\n\ntab_1 &lt;- tab[1:5,]\ntab_2 &lt;- tab[3:7,]\ndplyr::intersect(tab_1, tab_2)\n#&gt;        state abb region population total ev clinton trump\n#&gt; 1    Arizona  AZ   West    6392017   232 11    45.1  48.7\n#&gt; 2   Arkansas  AR  South    2915918    93  6    33.7  60.6\n#&gt; 3 California  CA   West   37253956  1257 55    61.7  31.6\n\n\n12.3.2 Union\nSimilarly union takes the union of vectors. For example:\n\nunion(1:10, 6:15)\n#&gt;  [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15\nunion(c(\"a\",\"b\",\"c\"), c(\"b\",\"c\",\"d\"))\n#&gt; [1] \"a\" \"b\" \"c\" \"d\"\n\nThe dplyr package includes a version of union that combines all the rows of two tables with the same column names.\n\ntab_1 &lt;- tab[1:5,]\ntab_2 &lt;- tab[3:7,]\ndplyr::union(tab_1, tab_2) \n#&gt;         state abb    region population total ev clinton trump\n#&gt; 1     Alabama  AL     South    4779736   135  9    34.4  62.1\n#&gt; 2      Alaska  AK      West     710231    19  3    36.6  51.3\n#&gt; 3     Arizona  AZ      West    6392017   232 11    45.1  48.7\n#&gt; 4    Arkansas  AR     South    2915918    93  6    33.7  60.6\n#&gt; 5  California  CA      West   37253956  1257 55    61.7  31.6\n#&gt; 6    Colorado  CO      West    5029196    65  9    48.2  43.3\n#&gt; 7 Connecticut  CT Northeast    3574097    97  7    54.6  40.9\n\n\n12.3.3 setdiff\n\nThe set difference between a first and second argument can be obtained with setdiff. Unlike intersect and union, this function is not symmetric:\n\nsetdiff(1:10, 6:15)\n#&gt; [1] 1 2 3 4 5\nsetdiff(6:15, 1:10)\n#&gt; [1] 11 12 13 14 15\n\nAs with the functions shown above, dplyr has a version for data frames:\n\ntab_1 &lt;- tab[1:5,]\ntab_2 &lt;- tab[3:7,]\ndplyr::setdiff(tab_1, tab_2)\n#&gt;     state abb region population total ev clinton trump\n#&gt; 1 Alabama  AL  South    4779736   135  9    34.4  62.1\n#&gt; 2  Alaska  AK   West     710231    19  3    36.6  51.3\n\n\n12.3.4 setequal\n\nFinally, the function setequal tells us if two sets are the same, regardless of order. So notice that:\n\nsetequal(1:5, 1:6)\n#&gt; [1] FALSE\n\nbut:\n\nsetequal(1:5, 5:1)\n#&gt; [1] TRUE\n\nWhen applied to data frames that are not equal, regardless of order, the dplyr version provides a useful message letting us know how the sets are different:\n\ndplyr::setequal(tab_1, tab_2)\n#&gt; [1] FALSE"
  },
  {
    "objectID": "wrangling/joining-tables.html#exercises",
    "href": "wrangling/joining-tables.html#exercises",
    "title": "\n12  Joining tables\n",
    "section": "\n12.4 Exercises",
    "text": "12.4 Exercises\n1. Install and load the Lahman library. This database includes data related to baseball teams. It includes summary statistics about how the players performed on offense and defense for several years. It also includes personal information about the players.\nThe Batting data frame contains the offensive statistics for all players for many years. You can see, for example, the top 10 hitters by running this code:\n\nlibrary(Lahman)\n\ntop &lt;- Batting |&gt; \n  filter(yearID == 2016) |&gt;\n  arrange(desc(HR)) |&gt;\n  slice(1:10)\n\ntop |&gt; as_tibble()\n\nBut who are these players? We see an ID, but not the names. The player names are in this table\n\nPeople |&gt; as_tibble()\n\nWe can see column names nameFirst and nameLast. Use the left_join function to create a table of the top home run hitters. The table should have playerID, first name, last name, and number of home runs (HR). Rewrite the object top with this new table.\n2. Now use the Salaries data frame to add each player’s salary to the table you created in exercise 1. Note that salaries are different every year so make sure to filter for the year 2016, then use right_join. This time show first name, last name, team, HR, and salary.\n3. In a previous exercise, we created a tidy version of the co2 dataset:\n\nco2_wide &lt;- data.frame(matrix(co2, ncol = 12, byrow = TRUE)) |&gt; \n  setNames(1:12) |&gt;\n  mutate(year = 1959:1997) |&gt;\n  pivot_longer(-year, names_to = \"month\", values_to = \"co2\") |&gt;\n  mutate(month = as.numeric(month))\n\nWe want to see if the monthly trend is changing so we are going to remove the year effects and then plot the results. We will first compute the year averages. Use the group_by and summarize to compute the average co2 for each year. Save in an object called yearly_avg.\n4. Now use the left_join function to add the yearly average to the co2_wide dataset. Then compute the residuals: observed co2 measure - yearly average.\n5. Make a plot of the seasonal trends by year but only after removing the year effect."
  },
  {
    "objectID": "wrangling/joining-tables.html#footnotes",
    "href": "wrangling/joining-tables.html#footnotes",
    "title": "\n12  Joining tables\n",
    "section": "",
    "text": "https://github.com/rstudio/cheatsheets↩︎\nhttps://github.com/rstudio/cheatsheets/blob/master/LICENSE↩︎"
  },
  {
    "objectID": "wrangling/dates-and-times.html#the-date-data-type",
    "href": "wrangling/dates-and-times.html#the-date-data-type",
    "title": "13  Parsing dates and times",
    "section": "\n13.1 The date data type",
    "text": "13.1 The date data type\nWe can see an example of the data type R uses for data here:\n\nlibrary(tidyverse)\nlibrary(dslabs)\npolls_us_election_2016$startdate |&gt; head()\n#&gt; [1] \"2016-11-03\" \"2016-11-01\" \"2016-11-02\" \"2016-11-04\" \"2016-11-03\"\n#&gt; [6] \"2016-11-03\"\n\nThe dates look like strings, but they are not:\n\nclass(polls_us_election_2016$startdate)\n#&gt; [1] \"Date\"\n\nLook at what happens when we convert them to numbers:\n\nas.numeric(polls_us_election_2016$startdate) |&gt; head()\n#&gt; [1] 17108 17106 17107 17109 17108 17108\n\nIt turns them into days since the epoch. The as.Date function can convert a character into a date. So to see that the epoch is day 0 we can type\n\nas.Date(\"1970-01-01\") |&gt; as.numeric()\n#&gt; [1] 0\n\nPlotting functions, such as those in ggplot, are aware of the date format. This means that, for example, a scatterplot can use the numeric representation to decide on the position of the point, but include the string in the labels:\n\npolls_us_election_2016 |&gt; filter(pollster == \"Ipsos\" & state == \"U.S.\") |&gt;\n  ggplot(aes(startdate, rawpoll_trump)) +\n  geom_line()\n\n\n\n\n\n\n\nNote in particular that the month names are displayed, a very convenient feature."
  },
  {
    "objectID": "wrangling/dates-and-times.html#sec-lubridate",
    "href": "wrangling/dates-and-times.html#sec-lubridate",
    "title": "13  Parsing dates and times",
    "section": "\n13.2 The lubridate package",
    "text": "13.2 The lubridate package\nThe lubridate package provides tools to work with date and times.\n\nlibrary(lubridate)\n\nWe will take a random sample of dates to show some of the useful things one can do:\n\nset.seed(2002)\ndates &lt;- sample(polls_us_election_2016$startdate, 10) |&gt; sort()\ndates\n#&gt;  [1] \"2016-05-31\" \"2016-08-08\" \"2016-08-19\" \"2016-09-22\" \"2016-09-27\"\n#&gt;  [6] \"2016-10-12\" \"2016-10-24\" \"2016-10-26\" \"2016-10-29\" \"2016-10-30\"\n\nThe functions year, month and day extract those values:\n\ntibble(date = dates, month = month(dates), day = day(dates), year = year(dates))\n#&gt; # A tibble: 10 × 4\n#&gt;   date       month   day  year\n#&gt;   &lt;date&gt;     &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;\n#&gt; 1 2016-05-31     5    31  2016\n#&gt; 2 2016-08-08     8     8  2016\n#&gt; 3 2016-08-19     8    19  2016\n#&gt; 4 2016-09-22     9    22  2016\n#&gt; 5 2016-09-27     9    27  2016\n#&gt; # ℹ 5 more rows\n\nWe can also extract the month labels:\n\nmonth(dates, label = TRUE)\n\n\n#&gt;  [1] May Aug Aug Sep Sep Oct Oct Oct Oct Oct\n#&gt; 12 Levels: Jan &lt; Feb &lt; Mar &lt; Apr &lt; May &lt; Jun &lt; Jul &lt; Aug &lt; ... &lt; Dec\n\nAnother useful set of functions are the parsers that convert strings into dates. The function ymd assumes the dates are in the format YYYY-MM-DD and tries to parse as well as possible.\n\nx &lt;- c(20090101, \"2009-01-02\", \"2009 01 03\", \"2009-1-4\",\n       \"2009-1, 5\", \"Created on 2009 1 6\", \"200901 !!! 07\")\nymd(x)\n#&gt; [1] \"2009-01-01\" \"2009-01-02\" \"2009-01-03\" \"2009-01-04\" \"2009-01-05\"\n#&gt; [6] \"2009-01-06\" \"2009-01-07\"\n\nA further complication comes from the fact that dates often come in different formats in which the order of year, month, and day are different. The preferred format is to show year (with all four digits), month (two digits), and then day, or what is called the ISO 8601. Specifically we use YYYY-MM-DD so that if we order the string, it will be ordered by date. You can see the function ymd returns them in this format.\nBut, what if you encounter dates such as “09/01/02”? This could be September 1, 2002 or January 2, 2009 or January 9, 2002. In these cases, examining the entire vector of dates will help you determine what format it is by process of elimination. Once you know, you can use the many parses provided by lubridate.\nFor example, if the string is:\n\nx &lt;- \"09/01/02\"\n\nThe ymd function assumes the first entry is the year, the second is the month, and the third is the day, so it converts it to:\n\nymd(x)\n#&gt; [1] \"2009-01-02\"\n\nThe mdy function assumes the first entry is the month, then the day, then the year:\n\nmdy(x)\n#&gt; [1] \"2002-09-01\"\n\nThe lubridate package provides a function for every possibility. Here are the other common one:\n\ndmy(x)\n#&gt; [1] \"2002-01-09\"\n\nThe lubridate package is also useful for dealing with times. In R base, you can get the current time typing Sys.time(). The lubridate package provides a slightly more advanced function, now, that permits you to define the time zone:\n\nnow()\n#&gt; [1] \"2023-11-24 12:04:40 EST\"\nnow(\"GMT\")\n#&gt; [1] \"2023-11-24 17:04:40 GMT\"\n\nYou can see all the available time zones with OlsonNames() function.\nWe can also extract hours, minutes, and seconds:\n\nnow() |&gt; hour()\n#&gt; [1] 12\nnow() |&gt; minute()\n#&gt; [1] 4\nnow() |&gt; second()\n#&gt; [1] 40.1\n\nThe package also includes a function to parse strings into times as well as parsers for time objects that include dates:\n\nx &lt;- c(\"12:34:56\")\nhms(x)\n#&gt; [1] \"12H 34M 56S\"\nx &lt;- \"Nov/2/2012 12:34:56\"\nmdy_hms(x)\n#&gt; [1] \"2012-11-02 12:34:56 UTC\"\n\nThis package has many other useful functions. We describe two of these here that we find particularly useful.\nThe make_date function can be used to quickly create a date object. It takes three arguments: year, month, day, hour, minute, seconds, and time zone defaulting to the epoch values on UTC time. To create an date object representing, for example, July 6, 2019 we write:\n\nmake_date(2019, 7, 6)\n#&gt; [1] \"2019-07-06\"\n\nTo make a vector of January 1 for the 80s we write:\n\nmake_date(1980:1989)\n#&gt;  [1] \"1980-01-01\" \"1981-01-01\" \"1982-01-01\" \"1983-01-01\" \"1984-01-01\"\n#&gt;  [6] \"1985-01-01\" \"1986-01-01\" \"1987-01-01\" \"1988-01-01\" \"1989-01-01\"\n\nAnother very useful function is the round_date. It can be used to round dates to nearest year, quarter, month, week, day, hour, minutes, or seconds. So if we want to group all the polls by week of the year we can do the following:\n\npolls_us_election_2016 |&gt; \n  mutate(week = round_date(startdate, \"week\")) |&gt;\n  group_by(week) |&gt;\n  summarize(margin = mean(rawpoll_clinton - rawpoll_trump)) |&gt;\n  ggplot(aes(week, margin)) +\n  geom_point()"
  },
  {
    "objectID": "wrangling/dates-and-times.html#exercises",
    "href": "wrangling/dates-and-times.html#exercises",
    "title": "13  Parsing dates and times",
    "section": "\n13.3 Exercises",
    "text": "13.3 Exercises\n1. We want to make a plot of death counts versus date. Confirm that the date variable are in fact dates and not strings.\n2. Plot deaths versus date.\n\nWhat time period is represented in these data?\n\n4. Note that after May 31, 2018, the deaths are all 0. The data is probably not entered yet. We also see a drop off starting around May 1. Redefine dat to exclude observations taken on or after May 1, 2018. Then, remake the plot.\n5. Repeat the plot but use the day of the year on the x-axis instead of date.\n6. Compute the deaths per day by month.\n7. Show the deaths per days for July and for September. What do you notice?\n8. Compute deaths per week and make a plot."
  },
  {
    "objectID": "wrangling/data-table-wrangling.html#reshaping-data",
    "href": "wrangling/data-table-wrangling.html#reshaping-data",
    "title": "\n14  Wrangling with data.table\n",
    "section": "\n14.1 Reshaping data",
    "text": "14.1 Reshaping data\nPreviously we used this example:\n\nlibrary(dslabs)\npath &lt;- system.file(\"extdata\", package = \"dslabs\")\nfilename &lt;- file.path(path, \"fertility-two-countries-example.csv\")\n\n\n14.1.1 pivot_longer is melt\n\nIf in tidyeverse we write\n\nwide_data &lt;- read_csv(filename)\nnew_tidy_data &lt;- wide_data |&gt;\n  pivot_longer(-1, names_to = \"year\", values_to = \"fertility\")\n\nin data.table we use the melt function\n\ndt_wide_data &lt;- fread(filename) \ndt_new_tidy_data  &lt;- melt(dt_wide_data, \n                      measure.vars = 2:ncol(dt_wide_data), \n                      variable.name = \"year\", \n                      value.name = \"fertility\")"
  },
  {
    "objectID": "wrangling/data-table-wrangling.html#pivot_wider-is-dcast",
    "href": "wrangling/data-table-wrangling.html#pivot_wider-is-dcast",
    "title": "\n14  Wrangling with data.table\n",
    "section": "\n14.2 pivot_wider is dcast\n",
    "text": "14.2 pivot_wider is dcast\n\nIf in tidyeverse we write\n\nnew_wide_data &lt;- new_tidy_data |&gt; \n  pivot_wider(names_from = year, values_from = fertility)\n\nin data.table we write:\n\ndt_new_wide_data &lt;- dcast(dt_new_tidy_data, formula = ... ~ year,\n                          value.var = \"fertility\")\n\n\n14.2.1 Separating variables\n\npath &lt;- system.file(\"extdata\", package = \"dslabs\")\nfilename &lt;- \"life-expectancy-and-fertility-two-countries-example.csv\"\nfilename &lt;-  file.path(path, filename)\n\nIn tidyverse we wrangled using\n\nraw_dat &lt;- read_csv(filename)\ndat &lt;- raw_dat |&gt; pivot_longer(-country) |&gt;\n  separate_wider_delim(name, delim = \"_\", names = c(\"year\", \"name\"), \n                       too_many = \"merge\") |&gt;\n  pivot_wider() |&gt;\n  mutate(year = as.integer(year))\n\nIn data.table we can use the tstrsplit function:\n\ndt_raw_dat &lt;- fread(filename)\ndat_long &lt;- melt(dt_raw_dat, \n                 measure.vars = which(names(dt_raw_dat) != \"country\"), \n                 variable.name = \"name\", value.name = \"value\")\ndat_long[, c(\"year\", \"name\", \"name2\") := \n           tstrsplit(name, \"_\", fixed = TRUE, type.convert = TRUE)]\ndat_long[is.na(name2), name2 := \"\"]\ndat_long[, name := paste(name, name2, sep = \"_\")][, name2 := NULL]\ndat_wide &lt;- dcast(dat_long, country + year ~ name, value.var = \"value\")"
  },
  {
    "objectID": "wrangling/data-table-wrangling.html#joins",
    "href": "wrangling/data-table-wrangling.html#joins",
    "title": "\n14  Wrangling with data.table\n",
    "section": "\n14.3 Joins",
    "text": "14.3 Joins\nIn *tidyverse** we joined two table like this:\n\ntab &lt;- left_join(murders, results_us_election_2016, by = \"state\") \n\nIn data.table the merge functions works similarly:\n\ntab &lt;- merge(murders, results_us_election_2016, by = \"state\", all.x = TRUE)\n\nInstead of defining different functions for the different type of joins, merge uses the the logical arguments all (full join), all.x (left join), and all.y (right join)."
  },
  {
    "objectID": "wrangling/data-table-wrangling.html#dates-and-times",
    "href": "wrangling/data-table-wrangling.html#dates-and-times",
    "title": "\n14  Wrangling with data.table\n",
    "section": "\n14.4 Dates and times",
    "text": "14.4 Dates and times\nThe data.table package also includes some of the functionality to lubridate. For example, it includes the mday, month, and year functions:\n\ndata.table::mday(now())\n#&gt; [1] 24\ndata.table::month(now())\n#&gt; [1] 11\ndata.table::year(now())\n#&gt; [1] 2023\n\nOther similar functions are second, minute, hour, wday, week, isoweek quarter, yearmon, yearqtr.\nThe package also includes the class IDate and ITime, which store dates and times more efficiently, convenient for large files with date stamps. You convert dates in the usual R format using as.IDate and as.ITime."
  },
  {
    "objectID": "wrangling/data-table-wrangling.html#exercises",
    "href": "wrangling/data-table-wrangling.html#exercises",
    "title": "\n14  Wrangling with data.table\n",
    "section": "\n14.5 Exercises",
    "text": "14.5 Exercises\nRepear exercises in Chapter 11, Section 12.1, and Chapter 13 using data.table instead of tidyverse."
  },
  {
    "objectID": "wrangling/web-scraping.html#html",
    "href": "wrangling/web-scraping.html#html",
    "title": "\n15  Web scraping\n",
    "section": "\n15.1 HTML",
    "text": "15.1 HTML\nBecause this code is accessible, we can download the HTML file, import it into R, and then write programs to extract the information we need from the page. However, once we look at HTML code, this might seem like a daunting task. But we will show you some convenient tools to facilitate the process. To get an idea of how it works, here are a few lines of code from the Wikipedia page that provides the US murders data:\n&lt;table class=\"wikitable sortable\"&gt;\n&lt;tr&gt;\n&lt;th&gt;State&lt;/th&gt;\n&lt;th&gt;&lt;a href=\"/wiki/List_of_U.S._states_and_territories_by_population\" \ntitle=\"List of U.S. states and territories by population\"&gt;Population&lt;/a&gt;&lt;br /&gt;\n&lt;small&gt;(total inhabitants)&lt;/small&gt;&lt;br /&gt;\n&lt;small&gt;(2015)&lt;/small&gt; &lt;sup id=\"cite_ref-1\" class=\"reference\"&gt;\n&lt;a href=\"#cite_note-1\"&gt;[1]&lt;/a&gt;&lt;/sup&gt;&lt;/th&gt;\n&lt;th&gt;Murders and Nonnegligent\n&lt;p&gt;Manslaughter&lt;br /&gt;\n&lt;small&gt;(total deaths)&lt;/small&gt;&lt;br /&gt;\n&lt;small&gt;(2015)&lt;/small&gt; &lt;sup id=\"cite_ref-2\" class=\"reference\"&gt;\n&lt;a href=\"#cite_note-2\"&gt;[2]&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;\n&lt;/th&gt;\n&lt;th&gt;Murder and Nonnegligent\n&lt;p&gt;Manslaughter Rate&lt;br /&gt;\n&lt;small&gt;(per 100,000 inhabitants)&lt;/small&gt;&lt;br /&gt;\n&lt;small&gt;(2015)&lt;/small&gt;&lt;/p&gt;\n&lt;/th&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;a href=\"/wiki/Alabama\" title=\"Alabama\"&gt;Alabama&lt;/a&gt;&lt;/td&gt;\n&lt;td&gt;4,853,875&lt;/td&gt;\n&lt;td&gt;348&lt;/td&gt;\n&lt;td&gt;7.2&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;a href=\"/wiki/Alaska\" title=\"Alaska\"&gt;Alaska&lt;/a&gt;&lt;/td&gt;\n&lt;td&gt;737,709&lt;/td&gt;\n&lt;td&gt;59&lt;/td&gt;\n&lt;td&gt;8.0&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\nYou can actually see the data, except data values are surrounded by html code such as &lt;td&gt;. We can also see a pattern of how it is stored. If you know HTML, you can write programs that leverage knowledge of these patterns to extract what we want. We also take advantage of a language widely used to make webpages look “pretty” called Cascading Style Sheets (CSS). We say more about this in Section Section 15.3.\nAlthough we provide tools that make it possible to scrape data without knowing HTML, it is useful to learn some HTML and CSS. Not only does this improve your scraping skills, but it might come in handy if you are creating a webpage to showcase your work. There are plenty of online courses and tutorials for learning these. Two examples are Codeacademy3 and W3schools4."
  },
  {
    "objectID": "wrangling/web-scraping.html#the-rvest-package",
    "href": "wrangling/web-scraping.html#the-rvest-package",
    "title": "\n15  Web scraping\n",
    "section": "\n15.2 The rvest package",
    "text": "15.2 The rvest package\nThe tidyverse provides a web harvesting package called rvest. The first step using this package is to import the webpage into R. The package makes this quite simple:\n\nlibrary(tidyverse)\nlibrary(rvest)\nh &lt;- read_html(url)\n\nNote that the entire Murders in the US Wikipedia webpage is now contained in h. The class of this object is:\n\nclass(h)\n#&gt; [1] \"xml_document\" \"xml_node\"\n\nThe rvest package is actually more general; it handles XML documents. XML is a general markup language (that’s what the ML stands for) that can be used to represent any kind of data. HTML is a specific type of XML specifically developed for representing webpages. Here we focus on HTML documents.\nNow, how do we extract the table from the object h? If we print h, we don’t really see much:\n\nh\n#&gt; {html_document}\n#&gt; &lt;html class=\"client-nojs vector-feature-language-in-header-enabled vector-feature-language-in-main-page-header-disabled vector-feature-sticky-header-disabled vector-feature-page-tools-pinned-disabled vector-feature-toc-pinned-clientpref-1 vector-feature-main-menu-pinned-disabled vector-feature-limited-width-clientpref-1 vector-feature-limited-width-content-enabled vector-feature-zebra-design-disabled vector-feature-custom-font-size-clientpref-0 vector-feature-client-preferences-disabled vector-feature-typography-survey-disabled vector-toc-available\" lang=\"en\" dir=\"ltr\"&gt;\n#&gt; [1] &lt;head&gt;\\n&lt;meta http-equiv=\"Content-Type\" content=\"text/html; chars ...\n#&gt; [2] &lt;body class=\"skin-vector skin-vector-search-vue mediawiki ltr sit ...\n\nWe can see all the code that defines the downloaded webpage using the html_text function like this:\n\nhtml_text(h)\n\nWe don’t show the output here because it includes thousands of characters, but if we look at it, we can see the data we are after are stored in an HTML table: you can see this in this line of the HTML code above &lt;table class=\"wikitable sortable\"&gt;. The different parts of an HTML document, often defined with a message in between &lt; and &gt; are referred to as nodes. The rvest package includes functions to extract nodes of an HTML document: html_nodes extracts all nodes of different types and html_node extracts the first one. To extract the tables from the html code we use:\n\ntab &lt;- h |&gt; html_nodes(\"table\")\n\nNow, instead of the entire webpage, we just have the html code for the tables in the page:\n\ntab\n#&gt; {xml_nodeset (2)}\n#&gt; [1] &lt;table class=\"wikitable sortable\"&gt;&lt;tbody&gt;\\n&lt;tr&gt;\\n&lt;th&gt;State\\n&lt;/th&gt; ...\n#&gt; [2] &lt;table class=\"nowraplinks hlist mw-collapsible mw-collapsed navbo ...\n\nThe table we are interested is the first one:\n\ntab[[1]]\n#&gt; {html_node}\n#&gt; &lt;table class=\"wikitable sortable\"&gt;\n#&gt; [1] &lt;tbody&gt;\\n&lt;tr&gt;\\n&lt;th&gt;State\\n&lt;/th&gt;\\n&lt;th&gt;\\n&lt;a href=\"/wiki/List_of_U.S ...\n\nThis is clearly not a tidy dataset, not even a data frame. In the code above, you can definitely see a pattern and writing code to extract just the data is very doable. In fact, rvest includes a function just for converting HTML tables into data frames:\n\ntab &lt;- tab[[1]] |&gt; html_table()\nclass(tab)\n#&gt; [1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\nWe are now much closer to having a usable data table:\n\ntab &lt;- tab |&gt; setNames(c(\"state\", \"population\", \"total\", \"murder_rate\")) \nhead(tab)\n#&gt; # A tibble: 6 × 4\n#&gt;   state      population total murder_rate\n#&gt;   &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;       &lt;dbl&gt;\n#&gt; 1 Alabama    4,853,875  348           7.2\n#&gt; 2 Alaska     737,709    59            8  \n#&gt; 3 Arizona    6,817,565  309           4.5\n#&gt; 4 Arkansas   2,977,853  181           6.1\n#&gt; 5 California 38,993,940 1,861         4.8\n#&gt; # ℹ 1 more row\n\nWe still have some wrangling to do. For example, we need to remove the commas and turn characters into numbers. Before continuing with this, we will learn a more general approach to extracting information from web sites."
  },
  {
    "objectID": "wrangling/web-scraping.html#sec-css-selectors",
    "href": "wrangling/web-scraping.html#sec-css-selectors",
    "title": "\n15  Web scraping\n",
    "section": "\n15.3 CSS selectors",
    "text": "15.3 CSS selectors\nThe default look of a webpage made with the most basic HTML is quite unattractive. The aesthetically pleasing pages we see today are made using CSS to define the look and style of webpages. The fact that all pages for a company have the same style usually results from their use of the same CSS file to define the style. The general way these CSS files work is by defining how each of the elements of a webpage will look. The title, headings, itemized lists, tables, and links, for example, each receive their own style including font, color, size, and distance from the margin. CSS does this by leveraging patterns used to define these elements, referred to as selectors. An example of such a pattern, which we used above, is table, but there are many, many more.\nIf we want to grab data from a webpage and we happen to know a selector that is unique to the part of the page containing this data, we can use the html_nodes function. However, knowing which selector can be quite complicated. In fact, the complexity of webpages has been increasing as they become more sophisticated. For some of the more advanced ones, it seems almost impossible to find the nodes that define a particular piece of data. However, selector gadgets actually make this possible.\nSelectorGadget5 is piece of software that allows you to interactively determine what CSS selector you need to extract specific components from the webpage. If you plan on scraping data other than tables from html pages, we highly recommend you install it. A Chrome extension is available which permits you to turn on the gadget and then, as you click through the page, it highlights parts and shows you the selector you need to extract these parts. There are various demos of how to do this including rvest author Hadley Wickham’s vignette6 and other tutorials based on the vignette7 8."
  },
  {
    "objectID": "wrangling/web-scraping.html#json",
    "href": "wrangling/web-scraping.html#json",
    "title": "\n15  Web scraping\n",
    "section": "\n15.4 JSON",
    "text": "15.4 JSON\nSharing data on the internet has become more and more common. Unfortunately, providers use different formats, which makes it harder for data analysts to wrangle data into R. Yet there are some standards that are also becoming more common. Currently, a format that is widely being adopted is the JavaScript Object Notation or JSON. Because this format is very general, it is nothing like a spreadsheet. This JSON file looks more like the code you use to define a list. Here is an example of information stored in a JSON format:\n\n#&gt; [\n#&gt;   {\n#&gt;     \"name\": \"Miguel\",\n#&gt;     \"student_id\": 1,\n#&gt;     \"exam_1\": 85,\n#&gt;     \"exam_2\": 86\n#&gt;   },\n#&gt;   {\n#&gt;     \"name\": \"Sofia\",\n#&gt;     \"student_id\": 2,\n#&gt;     \"exam_1\": 94,\n#&gt;     \"exam_2\": 93\n#&gt;   },\n#&gt;   {\n#&gt;     \"name\": \"Aya\",\n#&gt;     \"student_id\": 3,\n#&gt;     \"exam_1\": 87,\n#&gt;     \"exam_2\": 88\n#&gt;   },\n#&gt;   {\n#&gt;     \"name\": \"Cheng\",\n#&gt;     \"student_id\": 4,\n#&gt;     \"exam_1\": 90,\n#&gt;     \"exam_2\": 91\n#&gt;   }\n#&gt; ]\n\nThe file above actually represents a data frame. To read it, we can use the function fromJSON from the jsonlite package. Note that JSON files are often made available via the internet. Several organizations provide a JSON API or a web service that you can connect directly to and obtain data. Here is an example providing information Nobel prize winners:\n\nnobel &lt;- fromJSON(\"http://api.nobelprize.org/v1/prize.json\")\n\nThis downloads a list. The first argument is a table with information about Nobel prize winners:\n\nfilter(nobel$prize, category == \"literature\" & year == \"1971\") |&gt; pull(laureates)\n#&gt; [[1]]\n#&gt;    id firstname surname\n#&gt; 1 645     Pablo  Neruda\n#&gt;                                                                                                motivation\n#&gt; 1 \"for a poetry that with the action of an elemental force brings alive a continent's destiny and dreams\"\n#&gt;   share\n#&gt; 1     1\n\nYou can learn much more by examining tutorials and help files from the jsonlite package. This package is intended for relatively simple tasks such as converting data into tables. For more flexibility, we recommend rjson."
  },
  {
    "objectID": "wrangling/web-scraping.html#data-apis",
    "href": "wrangling/web-scraping.html#data-apis",
    "title": "\n15  Web scraping\n",
    "section": "\n15.5 Data APIs",
    "text": "15.5 Data APIs\nAn Application Programming Interface (API) is a set of rules and protocols that allows different software entities to communicate with each other. It defines methods and data formats that software components should use when requesting and exchanging information. APIs play a crucial role in enabling the integration that make today’s software so interconnected and versatile.\nThere are several types of APIs. The main ones related to retrieving data are:\n\nWeb Services - Often built using protocols like HTTP/HTTPS. Commonly used to enable applications to communicate with each other over the web. For instance, a weather application for a smartphone may use a web API to request weather data from a remote server.\nDatabase APIs - Enable communication between an application and a database, SQL-based calls for example.\n\nKey concepts associated with APIs:\n\nEndpoints: Specific functions available through the API. For web APIs, an endpoint is usually a specific URL where the API can be accessed.\nMethods: Actions that can be performed. In web APIs, these often correspond to HTTP methods like GET, POST, PUT, or DELETE.\nRequests and Responses: The act of asking the API to perform its function is a request. The data it returns is the response.\nRate Limits: Restrictions on how often you can call the API, often used to prevent abuse or overloading of the service.\nAuthentication and Authorization: Mechanisms to ensure that only approved users or applications can use the API. Common methods include API keys, OAuth, or Jason Web Tokens (JWT).\nData Formats: Many web APIs exchange data in a specific format, often JSON or CSV."
  },
  {
    "objectID": "wrangling/web-scraping.html#the-httr2-package",
    "href": "wrangling/web-scraping.html#the-httr2-package",
    "title": "\n15  Web scraping\n",
    "section": "\n15.6 The httr2 package",
    "text": "15.6 The httr2 package\nHTTP is the most widely used protocol for data sharing through the internet. The httr2 package provides functions to work with HTTP requests. One of the core functions in this package is request, which is used to form request to send to web services. The req_perform function sends the request.\nThis request function forms an HTTP GET request to the specified URL. Typically, HTTP GET requests are used to retrieve information from a server based on the provided URL.\nThe function returns an object of class response. This object contains all the details of the server’s response, including status code, headers, and content. You can then use other httr2 functions to extract or interpret information from this response.\nLet’s say you want to retrieve COVID-19 deaths by state from the CDC. By visiting https://data.cdc.gov you can search for datasets and find that the data is provided through this API:\n\nurl &lt;- \"https://data.cdc.gov/resource/muzy-jte6.csv\"\n\nWe cam then make create and perform a request like this:\n\nlibrary(httr2)\nresponse &lt;- request(url) |&gt; req_perform()\n\nWe can see the results of the request by looking at the returned object.\n\nresponse\n#&gt; &lt;httr2_response&gt;\n#&gt; GET https://data.cdc.gov/resource/muzy-jte6.csv\n#&gt; Status: 200 OK\n#&gt; Content-Type: text/csv\n#&gt; Body: In memory (210808 bytes)\n\nTo extract the body, which is where the data are, we can use resp_body_string and send the result, a comma delimited string, to read_csv\n\nlibrary(readr)\ntab &lt;- response |&gt; resp_body_string() |&gt; read_csv()\n\nWe note that the returned object is only 1000 entries. API often limit how much you can download. The documentation for this API9 explains that we can change this limit through the $limit parameters. We can use the req_url_path_append to add this to our request:\n\nresponse &lt;- request(url) |&gt; req_url_path_append(\"?$limit=100000\") |&gt; req_perform() \n\nThe CDC service returns data in csv format but a more common format used by web services is JSON. The CDC also provides data in json format through a the url:\n\nurl &lt;- \"https://data.cdc.gov/resource/muzy-jte6.json\"\n\nTo extract the data table we use the fromJSON function from the jsonlite package.\n\ntab &lt;- request(url) |&gt; \n   req_perform() |&gt; \n   resp_body_string() |&gt; \n   fromJSON(flatten = TRUE)\n\nWhen working with APIs, it’s essential to check the API’s documentation for rate limits, required headers, or authentication methods. The httr2 package provides tools to handle these requirements, such as setting headers or authentication parameters."
  },
  {
    "objectID": "wrangling/web-scraping.html#exercises",
    "href": "wrangling/web-scraping.html#exercises",
    "title": "\n15  Web scraping\n",
    "section": "\n15.7 Exercises",
    "text": "15.7 Exercises\n1. Visit the following web page: https://web.archive.org/web/20181024132313/http://www.stevetheump.com/Payrolls.htm\nNotice there are several tables. Say we are interested in comparing the payrolls of teams across the years. The next few exercises take us through the steps needed to do this.\nStart by applying what you learned to read in the website into an object called h.\n2. Note that, although not very useful, we can actually see the content of the page by typing:\n\nhtml_text(h)\n\nThe next step is to extract the tables. For this, we can use the html_nodes function. We learned that tables in html are associated with the table node. Use the html_nodes function and the table node to extract the first table. Store it in an object nodes.\n3. The html_nodes function returns a list of objects of class xml_node. We can see the content of each one using, for example, the html_text function. You can see the content for an arbitrarily picked component like this:\n\nhtml_text(nodes[[8]])\n\nIf the content of this object is an html table, we can use the html_table function to convert it to a data frame. Use the html_table function to convert the 8th entry of nodes into a table.\n4. Repeat the above for the first 4 components of nodes. Which of the following are payroll tables:\n\nAll of them.\n1\n2\n2-4\n\n5. Repeat the above for the first last 3 components of nodes. Which of the following is true:\n\nThe last entry in nodes shows the average across all teams through time, not payroll per team.\nAll three are payroll per team tables.\nAll three are like the first entry, not a payroll table.\nAll of the above.\n\n6. We have learned that the first and last entries of nodes are not payroll tables. Redefine nodes so that these two are removed.\n7. We saw in the previous analysis that the first table node is not actually a table. This happens sometimes in html because tables are used to make text look a certain way, as opposed to storing numeric values. Remove the first component and then use sapply and html_table to convert each node in nodes into a table. Note that in this case, sapply will return a list of tables. You can also use lapply to assure that a list is applied.\n8. Look through the resulting tables. Are they all the same? Could we just join them with bind_rows?\n9. Create two tables, call them tab_1 and tab_2 using entries 10 and 19.\n10. Use a full_join function to combine these two tables. Before you do this you will have to fix the missing header problem. You will also need to make the names match.\n11. After joining the tables, you see several NAs. This is because some teams are in one table and not the other. Use the anti_join function to get a better idea of why this is happening.\n12. We see see that one of the problems is that Yankees are listed as both N.Y. Yankees and NY Yankees. In the next section, we will learn efficient approaches to fixing problems like this. Here we can do it “by hand” as follows:\n\ntab_1 &lt;- tab_1 |&gt;\n  mutate(Team = ifelse(Team == \"N.Y. Yankees\", \"NY Yankees\", Team))\n\nNow join the tables and show only Oakland and the Yankees and the payroll columns.\n13. Advanced: extract the titles of the movies that won Best Picture from this website: https://m.imdb.com/chart/bestpicture/"
  },
  {
    "objectID": "wrangling/web-scraping.html#footnotes",
    "href": "wrangling/web-scraping.html#footnotes",
    "title": "\n15  Web scraping\n",
    "section": "",
    "text": "https://en.wikipedia.org/w/index.php?title=Gun_violence_in_the_United_States_by_state&direction=prev&oldid=810166167↩︎\nhttps://en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License↩︎\nhttps://www.codecademy.com/learn/learn-html↩︎\nhttps://www.w3schools.com/↩︎\nhttp://selectorgadget.com/↩︎\nhttps://cran.r-project.org/web/packages/rvest/vignettes/selectorgadget.html↩︎\nhttps://stat4701.github.io/edav/2015/04/02/rvest_tutorial/↩︎\nhttps://www.analyticsvidhya.com/blog/2017/03/beginners-guide-on-web-scraping-in-r-using-rvest-with-hands-on-knowledge/↩︎\nhttps://dev.socrata.com/docs/queries/↩︎"
  },
  {
    "objectID": "wrangling/string-processing.html#sec-stringr",
    "href": "wrangling/string-processing.html#sec-stringr",
    "title": "\n16  String processing\n",
    "section": "\n16.1 The stringr package",
    "text": "16.1 The stringr package\n\nlibrary(tidyverse)\nlibrary(stringr)\n\nIn general, string processing tasks can be divided into detecting, locating, extracting, or replacing patterns in strings. We will see several examples. The table below includes the functions available to you in the stringr package. We split them by task. We also include the R-base equivalent when available.\nAll these functions take a character vector as first argument. Also, for each function, operations are vectorized: the operation gets applied to each string in the vector.\nFinally, note that in this table we mention groups. These will be explained in Section Section 16.4.9.\n\n\n\n\n\n\n\n\nstringr\nTask\nDescription\nR-base\n\n\n\nstr_detect\nDetect\nIs the pattern in the string?\ngrepl\n\n\nstr_which\nDetect\nReturns the index of entries that contain the pattern.\ngrep\n\n\nstr_subset\nDetect\nReturns the subset of strings that contain the pattern.\n\ngrep with value = TRUE\n\n\n\nstr_locate\nLocate\nReturns positions of first occurrence of pattern in a string.\nregexpr\n\n\nstr_locate_all\nLocate\nReturns position of all occurrences of pattern in a string.\ngregexpr\n\n\nstr_view\nLocate\nShow the first part of the string that matches pattern.\n\n\n\nstr_view_all\nLocate\nShow me all the parts of the string that match the pattern.\n\n\n\nstr_extract\nExtract\nExtract the first part of the string that matches the pattern.\n\n\n\nstr_extract_all\nExtract\nExtract all parts of the string that match the pattern.\n\n\n\nstr_match\nExtract\nExtract first part of the string that matches the groups and the patterns defined by the groups.\n\n\n\nstr_match_all\nExtract\nExtract all parts of the string that matches the groups and the patterns defined by the groups.\n\n\n\nstr_sub\nExtract\nExtract a substring.\nsubstring\n\n\nstr_split\nExtract\nSplit a string into a list with parts separated by pattern.\nstrsplit\n\n\nstr_split_fixed\nExtract\nSplit a string into a matrix with parts separated by pattern.\n\nstrsplit with fixed = TRUE\n\n\n\nstr_count\nDescribe\nCount number of times a pattern appears in a string.\n\n\n\nstr_length\nDescribe\nNumber of character in string.\nnchar\n\n\nstr_replace\nReplace\nReplace first part of a string matching a pattern with another.\n\n\n\nstr_replace_all\nReplace\nReplace all parts of a string matching a pattern with another.\ngsub\n\n\nstr_to_upper\nReplace\nChange all characters to upper case.\ntoupper\n\n\nstr_to_lower\nReplace\nChange all characters to lower case.\ntolower\n\n\nstr_to_title\nReplace\nChange first character to upper and rest to lower.\n\n\n\nstr_replace_na\nReplace\nReplace all NAs to a new value.\n\n\n\nstr_trim\nReplace\nRemove white space from start and end of string.\n\n\n\nstr_c\nManipulate\nJoin multiple strings.\npaste0\n\n\nstr_conv\nManipulate\nChange the encoding of the string.\n\n\n\nstr_sort\nManipulate\nSort the vector in alphabetical order.\nsort\n\n\nstr_order\nManipulate\nIndex needed to order the vector in alphabetical order.\norder\n\n\nstr_trunc\nManipulate\nTruncate a string to a fixed size.\n\n\n\nstr_pad\nManipulate\nAdd white space to string to make it a fixed size.\n\n\n\nstr_dup\nManipulate\nRepeat a string.\n\nrep then paste\n\n\n\nstr_wrap\nManipulate\nWrap things into formatted paragraphs.\n\n\n\nstr_interp\nManipulate\nString interpolation.\nsprintf"
  },
  {
    "objectID": "wrangling/string-processing.html#case-study-1-self-reported-heights",
    "href": "wrangling/string-processing.html#case-study-1-self-reported-heights",
    "title": "\n16  String processing\n",
    "section": "\n16.2 Case study 1: self-reported heights",
    "text": "16.2 Case study 1: self-reported heights\nThe dslabs package includes the raw data from which the heights dataset was obtained. You can load it like this:\n\nlibrary(dslabs)\nhead(reported_heights)\n#&gt;            time_stamp    sex height\n#&gt; 1 2014-09-02 13:40:36   Male     75\n#&gt; 2 2014-09-02 13:46:59   Male     70\n#&gt; 3 2014-09-02 13:59:20   Male     68\n#&gt; 4 2014-09-02 14:51:53   Male     74\n#&gt; 5 2014-09-02 15:16:15   Male     61\n#&gt; 6 2014-09-02 15:16:16 Female     65\n\nThese heights were obtained using a web form in which students were asked to enter their heights. They could enter anything, but the instructions asked for height in inches, a number. We compiled 1,095 submissions, but unfortunately the column vector with the reported heights had several non-numeric entries and as a result became a character vector:\n\nclass(reported_heights$height)\n#&gt; [1] \"character\"\n\nIf we try to parse it into numbers, we get a warning:\n\nx &lt;- as.numeric(reported_heights$height)\n#&gt; Warning: NAs introduced by coercion\n\nAlthough most values appear to be height in inches as requested we do end up with many NAs:\n\nsum(is.na(x))\n#&gt; [1] 81\n\nHere are some of the entries that are not successfully converted:\n\nreported_heights |&gt; \n  mutate(new_height = as.numeric(height)) |&gt;\n  filter(is.na(new_height)) |&gt; \n  head(n = 10)\n#&gt;             time_stamp    sex                 height new_height\n#&gt; 1  2014-09-02 15:16:28   Male                  5' 4\"         NA\n#&gt; 2  2014-09-02 15:16:37 Female                  165cm         NA\n#&gt; 3  2014-09-02 15:16:52   Male                    5'7         NA\n#&gt; 4  2014-09-02 15:16:56   Male                  &gt;9000         NA\n#&gt; 5  2014-09-02 15:16:56   Male                   5'7\"         NA\n#&gt; 6  2014-09-02 15:17:09 Female                   5'3\"         NA\n#&gt; 7  2014-09-02 15:18:00   Male 5 feet and 8.11 inches         NA\n#&gt; 8  2014-09-02 15:19:48   Male                   5'11         NA\n#&gt; 9  2014-09-04 00:46:45   Male                  5'9''         NA\n#&gt; 10 2014-09-04 10:29:44   Male                 5'10''         NA\n\nWe immediately see what is happening. Some of the students did not report their heights in inches as requested. We could discard these data and continue. However, many of the entries follow patterns that, in principle, we can easily convert to inches. For example, in the output above, we see various cases that use the format x'y'' with x and y representing feet and inches, respectively. Each one of these cases can be read and converted to inches by a human, for example 5'4'' is 5*12 + 4 = 64. So we could fix all the problematic entries by hand. However, humans are prone to making mistakes, so an automated approach is preferable. Also, because we plan on continuing to collect data, it will be convenient to write code that automatically corrects entries entered in error.\nA first step in this type of task is to survey the problematic entries and try to define specific patterns followed by a large groups of entries. The larger these groups, the more entries we can fix with a single programmatic approach. We want to find patterns that can be accurately described with a rule, such as “a digit, followed by a feet symbol, followed by one or two digits, followed by an inches symbol”.\nTo look for such patterns, it helps to remove the entries that are consistent with being in inches and to view only the problematic entries. We thus write a function to automatically do this. We keep entries that either result in NAs when applying as.numeric or are outside a range of plausible heights. We permit a range that covers about 99.9999% of the adult population. We also use suppressWarnings to avoid the warning message we know as.numeric will gives us.\nWe apply this function and find the number of problematic entries:\n\nproblems &lt;- reported_heights |&gt; \n  mutate(inches = suppressWarnings(as.numeric(x))) |&gt;\n  filter(is.na(inches) | inches &lt; 50 | inches &gt; 84) |&gt;\n  pull(height)\nlength(problems)\n#&gt; [1] 292\n\nWe can now view all the cases by simply printing them. If we do, we see that three patterns can be used to define three large groups within these exceptions.\n1. A pattern of the form x'y or x' y'' or x'y\" with x and y representing feet and inches, respectively. Here are ten examples:\n\n#&gt; 5' 4\" 5'7 5'7\" 5'3\" 5'11 5'9'' 5'10'' 5' 10 5'5\" 5'2\"\n\n2. A pattern of the form x.y or x,y with x feet and y inches. Here are ten examples:\n\n#&gt; 5.3 5.5 6.5 5.8 5.6 5,3 5.9 6,8 5.5 6.2\n\n3. Entries that were reported in centimeters rather than inches. Here are ten examples:\n\n#&gt; 150 175 177 178 163 175 178 165 165 180\n\nOnce we see these large groups following specific patterns, we can develop a plan of attack.\n\nConvert entries fitting the first two patterns into one standardized one.\nLeverage the standardization to extract the feet and inches and convert to inches.\nDefine a procedure for identifying entries that are in centimeters and convert them to inches.\nCheck again to see what entries were not fixed and see if we can tweak our approach to be more comprehensive.\n\nAt the end, we hope to have a script that makes web-based data collection methods robust to the most common user mistakes.\nRemember that there is rarely just one way to perform these tasks. Here we pick one that helps us teach several useful techniques. But surely there is a more efficient way of performing the task.\nTo achieve our goal, we will use a technique that enables us to accurately detect patterns and extract the parts we want: regular expressions (regex). But first, we quickly describe how to escape the function of certain characters so that they can be included in strings."
  },
  {
    "objectID": "wrangling/string-processing.html#escaping",
    "href": "wrangling/string-processing.html#escaping",
    "title": "\n16  String processing\n",
    "section": "\n16.3 Escaping",
    "text": "16.3 Escaping\nTo define strings in R, we can use either double quotes or single quotes:\n\ns &lt;- \"Hello!\"\ns &lt;- 'Hello!' \n\nMake sure you choose the correct single quote, as opposed to the back quote `.\nNow, what happens if the string we want to define includes double quotes? For example, if we want to write 10 inches like this 10\"? In this case you can’t use:\n\ns &lt;- \"10\"\"\n\nbecause this is just the string 10 followed by a double quote. If you type this into R, you get an error because you failed to close the double quote. To avoid this, we can use the single quotes:\n\ns &lt;- '10\"'\n\nIf we print out s we see that the double quotes are escaped with the backslash \\.\n\ns\n#&gt; [1] \"10\\\"\"\n\nIn fact, escaping with the backslash provides a way to define the string while still using the double quotes to define strings:\n\ns &lt;- \"10\\\"\"\n\nIn R, the function cat lets us see what the string actually looks like:\n\ncat(s)\n#&gt; 10\"\n\nNow, what if we want our string to be 5 feet written like this 5'? In this case, we can use the double quotes or escape the single quote\n\ns &lt;- \"5'\"\ns &lt;- '5\\''\n\nSo we’ve learned how to write 5 feet and 10 inches separately, but what if we want to write them together to represent 5 feet and 10 inches like this 5'10\"? In this case, neither the single nor double quotes will work since '5'10\"' closes the string after 5 and this \"5'10\"\" closes the string after 10. Keep in mind that if we type one of the above code snippets into R, it will get stuck waiting for you to close the open quote and you will have to exit the execution with the esc button.\nTo achieve the desired result we need to escape both quotes with the backslash \\. You can escape either character that can be confused with a closing quote. These are the two options:\n\ns &lt;- '5\\'10\"'\ns &lt;- \"5'10\\\"\"\n\nEscaping characters is something we often have to use when processing strings. Another characters that often needs escaping is the escape chatacter itself. We can do this with \\\\. When using regular expression, the topic of the next section, we often have to escape the special characters used in this approach."
  },
  {
    "objectID": "wrangling/string-processing.html#sec-regex",
    "href": "wrangling/string-processing.html#sec-regex",
    "title": "\n16  String processing\n",
    "section": "\n16.4 Regular expressions",
    "text": "16.4 Regular expressions\nA regular expression (regex) is a way to describe specific patterns of characters of text. They can be used to determine if a given string matches the pattern. A set of rules has been defined to do this efficiently and precisely and here we show some examples. We can learn more about these rules by reading a detailed tutorials1 2. This RStudio cheat sheet3 is also very useful.\nThe patterns supplied to the stringr functions can be a regex rather than a standard string. We will learn how this works through a series of examples.\nThroughout this section you will see that we create strings to test out our regex. To do this, we define patterns that we know should match and also patterns that we know should not. We will call them yes and no, respectively. This permits us to check for the two types of errors: failing to match and incorrectly matching.\n\n16.4.1 Strings are a regexp\nTechnically any string is a regex, perhaps the simplest example is a single character. So the comma , used in the next code example is a simple example of searching with regex.\n\npattern &lt;- \",\"\nstr_detect(c(\"1\", \"10\", \"100\", \"1,000\", \"10,000\"), pattern) \n#&gt; [1] FALSE FALSE FALSE  TRUE  TRUE\n\nAbove, we noted that an entry included a cm. This is also a simple example of a regex. We can show all the entries that used cm like this:\n\nstr_subset(reported_heights$height, \"cm\")\n#&gt; [1] \"165cm\"  \"170 cm\"\n\n\n16.4.2 Special characters\nNow let’s consider a slightly more complicated example. Which of the following strings contain the pattern cm or inches?\n\nyes &lt;- c(\"180 cm\", \"70 inches\")\nno &lt;- c(\"180\", \"70''\")\ns &lt;- c(yes, no)\n\nWe can do this with two searches:\n\nstr_detect(s, \"cm\") | str_detect(s, \"inches\")\n#&gt; [1]  TRUE  TRUE FALSE FALSE\n\nHowever, we don’t need to do this. The main feature that distinguishes the regex language from plain strings is that we can use special characters. These are characters with a meaning. We start by introducing | which means or. So if we want to know if either cm or inches appears in the strings, we can use the regex cm|inches:\n\nstr_detect(s, \"cm|inches\")\n#&gt; [1]  TRUE  TRUE FALSE FALSE\n\nand obtain the correct answer.\nAnother special character that will be useful for identifying feet and inches values is \\d which means any digit: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9. The backslash is used to distinguish it from the character d. In R, we have to escape the backslash \\ so we actually have to use \\\\d to represent digits. Here is an example:\n\nyes &lt;- c(\"5\", \"6\", \"5'10\", \"5 feet\", \"4'11\")\nno &lt;- c(\"\", \".\", \"Five\", \"six\")\ns &lt;- c(yes, no)\npattern &lt;- \"\\\\d\"\nstr_detect(s, pattern)\n#&gt; [1]  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE\n\nWe take this opportunity to introduce the str_view function, which is helpful for troubleshooting as it shows us the first match for each string:\n\nstr_view(s, pattern)\n\n\nand str_view_all shows us all the matches, so 3'2 has two matches and 5'10 has three.\n\nstr_view_all(s, pattern)\n\n\nThere are many other special characters. We will learn some others below, but you can see most or all of them in the cheat sheet4 mentioned earlier.\nFinally, a useful special character is \\w which stands for word character and it matches any letter, number, or underscore.\n\n16.4.3 Character classes\nCharacter classes are used to define a series of characters that can be matched. We define character classes with square brackets []. So, for example, if we want the pattern to match only if we have a 5 or a 6, we use the regex [56]:\n\nstr_view(s, \"[56]\")\n\n\nSuppose we want to match values between 4 and 7. A common way to define character classes is with ranges. So, for example, [0-9] is equivalent to \\\\d. The pattern we want is therefore [4-7].\n\nyes &lt;- as.character(4:7)\nno &lt;- as.character(1:3)\ns &lt;- c(yes, no)\nstr_detect(s, \"[4-7]\")\n#&gt; [1]  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE\n\nHowever, it is important to know that in regex everything is a character; there are no numbers. So 4 is the character 4 not the number four. Notice, for example, that [1-20] does not mean 1 through 20, it means the characters 1 through 2 or the character 0. So [1-20] simply means the character class composed of 0, 1, and 2.\nKeep in mind that characters do have an order and the digits do follow the numeric order. So 0 comes before 1 which comes before 2 and so on. For the same reason, we can define lower case letters as [a-z], upper case letters as [A-Z], and [a-zA-z] as both.\nNotice that \\w is equivalent to [a-zA-Z0-9_].\n\n16.4.4 Anchors\nWhat if we want a match when we have exactly 1 digit? This will be useful in our case study since feet are never more than 1 digit so a restriction will help us. One way to do this with regex is by using anchors, which let us define patterns that must start or end at a specific place. The two most common anchors are ^ and $ which represent the beginning and end of a string, respectively. So the pattern ^\\\\d$ is read as “start of the string followed by one digit followed by end of string”.\nThis pattern now only detects the strings with exactly one digit:\n\npattern &lt;- \"^\\\\d$\"\nyes &lt;- c(\"1\", \"5\", \"9\")\nno &lt;- c(\"12\", \"123\", \" 1\", \"a4\", \"b\")\ns &lt;- c(yes, no)\nstr_view_all(s, pattern)\n\n\nThe 1 does not match because it does not start with the digit but rather with a space, which is actually not easy to see.\n\n16.4.5 Bounded quantifiers\nFor the inches part, we can have one or two digits. This can be specified in regex with quantifiers. This is done by following the pattern with curly brackets containing the number of times the previous entry can be repeated. We call the bounded because the numbers in the quantifier are limited by the numbers in the curly brackets. Later we learn about unbounded quantifiers.\nWe use an example to illustrate. The pattern for one or two digits is:\n\npattern &lt;- \"^\\\\d{1,2}$\"\nyes &lt;- c(\"1\", \"5\", \"9\", \"12\")\nno &lt;- c(\"123\", \"a4\", \"b\")\nstr_view(c(yes, no), pattern)\n\n\nIn this case, 123 does not match, but 12 does. So to look for our feet and inches pattern, we can add the symbols for feet ' and inches \" after the digits.\nWith what we have learned, we can now construct an example for the pattern x'y\\\" with x feet and y inches.\n\npattern &lt;- \"^[4-7]'\\\\d{1,2}\\\"$\"\n\nThe pattern is now getting complex, but you can look at it carefully and break it down:\n\n\n^ = start of the string\n\n[4-7] = one digit, either 4,5,6 or 7\n\n' = feet symbol\n\n\\\\d{1,2} = one or two digits\n\n\\\" = inches symbol\n\n$ = end of the string\n\nLet’s test it out:\n\nyes &lt;- c(\"5'7\\\"\", \"6'2\\\"\",  \"5'12\\\"\")\nno &lt;- c(\"6,2\\\"\", \"6.2\\\"\",\"I am 5'11\\\"\", \"3'2\\\"\", \"64\")\nstr_detect(yes, pattern)\n#&gt; [1] TRUE TRUE TRUE\nstr_detect(no, pattern)\n#&gt; [1] FALSE FALSE FALSE FALSE FALSE\n\nFor now, we are permitting the inches to be 12 or larger. We will add a restriction later as the regex for this is a bit more complex than we are ready to show.\n\n16.4.6 White space \\s\n\nAnother problem we have are spaces. For example, our pattern does not match 5' 4\" because there is a space between ' and 4 which our pattern does not permit. Spaces are characters and R does not ignore them:\n\nidentical(\"Hi\", \"Hi \")\n#&gt; [1] FALSE\n\nIn regex, \\s represents white space. To find patterns like 5' 4, we can change our pattern to:\n\npattern_2 &lt;- \"^[4-7]'\\\\s\\\\d{1,2}\\\"$\"\nstr_subset(problems, pattern_2)\n#&gt; [1] \"5' 4\\\"\"  \"5' 11\\\"\" \"5' 7\\\"\"\n\nHowever, this will not match the patterns with no space. So do we need more than one regex pattern? It turns out we can use a quantifier for this as well.\n\n16.4.7 Unbounded quantifiers: *, ?, +\n\nWe want the pattern to permit spaces but not require them. Even if there are several spaces, like in this example 5'   4, we still want it to match. There is a quantifier for exactly this purpose. In regex, the character * means zero or more instances of the previous character. Here is an example:\n\nyes &lt;- c(\"AB\", \"A1B\", \"A11B\", \"A111B\", \"A1111B\")\nno &lt;- c(\"A2B\", \"A21B\")\nstr_detect(yes, \"A1*B\")\n#&gt; [1] TRUE TRUE TRUE TRUE TRUE\nstr_detect(no, \"A1*B\")\n#&gt; [1] FALSE FALSE\n\nThe above matches the first string which has zero 1s and all the strings with one or more 1. We can then improve our pattern by adding the * after the space character \\s.\nThere are two other similar quantifiers. For none or once, we can use ?, and for one or more, we can use +. You can see how they differ with this example:\n\ndata.frame(string = c(\"AB\", \"A1B\", \"A11B\", \"A111B\", \"A1111B\"),\n           none_or_more = str_detect(yes, \"A1*B\"),\n           nore_or_once = str_detect(yes, \"A1?B\"),\n           once_or_more = str_detect(yes, \"A1+B\"))\n#&gt;   string none_or_more nore_or_once once_or_more\n#&gt; 1     AB         TRUE         TRUE        FALSE\n#&gt; 2    A1B         TRUE         TRUE         TRUE\n#&gt; 3   A11B         TRUE        FALSE         TRUE\n#&gt; 4  A111B         TRUE        FALSE         TRUE\n#&gt; 5 A1111B         TRUE        FALSE         TRUE\n\nWe will actually use all three in our reported heights example, but we will see these in a later section.\n\n16.4.8 Not\nTo specify patterns that we do not want to detect, we can use the ^ symbol but only inside square brackets. Remember that outside the square bracket ^ means the start of the string. So, for example, if we want to detect digits that are preceded by anything except a letter we can do the following:\n\npattern &lt;- \"[^a-zA-Z]\\\\d\"\nyes &lt;- c(\".3\", \"+2\", \"-0\",\"*4\")\nno &lt;- c(\"A3\", \"B2\", \"C0\", \"E4\")\nstr_detect(yes, pattern)\n#&gt; [1] TRUE TRUE TRUE TRUE\nstr_detect(no, pattern)\n#&gt; [1] FALSE FALSE FALSE FALSE\n\nAnother way to generate a pattern that searches for everything except is to use the upper case of the special character. For example \\\\D means anything other than a digit, \\\\S means anything except a space, and so on.\n\n16.4.9 Groups\nGroups are a powerful aspect of regex that permits the extraction of values. Groups are defined using parentheses. They don’t affect the pattern matching per se. Instead, it permits tools to identify specific parts of the pattern so we can extract them.\nWe want to change heights written like 5.6 to 5'6.\nTo avoid changing patterns such as 70.2, we will require that the first digit be between 4 and 7 [4-7] and that the second be none or more digits \\\\d*. Let’s start by defining a simple pattern that matches this:\n\npattern_without_groups &lt;- \"^[4-7],\\\\d*$\"\n\nWe want to extract the digits so we can then form the new version using a period. These are our two groups, so we encapsulate them with parentheses:\n\npattern_with_groups &lt;-  \"^([4-7]),(\\\\d*)$\"\n\nWe encapsulate the part of the pattern that matches the parts we want to keep for later use. Adding groups does not affect the detection, since it only signals that we want to save what is captured by the groups. Note that both patterns return the same result when using str_detect:\n\nyes &lt;- c(\"5,9\", \"5,11\", \"6,\", \"6,1\")\nno &lt;- c(\"5'9\", \",\", \"2,8\", \"6.1.1\")\ns &lt;- c(yes, no)\nstr_detect(s, pattern_without_groups)\n#&gt; [1]  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE\nstr_detect(s, pattern_with_groups)\n#&gt; [1]  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE\n\nOnce we define groups, we can use the function str_match to extract the values these groups define:\n\nstr_match(s, pattern_with_groups)\n#&gt;      [,1]   [,2] [,3]\n#&gt; [1,] \"5,9\"  \"5\"  \"9\" \n#&gt; [2,] \"5,11\" \"5\"  \"11\"\n#&gt; [3,] \"6,\"   \"6\"  \"\"  \n#&gt; [4,] \"6,1\"  \"6\"  \"1\" \n#&gt; [5,] NA     NA   NA  \n#&gt; [6,] NA     NA   NA  \n#&gt; [7,] NA     NA   NA  \n#&gt; [8,] NA     NA   NA\n\nNotice that the second and third columns contain feet and inches, respectively. The first column is the part of the string matching the pattern. If no match occurred, we see an NA.\nNow we can understand the difference between the functions str_extract and str_match. str_extract extracts only strings that match a pattern, not the values defined by groups:\n\nstr_extract(s, pattern_with_groups)\n#&gt; [1] \"5,9\"  \"5,11\" \"6,\"   \"6,1\"  NA     NA     NA     NA\n\n\n16.4.10 Search and replace\nEarlier we defined the object problems containing the strings that do not appear to be in inches. We can see that not too many of our problematic strings match the pattern:\n\npattern &lt;- \"^[4-7]'\\\\d{1,2}\\\"$\"\nsum(str_detect(problems, pattern))\n#&gt; [1] 14\n\nTo see why this is, we show some examples that expose why we don’t have more matches:\n\nproblems[c(2, 10, 11, 12, 15)] |&gt; str_view(pattern)\n\n\nAn initial problem we see immediately is that some students wrote out the words “feet” and “inches”. We can see the entries that did this with the str_subset function:\n\nstr_subset(problems, \"inches\")\n#&gt; [1] \"5 feet and 8.11 inches\" \"Five foot eight inches\"\n#&gt; [3] \"5 feet 7inches\"         \"5ft 9 inches\"          \n#&gt; [5] \"5 ft 9 inches\"          \"5 feet 6 inches\"\n\nWe also see that some entries used two single quotes '' instead of a double quote \".\n\nstr_subset(problems, \"''\")\n#&gt;  [1] \"5'9''\"   \"5'10''\"  \"5'10''\"  \"5'3''\"   \"5'7''\"   \"5'6''\"  \n#&gt;  [7] \"5'7.5''\" \"5'7.5''\" \"5'10''\"  \"5'11''\"  \"5'10''\"  \"5'5''\"\n\nTo correct this, we can replace the different ways of representing inches and feet with a uniform symbol. We will use ' for feet, whereas for inches we will simply not use a symbol since some entries were of the form x'y. Now, if we no longer use the inches symbol, we have to change our pattern accordingly:\n\npattern &lt;- \"^[4-7]'\\\\d{1,2}$\"\n\nIf we do this replacement before the matching, we get many more matches:\n\nproblems |&gt; \n  str_replace(\"feet|ft|foot\", \"'\") |&gt; # replace feet, ft, foot with ' \n  str_replace(\"inches|in|''|\\\"\", \"\") |&gt; # remove all inches symbols\n  str_detect(pattern) |&gt; \n  sum()\n#&gt; [1] 48\n\nHowever, we still have many cases to go.\nNote that in the code above, we leveraged the stringr consistency and used the pipe.\nFor now, we improve our pattern by adding \\\\s* in front of and after the feet symbol ' to permit space between the feet symbol and the numbers. Now we match a few more entries:\n\npattern &lt;- \"^[4-7]\\\\s*'\\\\s*\\\\d{1,2}$\"\nproblems |&gt; \n  str_replace(\"feet|ft|foot\", \"'\") |&gt; # replace feet, ft, foot with ' \n  str_replace(\"inches|in|''|\\\"\", \"\") |&gt; # remove all inches symbols\n  str_detect(pattern) |&gt; \n  sum()\n#&gt; [1] 53\n\nWe might be tempted to avoid doing this by removing all the spaces with str_replace_all. However, when doing such an operation we need to make sure that it does not have unintended effects. In our reported heights examples, this will be a problem because some entries are of the form x y with space separating the feet from the inches. If we remove all spaces, we will incorrectly turn x y into xy which implies that a 6 1 would become 61 inches instead of 73 inches.\nThe second large type of problematic entries were of the form x.y, x,y and x y. We want to change all these to our common format x'y. But we can’t just do a search and replace because we would change values such as 70.5 into 70'5. Our strategy will therefore be to search for a very specific pattern that assures us feet and inches are being provided and then, for those that match, replace appropriately.\n\n16.4.11 Search and replace using groups\nAnother powerful aspect of groups is that you can refer to the extracted values in a regex when searching and replacing.\nThe regex special character for the i-th group is \\\\i. So \\\\1 is the value extracted from the first group, \\\\2 the value from the second and so on. As a simple example, note that the following code will replace a comma with period, but only if it is between two digits:\n\npattern_with_groups &lt;-  \"^([4-7]),(\\\\d*)$\"\nyes &lt;- c(\"5,9\", \"5,11\", \"6,\", \"6,1\")\nno &lt;- c(\"5'9\", \",\", \"2,8\", \"6.1.1\")\ns &lt;- c(yes, no)\nstr_replace(s, pattern_with_groups, \"\\\\1'\\\\2\")\n#&gt; [1] \"5'9\"   \"5'11\"  \"6'\"    \"6'1\"   \"5'9\"   \",\"     \"2,8\"   \"6.1.1\"\n\nWe can use this to convert cases in our reported heights.\nWe are now ready to define a pattern that helps us convert all the x.y, x,y and x y to our preferred format. We need to adapt pattern_with_groups to be a bit more flexible and capture all the cases.\n\npattern_with_groups &lt;- \"^([4-7])\\\\s*[,\\\\.\\\\s+]\\\\s*(\\\\d*)$\"\n\nLet’s break this one down:\n\n\n^ = start of the string\n\n[4-7] = one digit, either 4, 5, 6, or 7\n\n\\\\s* = none or more white space\n\n[,\\\\.\\\\s+] = feet symbol is either ,, . or at least one space\n\n\\\\s* = none or more white space\n\n\\\\d* = none or more digits\n\n$ = end of the string\n\nWe can see that it appears to be working:\n\nstr_subset(problems, pattern_with_groups) |&gt; head()\n#&gt; [1] \"5.3\"  \"5.25\" \"5.5\"  \"6.5\"  \"5.8\"  \"5.6\"\n\nand will be able to perform the search and replace:\n\nstr_subset(problems, pattern_with_groups) |&gt; \n  str_replace(pattern_with_groups, \"\\\\1'\\\\2\") |&gt; head()\n#&gt; [1] \"5'3\"  \"5'25\" \"5'5\"  \"6'5\"  \"5'8\"  \"5'6\"\n\nAgain, we will deal with the inches-larger-than-twelve challenge later.\n\n16.4.12 Lookarounds\nLookarounds provide a way to ask for one or more conditions to be satisfied without moving the search forward or matching it. For example, you might want to check for multiple conditions and if they are matched, then return the pattern or part of the pattern that matched.\nThere are four types of lookarounds: lookahead (?=pattern), lookbehind (?&lt;=pattern), negative lookahead (?!pattern), and negative lookbehind (?&lt;!pattern).\nThe conventional example checking password that must satisfy several conditions such as 1) 8-16 word characters, 2) starts with a letter, and 3) has at least one digit. You can concatenate lookarounds to check for multiple conditions. For our example we can write\n\npattern &lt;- \"(?=\\\\w{8,16})(?=^[a-z|A-Z].*)(?=.*\\\\d+.*)\"\n\nA simpler example is changing all superman to supergirl without changing all the men to girl. We could use a lookaround like this:\n\ns &lt;- \"Superman saved a man. The man thanked superman.\"\nstr_replace_all(s, \"(?&lt;=[Ss]uper)man\", \"girl\")\n#&gt; [1] \"Supergirl saved a man. The man thanked supergirl.\"\n\n\n16.4.13 Separating variables\nIn Section 11.3 we introduced functions that can split columns into new ones. The separate_wider_regex uses regex instead of delimiters to separate variables in data frames. It uses an approach similar to regex groups ts of the original column we keep in which new column.\nSuppose we have data frame like this:\n\ntab &lt;- data.frame(x = c(\"5'10\", \"6' 1\", \"5 ' 9\", \"5'11\\\"\"))\n\nNote that using separate_wider_delim won’t work here because the delimiter can varies across entries. With separate_wider_regex we can define flexible patterns that are matched to define each column.\n\npatterns &lt;- c(feet = \"\\\\d\", \"\\\\s*'\\\\s*\", inches = \"\\\\d{1,2}\", \".*\")\ntab |&gt; separate_wider_regex(x, patterns = patterns)\n#&gt; # A tibble: 4 × 2\n#&gt;   feet  inches\n#&gt;   &lt;chr&gt; &lt;chr&gt; \n#&gt; 1 5     10    \n#&gt; 2 6     1     \n#&gt; 3 5     9     \n#&gt; 4 5     11\n\nBy not naming the second and fourth entries of patterns we tell the function not to keep the values that match that pattern."
  },
  {
    "objectID": "wrangling/string-processing.html#trimming",
    "href": "wrangling/string-processing.html#trimming",
    "title": "\n16  String processing\n",
    "section": "\n16.5 Trimming",
    "text": "16.5 Trimming\nIn general, spaces at the start or end of the string are uninformative. These can be particularly deceptive because sometimes they can be hard to see:\n\ns &lt;- \"Hi \"\ncat(s)\n#&gt; Hi\nidentical(s, \"Hi\")\n#&gt; [1] FALSE\n\nThis is a general enough problem that there is a function dedicated to removing them: str_trim.\n\nstr_trim(\"5 ' 9 \")\n#&gt; [1] \"5 ' 9\""
  },
  {
    "objectID": "wrangling/string-processing.html#case-conversion",
    "href": "wrangling/string-processing.html#case-conversion",
    "title": "\n16  String processing\n",
    "section": "\n16.6 Case conversion",
    "text": "16.6 Case conversion\nNotice that regex is case sensitive. Often we want to match a word regardless of case. One approach to doing this is to first change everything to lower case and then proceeding ignoring case. As an example, note that one of the entries writes out numbers as words Five foot eight inches. Although not efficient, we could add 13 extra str_replace calls to convert zero to 0, one to 1, and so on. To avoid having to write two separate operations for Zero and zero, One and one, etc., we can use the str_to_lower function to make all works lower case first:\n\ns &lt;- c(\"Five feet eight inches\")\nstr_to_lower(s)\n#&gt; [1] \"five feet eight inches\"\n\nOther related functions are str_to_upper and str_to_title. We are now ready to define a procedure that converts all the problematic cases to inches."
  },
  {
    "objectID": "wrangling/string-processing.html#case-study-1-putting-it-all-together",
    "href": "wrangling/string-processing.html#case-study-1-putting-it-all-together",
    "title": "\n16  String processing\n",
    "section": "\n16.7 Case study 1: Putting it all together",
    "text": "16.7 Case study 1: Putting it all together\nWe are now ready to put it all together and wrangle our reported heights data to try to recover as many heights as possible. The code is complex, but we will break it down into parts.\nLet’s see how many problematic entries we can recover with the approaches we covered in Section 16.4. We will first define a function that detects entries that are not reported as inches or centimeters:\n\nnot_inches_or_cm &lt;- function(x, smallest = 50, tallest = 84){\n  inches &lt;- suppressWarnings(as.numeric(x))\n  is.na(inches) |\n    !((inches &gt;= smallest & inches &lt;= tallest) |\n        (inches/2.54 &gt;= smallest & inches/2.54 &lt;= tallest))\n}\n\nWe can see how many entries are not inches or centimeters:\n\nproblems &lt;- reported_heights |&gt; \n  filter(not_inches_or_cm(height)) |&gt;\n  pull(height)\nlength(problems)\n#&gt; [1] 200\n\nLet’s see how many feet'inches format we can recover applying the finding from Section 16.4. Specifically we replace feet/foot/ft with ', we remove the word inches and its abbreviations, then rewrite similar patterns into the desired feet'inches pattern. Once this is done we see what proportion don’t fit the desired pattern\n\nconverted &lt;- problems |&gt; \n  str_replace(\"feet|foot|ft\", \"'\") |&gt; \n  str_remove_all(\"inches|in|''|\\\"\") |&gt;  \n  str_replace(\"^([4-7])\\\\s*[,\\\\.\\\\s+]\\\\s*(\\\\d*)$\", \"\\\\1'\\\\2\") \nindex &lt;- str_detect(converted, \"^[4-7]\\\\s*'\\\\s*\\\\d{1,2}$\")\nmean(!index)\n#&gt; [1] 0.385\n\nWe see that a substantial proportion has not yet been fixed. Trial and error is a common approach to finding the regex pattern that satisfies all desired conditions. We can examine the remaining cases to try to decipher if new patterns we can use to fix them:\n\nconverted[!index]\n#&gt;  [1] \"6\"             \"165cm\"         \"511\"           \"6\"            \n#&gt;  [5] \"2\"             \"&gt;9000\"         \"5 ' and 8.11 \" \"11111\"        \n#&gt;  [9] \"6\"             \"103.2\"         \"19\"            \"5\"            \n#&gt; [13] \"300\"           \"6'\"            \"6\"             \"Five ' eight \"\n#&gt; [17] \"7\"             \"214\"           \"6\"             \"0.7\"          \n#&gt; [21] \"6\"             \"2'33\"          \"612\"           \"1,70\"         \n#&gt; [25] \"87\"            \"5'7.5\"         \"5'7.5\"         \"111\"          \n#&gt; [29] \"5' 7.78\"       \"12\"            \"6\"             \"yyy\"          \n#&gt; [33] \"89\"            \"34\"            \"25\"            \"6\"            \n#&gt; [37] \"6\"             \"22\"            \"684\"           \"6\"            \n#&gt; [41] \"1\"             \"1\"             \"6*12\"          \"87\"           \n#&gt; [45] \"6\"             \"1.6\"           \"120\"           \"120\"          \n#&gt; [49] \"23\"            \"1.7\"           \"6\"             \"5\"            \n#&gt; [53] \"69\"            \"5' 9 \"         \"5 ' 9 \"        \"6\"            \n#&gt; [57] \"6\"             \"86\"            \"708,661\"       \"5 ' 6 \"       \n#&gt; [61] \"6\"             \"649,606\"       \"10000\"         \"1\"            \n#&gt; [65] \"728,346\"       \"0\"             \"6\"             \"6\"            \n#&gt; [69] \"6\"             \"100\"           \"88\"            \"6\"            \n#&gt; [73] \"170 cm\"        \"7,283,465\"     \"5\"             \"5\"            \n#&gt; [77] \"34\"\n\nWe noticed that two students added cm and some entries have space at the end so we write incorporate this into our cleanup stage. We also notice that at least one entry wrote out numbers such as Five foot eight inches. We can use the words() function in the english package to change these to actual numbers. In preparation for our final product, we define a function that cleans up previously noticed problems and these new ones:\n\nlibrary(english)\ncleanup &lt;- function(s){\n  s &lt;- str_remove_all(s, \"inches|in|''|\\\"|cm|and\") |&gt; \n    str_trim() |&gt;\n    str_to_lower()\n  for (i in 0:11)\n    s &lt;- str_replace_all(s, words(i), as.character(i))\n  return(s)\n}\n\nMany also notice that students measuring exactly 5 or 6 feet did not enter any inches, for example 6', and our pattern requires that inches be included, and that some entries are in meters and some of these use European decimals, for example 1.6 and 1,70. So we create a function that add these corrections to those already identified as needed to reformat the entry as feet'inches:\n\nconvert_format &lt;- function(s){\n  s |&gt; str_replace(\"feet|foot|ft\", \"'\") |&gt; \n    str_replace(\"^([4-7])\\\\s*[,\\\\.\\\\s+]\\\\s*(\\\\d*)$\", \"\\\\1'\\\\2\") |&gt; \n    str_replace(\"^([56])'?$\", \"\\\\1'0\") |&gt; \n    str_replace(\"^([12])\\\\s*,\\\\s*(\\\\d*)$\", \"\\\\1\\\\.\\\\2\")\n}\n\nNow we are ready to wrangle our reported heights dataset. The strategy is as follows:\n1. We start by defining a variable to keep a copy of the original entry, we then clean up and covert the height entry using the functions described above.\n2. We use then use the separate_wider_regex_ function to extract feet and inches when the entry matches our feet'inches format.\n3. We use a lookaround so make sure that entries that have numbers with no ' following them don’t match and return an NA.\n\nOnce this is done we convert the feet and inches into inches.\nFinally, we decide if the entries are in inches, centimeters or meters and convert appropriately.\n\n\npatterns &lt;- c(feet = \"[4-7](?=\\\\s*'\\\\s*)\", \n              \"\\\\s*'\\\\s*\", \n              inches = \"\\\\d+\\\\.?\\\\d*\")\nsmallest &lt;- 50\ntallest &lt;- 84\nnew_heights &lt;- reported_heights |&gt; \n  mutate(original = height, \n         height = convert_format(cleanup(height))) |&gt;\n  separate_wider_regex(height, patterns = patterns, too_few = \"align_start\", cols_remove = FALSE) |&gt; \n  mutate(across(c(height, feet, inches), as.numeric)) |&gt;\n  mutate(guess = 12 * feet + inches) |&gt;\n  mutate(height = case_when(\n    is.na(height) ~ as.numeric(NA),\n    between(height, smallest, tallest) ~ height,  #inches\n    between(height/2.54, smallest, tallest) ~ height/2.54, #cm\n    between(height*100/2.54, smallest, tallest) ~ height*100/2.54, #meters\n    TRUE ~ as.numeric(NA))) |&gt;\n  mutate(height = ifelse(is.na(height) & \n                           inches &lt;= 12 & between(guess, smallest, tallest),\n                         guess, height)) |&gt;\n  select(-feet, -inches, -guess)\n\nWe see that we fix all but 44 entries. You can see that these are mostly un-fixable:\n\nnew_heights |&gt; filter(is.na(height)) |&gt; pull(original)\n#&gt;  [1] \"511\"       \"&gt;9000\"     \"5.25\"      \"11111\"     \"103.2\"    \n#&gt;  [6] \"19\"        \"300\"       \"5.75\"      \"7\"         \"214\"      \n#&gt; [11] \"0.7\"       \"2'33\"      \"612\"       \"87\"        \"111\"      \n#&gt; [16] \"12\"        \"yyy\"       \"89\"        \"34\"        \"25\"       \n#&gt; [21] \"22\"        \"684\"       \"1\"         \"1\"         \"6*12\"     \n#&gt; [26] \"87\"        \"120\"       \"120\"       \"23\"        \"5.51\"     \n#&gt; [31] \"5.69\"      \"86\"        \"708,661\"   \"5.25\"      \"649,606\"  \n#&gt; [36] \"10000\"     \"1\"         \"728,346\"   \"0\"         \"100\"      \n#&gt; [41] \"5.57\"      \"88\"        \"7,283,465\" \"34\"\n\nYou can review the ones we did fix by typing:\n\nnew_heights |&gt;\n  filter(not_inches_or_cm(original)) |&gt;\n  select(original, height) |&gt; \n  arrange(height) |&gt;\n  View()\n\nA final observation is that if we look at the shortest students in our course:\n\nnew_heights |&gt; arrange(height) |&gt; head(n = 6)\n#&gt; # A tibble: 6 × 4\n#&gt;   time_stamp          sex    height original\n#&gt;   &lt;chr&gt;               &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;   \n#&gt; 1 2017-07-04 01:30:25 Male       50 50      \n#&gt; 2 2017-09-07 10:40:35 Male       50 50      \n#&gt; 3 2014-09-02 15:18:30 Female     51 51      \n#&gt; 4 2016-06-05 14:07:20 Female     52 52      \n#&gt; 5 2016-06-05 14:07:38 Female     52 52      \n#&gt; # ℹ 1 more row\n\nWe see heights of 50, 51, 52, and so on. These short heights are rare and it is likely that the students actually meant 5’0, 5’1, 5’2 and so on. Because we are not completely sure, we will leave them as reported. The object new_heights contains our final solution for this case study."
  },
  {
    "objectID": "wrangling/string-processing.html#case-study-3-extracting-tables-from-a-pdf",
    "href": "wrangling/string-processing.html#case-study-3-extracting-tables-from-a-pdf",
    "title": "\n16  String processing\n",
    "section": "\n16.8 Case study 3: extracting tables from a PDF",
    "text": "16.8 Case study 3: extracting tables from a PDF\nOne of the datasets provided in dslabs shows scientific funding rates by gender in the Netherlands:\n\nlibrary(dslabs)\nresearch_funding_rates |&gt; \n  select(\"discipline\", \"success_rates_men\", \"success_rates_women\")\n#&gt;            discipline success_rates_men success_rates_women\n#&gt; 1   Chemical sciences              26.5                25.6\n#&gt; 2   Physical sciences              19.3                23.1\n#&gt; 3             Physics              26.9                22.2\n#&gt; 4          Humanities              14.3                19.3\n#&gt; 5  Technical sciences              15.9                21.0\n#&gt; 6   Interdisciplinary              11.4                21.8\n#&gt; 7 Earth/life sciences              24.4                14.3\n#&gt; 8     Social sciences              15.3                11.5\n#&gt; 9    Medical sciences              18.8                11.2\n\nThe data comes from a paper published in the Proceedings of the National Academy of Science (PNAS)5, a widely read scientific journal. However, the data is not provided in a spreadsheet; it is in a table in a PDF document. Here is a screenshot of the table:\n\n(Source: Romy van der Lee and Naomi Ellemers, PNAS 2015 112 (40) 12349-123536.)\nWe could extract the numbers by hand, but this could lead to human error. Instead, we can try to wrangle the data using R. We start by downloading the pdf document, then importing into R:\n\nlibrary(\"pdftools\")\ntemp_file &lt;- tempfile()\nurl &lt;- paste0(\"https://web.archive.org/web/20150927033124/\",\n              \"https://www.pnas.org/content/suppl/2015/09/16/\",\n              \"1510159112.DCSupplemental/pnas.201510159SI.pdf\")\ndownload.file(url, temp_file)\ntxt &lt;- pdf_text(temp_file)\nfile.remove(temp_file)\n\nIf we examine the object text, we notice that it is a character vector with an entry for each page. So we keep the page we want:\n\nraw_data_research_funding_rates &lt;- txt[2]\n\nThe steps above can actually be skipped because we include this raw data in the dslabs package as well.\nExamining the object raw_data_research_funding_rates we see that it is a long string and each line on the page, including the table rows, are separated by the symbol for newline: \\n. A first step in converting this into a data frame is to store each separate the rows in a way that will make it easy to access. For this we use the str_split function:\n\ntab &lt;- str_split(raw_data_research_funding_rates, \"\\n\")\n\nBecause we start off with just one element in the string, we end up with a list with just one entry.\n\ntab &lt;- tab[[1]]\n\nBy examining tab we see that the information for the column names is the third and fourth entries:\n\nthe_names_1 &lt;- tab[3]\nthe_names_2 &lt;- tab[4]\n\nThe first of these rows looks like this:\n\n#&gt;                                                       Applications, n\n#&gt;                   Awards, n                      Success rates, %\n\nWe want to create one vector with one name for each column. Using some of the functions we have just learned, we do this.\nLet’s start with the_names_1, shown above. We want to remove the leading space and anything following the comma. We use regex for the latter. Then we can obtain the elements by splitting strings separated by space. We want to split only when there are 2 or more spaces to avoid splitting Success rates. So we use the regex \\\\s{2,}\n\nthe_names_1 &lt;- the_names_1 |&gt;\n  str_trim() |&gt;\n  str_replace_all(\",\\\\s.\", \"\") |&gt;\n  str_split(\"\\\\s{2,}\", simplify = TRUE)\nthe_names_1 \n#&gt;      [,1]           [,2]     [,3]           \n#&gt; [1,] \"Applications\" \"Awards\" \"Success rates\"\n\nNow we will look at the_names_2:\n\n#&gt;                         Discipline              Total     Men      Women\n#&gt; n         Total    Men       Women          Total    Men      Women\n\nHere we want to trim the leading space and then split by space as we did for the first line:\n\nthe_names_2 &lt;- the_names_2 |&gt;\n  str_trim() |&gt;\n  str_split(\"\\\\s+\", simplify = TRUE)\nthe_names_2\n#&gt;      [,1]         [,2]    [,3]  [,4]    [,5]    [,6]  [,7]    [,8]   \n#&gt; [1,] \"Discipline\" \"Total\" \"Men\" \"Women\" \"Total\" \"Men\" \"Women\" \"Total\"\n#&gt;      [,9]  [,10]  \n#&gt; [1,] \"Men\" \"Women\"\n\nWe can then join these to generate one name for each column:\n\ntmp_names &lt;- paste(rep(the_names_1, each = 3), the_names_2[-1], sep = \"_\")\nthe_names &lt;- c(the_names_2[1], tmp_names) |&gt;\n  str_to_lower() |&gt;\n  str_replace_all(\"\\\\s\", \"_\")\nthe_names\n#&gt;  [1] \"discipline\"          \"applications_total\"  \"applications_men\"   \n#&gt;  [4] \"applications_women\"  \"awards_total\"        \"awards_men\"         \n#&gt;  [7] \"awards_women\"        \"success_rates_total\" \"success_rates_men\"  \n#&gt; [10] \"success_rates_women\"\n\nNow we are ready to get the actual data. By examining the tab object, we notice that the information is in lines 6 through 14. We can use str_split again to achieve our goal:\n\nnew_research_funding_rates &lt;- tab[6:14] |&gt;\n  str_trim() |&gt;\n  str_split(\"\\\\s{2,}\", simplify = TRUE) |&gt;\n  data.frame() |&gt;\n  setNames(the_names) |&gt;\n  mutate(across(-1, parse_number))\nnew_research_funding_rates |&gt; as_tibble()\n#&gt; # A tibble: 9 × 10\n#&gt;   discipline      applications_total applications_men applications_women\n#&gt;   &lt;chr&gt;                        &lt;dbl&gt;            &lt;dbl&gt;              &lt;dbl&gt;\n#&gt; 1 Chemical scien…                122               83                 39\n#&gt; 2 Physical scien…                174              135                 39\n#&gt; 3 Physics                         76               67                  9\n#&gt; 4 Humanities                     396              230                166\n#&gt; 5 Technical scie…                251              189                 62\n#&gt; # ℹ 4 more rows\n#&gt; # ℹ 6 more variables: awards_total &lt;dbl&gt;, awards_men &lt;dbl&gt;,\n#&gt; #   awards_women &lt;dbl&gt;, success_rates_total &lt;dbl&gt;,\n#&gt; #   success_rates_men &lt;dbl&gt;, success_rates_women &lt;dbl&gt;\n\nWe can see that the objects are identical:\n\nidentical(research_funding_rates, new_research_funding_rates)\n#&gt; [1] TRUE"
  },
  {
    "objectID": "wrangling/string-processing.html#sec-recode",
    "href": "wrangling/string-processing.html#sec-recode",
    "title": "\n16  String processing\n",
    "section": "\n16.9 Renaming levels",
    "text": "16.9 Renaming levels\nAnother common operation involving strings is renaming the levels of of a categorical variables. Let’s say you have really long names for your levels and you will be displaying them in plots, you might want to use shorter versions of these names. For example, in character vectors with country names, you might want to change “United States of America” to “USA” and “United Kingdom” to UK, and so on.\nRather than changing each entry, a more efficient approach is to change the levels.\nHere is an example that shows how to rename countries with long names:\n\nlibrary(dslabs)\n\nSuppose we want to show life expectancy time series by country for the Caribbean. If we make this plot\n\ngapminder |&gt; \n  filter(region == \"Caribbean\") |&gt;\n  ggplot(aes(year, life_expectancy, color = country)) +\n  geom_line()\n\nwe see that the legend takes up much of the plot because we have four countries with names longer than 12 characters. We can\n\nx &lt;- levels(gapminder$country)\nlevels(gapminder$country) &lt;- case_when(\n  x == \"Antigua and Barbuda\" ~ \"Barbuda\",\n  x == \"Dominican Republic\" ~ \"DR\",\n  x == \"St. Vincent and the Grenadines\" ~ \"St. Vincent\",\n  x == \"Trinidad and Tobago\" ~ \"Trinidad\",\n  .default = x)\ngapminder |&gt; \n  filter(region == \"Caribbean\") |&gt;\n  ggplot(aes(year, life_expectancy, color = country)) +\n  geom_line()\n\n\n\n\n\n\n\nWe can also use the fct_recode function in the forcats package:\n\ngapminder$country &lt;- \n  fct_recode(gapminder$country, \n             \"Barbuda\" = \"Antigua and Barbuda\",\n             \"DR\" = \"Dominican Republic\",\n             \"St. Vincent\" = \"St. Vincent and the Grenadines\",\n             \"Trinidad\" = \"Trinidad and Tobago\")\n#&gt; Warning: Unknown levels in `f`: Antigua and Barbuda, Dominican\n#&gt; Republic, St. Vincent and the Grenadines, Trinidad and Tobago"
  },
  {
    "objectID": "wrangling/string-processing.html#exercises",
    "href": "wrangling/string-processing.html#exercises",
    "title": "\n16  String processing\n",
    "section": "\n16.10 Exercises",
    "text": "16.10 Exercises\n1. Complete all lessons and exercises in the https://regexone.com/ online interactive tutorial.\n2. In the extdata directory of the dslabs package, you will find a PDF file containing daily mortality data for Puerto Rico from Jan 1, 2015 to May 31, 2018. You can find the file like this:\n\nfn &lt;- system.file(\"extdata\", \"RD-Mortality-Report_2015-18-180531.pdf\",\n                  package=\"dslabs\")\n\nFind and open the file or open it directly from RStudio. On a Mac, you can type:\n\nsystem2(\"open\", args = fn)\n\nand on Windows, you can type:\n\nsystem(\"cmd.exe\", input = paste(\"start\", fn))\n\nWhich of the following best describes this file:\n\nIt is a table. Extracting the data will be easy.\nIt is a report written in prose. Extracting the data will be impossible.\nIt is a report combining graphs and tables. Extracting the data seems possible.\nIt shows graphs of the data. Extracting the data will be difficult.\n\n3. We are going to create a tidy dataset with each row representing one observation. The variables in this dataset will be year, month, day, and deaths. Start by installing and loading the pdftools package:\n\ninstall.packages(\"pdftools\")\nlibrary(pdftools)\n\nNow read-in fn using the pdf_text function and store the results in an object called txt. Which of the following best describes what you see in txt?\n\nA table with the mortality data.\nA character string of length 12. Each entry represents the text in each page. The mortality data is in there somewhere.\nA character string with one entry containing all the information in the PDF file.\nAn html document.\n\n4. Extract the ninth page of the PDF file from the object txt, then use the str_split from the stringr package so that you have each line in a different entry. Call this string vector s. Then look at the result and choose the one that best describes what you see.\n\nIt is an empty string.\nI can see the figure shown in page 1.\nIt is a tidy table.\nI can see the table! But there is a bunch of other stuff we need to get rid of.\n\n5. What kind of object is s and how many entries does it have?\n6. We see that the output is a list with one component. Redefine s to be the first entry of the list. What kind of object is s and how many entries does it have?\n7. When inspecting the string we obtained above, we see a common problem: white space before and after the other characters. Trimming is a common first step in string processing. These extra spaces will eventually make splitting the strings hard so we start by removing them. We learned about the command str_trim that removes spaces at the start or end of the strings. Use this function to trim s.\n8. We want to extract the numbers from the strings stored in s. However, there are many non-numeric characters that will get in the way. We can remove these, but before doing this we want to preserve the string with the column header, which includes the month abbreviation. Use the str_which function to find the rows with a header. Save these results to header_index. Hint: find the first string that matches the pattern 2015 using the str_which function.\n9. Now we are going to define two objects: month will store the month and header will store the column names. Identify which row contains the header of the table. Save the content of the row into an object called header, then use str_split to help define the two objects we need. Hints: the separator here is one or more spaces. Also, consider using the simplify argument.\n10. Notice that towards the end of the page you see a totals row followed by rows with other summary statistics. Create an object called tail_index with the index of the totals entry.\n11. Because our PDF page includes graphs with numbers, some of our rows have just one number (from the y-axis of the plot). Use the str_count function to create an object n with the number of numbers in each each row. Hint: you can write a regex for number like this \\\\d+.\n12. We are now ready to remove entries from rows that we know we don’t need. The entry header_index and everything before it should be removed. Entries for which n is 1 should also be removed, and the entry tail_index and everything that comes after it should be removed as well.\n13. Now we are ready to remove all the non-numeric entries. Do this using regex and the str_remove_all function. Hint: remember that in regex, using the upper case version of a special character usually means the opposite. So \\\\D means “not a digit”. Remember you also want to keep spaces.\n14. To convert the strings into a table, use the str_split_fixed function. Convert s into a data matrix with just the day and death count data. Hints: note that the separator is one or more spaces. Make the argument n a value that limits the number of columns to the values in the 4 columns and the last column captures all the extra stuff. Then keep only the first four columns.\n15. Now you are almost ready to finish. Add column names to the matrix, including one called day. Also, add a column with the month. Call the resulting object dat. Finally, make sure the day is an integer not a character. Hint: use only the first five columns.\n16. Now finish it up by tidying tab with the `pivot_longer_ function.\n17. Make a plot of deaths versus day with color to denote year. Exclude 2018 since we do not have data for the entire year.\n18. Now that we have wrangled this data step-by-step, put it all together in one R chunk, using the pipe as much as possible. Hint: first define the indexes, then write one line of code that does all the string processing.\n19. Advanced: let’s return to the MLB Payroll example from the web scraping section. Use what you have learned in the web scraping and string processing chapters to extract the payroll for the New York Yankees, Boston Red Sox, and Oakland A’s and plot them as a function of time."
  },
  {
    "objectID": "wrangling/string-processing.html#footnotes",
    "href": "wrangling/string-processing.html#footnotes",
    "title": "\n16  String processing\n",
    "section": "",
    "text": "https://www.regular-expressions.info/tutorial.html↩︎\nhttp://r4ds.had.co.nz/strings.html#matching-patterns-with-regular-expressions↩︎\nhttps://www.rstudio.com/wp-content/uploads/2016/09/RegExCheatsheet.pdf↩︎\nhttps://www.rstudio.com/wp-content/uploads/2016/09/RegExCheatsheet.pdf↩︎\nhttp://www.pnas.org/content/112/40/12349.abstract↩︎\nhttp://www.pnas.org/content/112/40/12349↩︎"
  },
  {
    "objectID": "wrangling/text-analysis.html#case-study-trump-tweets",
    "href": "wrangling/text-analysis.html#case-study-trump-tweets",
    "title": "\n17  Text analysis\n",
    "section": "\n17.1 Case study: Trump tweets",
    "text": "17.1 Case study: Trump tweets\nDuring the 2016 US presidential election, then candidate Donald J. Trump used his twitter account as a way to communicate with potential voters. On August 6, 2016, Todd Vaziri tweeted1 about Trump that “Every non-hyperbolic tweet is from iPhone (his staff). Every hyperbolic tweet is from Android (from him).” David Robinson conducted an analysis2 to determine if data supported this assertion. Here, we go through David’s analysis to learn some of the basics of text analysis. To learn more about text analysis in R, we recommend the Text Mining with R book3 by Julia Silge and David Robinson.\nWe will use the following libraries:\n\nlibrary(tidyverse)\nlibrary(scales)\nlibrary(tidytext)\nlibrary(textdata)\nlibrary(dslabs)\n\nX.com (formerly twitter) provides an API that permits downloading tweets. Brendan Brown runs the trump archive^[https://www.thetrumparchive.com/], which compiles tweet data from Trump’s account. The dslabs package includes tweets from the following range:\n\nrange(trump_tweets$created_at)\n#&gt; [1] \"2009-05-04 13:54:25 EST\" \"2018-01-01 08:37:52 EST\"\n\nThe data frame includes the the following variables:\n\nnames(trump_tweets)\n#&gt; [1] \"source\"                  \"id_str\"                 \n#&gt; [3] \"text\"                    \"created_at\"             \n#&gt; [5] \"retweet_count\"           \"in_reply_to_user_id_str\"\n#&gt; [7] \"favorite_count\"          \"is_retweet\"\n\nThe help file ?trump_tweets provides details on what each variable represents. The actual tweets are in the text variable:\n\ntrump_tweets$text[16413] |&gt; str_wrap(width = options()$width) |&gt; cat()\n#&gt; Great to be back in Iowa! #TBT with @JerryJrFalwell joining me in\n#&gt; Davenport- this past winter. #MAGA https://t.co/A5IF0QHnic\n\nand the source variable tells us which device was used to compose and upload each tweet:\n\ntrump_tweets |&gt; count(source) |&gt; arrange(desc(n)) |&gt; head(5)\n#&gt;                source     n\n#&gt; 1  Twitter Web Client 10718\n#&gt; 2 Twitter for Android  4652\n#&gt; 3  Twitter for iPhone  3962\n#&gt; 4           TweetDeck   468\n#&gt; 5     TwitLonger Beta   288\n\nWe are interested in what happened during the 2016 campaign, so for this analysis we will focus on what was tweeted between the day Trump announced his campaign and election day. We define the following table containing just the tweets from that time period. We remove the Twitter for part of the source and filter out retweets.\n\ncampaign_tweets &lt;- trump_tweets |&gt; \n  filter(source %in% paste(\"Twitter for\", c(\"Android\", \"iPhone\")) &\n           created_at &gt;= ymd(\"2015-06-17\") & \n           created_at &lt; ymd(\"2016-11-08\")) |&gt;\n  mutate(source = str_remove(source, \"Twitter for \")) |&gt;\n  filter(!is_retweet) |&gt;\n  arrange(created_at) |&gt; \n  as_tibble()\n\nWe can now use data visualization to explore the possibility that two different groups were tweeting from these devices. For each tweet, we will extract the hour, East Coast time (EST), it was tweeted and then compute the proportion of tweets tweeted at each hour for each device:\n\ncampaign_tweets |&gt;\n  mutate(hour = hour(with_tz(created_at, \"EST\"))) |&gt;\n  count(source, hour) |&gt;\n  group_by(source) |&gt;\n  mutate(percent = n / sum(n)) |&gt;\n  ungroup() |&gt;\n  ggplot(aes(hour, percent, color = source)) +\n  geom_line() +\n  geom_point() +\n  scale_y_continuous(labels = percent_format()) +\n  labs(x = \"Hour of day (EST)\", y = \"% of tweets\", color = \"\")\n\n\n\n\n\n\n\nWe notice a big peak for the Android in the early hours of the morning, between 6 and 8 AM. There seems to be a clear difference in these patterns. We will therefore assume that two different entities are using these two devices.\nWe will now study how the tweets differ when we compare Android to iPhone. To do this, we introduce the tidytext package."
  },
  {
    "objectID": "wrangling/text-analysis.html#text-as-data",
    "href": "wrangling/text-analysis.html#text-as-data",
    "title": "\n17  Text analysis\n",
    "section": "\n17.2 Text as data",
    "text": "17.2 Text as data\nThe tidytext package helps us convert free form text into a tidy table. Having the data in this format greatly facilitates data visualization and the use of statistical techniques.\nThe main function needed to achieve this is unnest_tokens. A token refers to a unit that we are considering to be a data point. The most common token will be words, but they can also be single characters, n-grams, sentences, lines, or a pattern defined by a regex. The functions will take a vector of strings and extract the tokens so that each one gets a row in the new table. Here is a simple example:\n\npoem &lt;- c(\"Roses are red,\", \"Violets are blue,\", \n          \"Sugar is sweet,\", \"And so are you.\")\nexample &lt;- tibble(line = c(1, 2, 3, 4),\n                      text = poem)\nexample\n#&gt; # A tibble: 4 × 2\n#&gt;    line text             \n#&gt;   &lt;dbl&gt; &lt;chr&gt;            \n#&gt; 1     1 Roses are red,   \n#&gt; 2     2 Violets are blue,\n#&gt; 3     3 Sugar is sweet,  \n#&gt; 4     4 And so are you.\nexample |&gt; unnest_tokens(word, text)\n#&gt; # A tibble: 13 × 2\n#&gt;    line word   \n#&gt;   &lt;dbl&gt; &lt;chr&gt;  \n#&gt; 1     1 roses  \n#&gt; 2     1 are    \n#&gt; 3     1 red    \n#&gt; 4     2 violets\n#&gt; 5     2 are    \n#&gt; # ℹ 8 more rows\n\nNow let’s look at trump tweet. We will look at tweet number 3008 because it will later permit us to illustrate a couple of points:\n\ni &lt;- 3008\ncampaign_tweets$text[i] |&gt; str_wrap(width = 65) |&gt; cat()\n#&gt; Great to be back in Iowa! #TBT with @JerryJrFalwell joining me in\n#&gt; Davenport- this past winter. #MAGA https://t.co/A5IF0QHnic\ncampaign_tweets[i,] |&gt; \n  unnest_tokens(word, text) |&gt;\n  pull(word) \n#&gt;  [1] \"great\"          \"to\"             \"be\"             \"back\"          \n#&gt;  [5] \"in\"             \"iowa\"           \"tbt\"            \"with\"          \n#&gt;  [9] \"jerryjrfalwell\" \"joining\"        \"me\"             \"in\"            \n#&gt; [13] \"davenport\"      \"this\"           \"past\"           \"winter\"        \n#&gt; [17] \"maga\"           \"https\"          \"t.co\"           \"a5if0qhnic\"\n\nNote that the function tries to convert tokens into words. A minor adjustment is to remove the links to pictures:\n\nlinks_to_pics &lt;- \"https://t.co/[A-Za-z\\\\d]+|&amp;\"\ncampaign_tweets[i,] |&gt; \n  mutate(text = str_remove_all(text, links_to_pics))  |&gt;\n  unnest_tokens(word, text) |&gt;\n  pull(word)\n#&gt;  [1] \"great\"          \"to\"             \"be\"             \"back\"          \n#&gt;  [5] \"in\"             \"iowa\"           \"tbt\"            \"with\"          \n#&gt;  [9] \"jerryjrfalwell\" \"joining\"        \"me\"             \"in\"            \n#&gt; [13] \"davenport\"      \"this\"           \"past\"           \"winter\"        \n#&gt; [17] \"maga\"\n\nNow we are now ready to extract the words for all our tweets.\n\ntweet_words &lt;- campaign_tweets |&gt; \n  mutate(text = str_remove_all(text, links_to_pics))  |&gt;\n  unnest_tokens(word, text)\n\nAnd we can now answer questions such as “what are the most commonly used words?”:\n\ntweet_words |&gt; \n  count(word) |&gt;\n  arrange(desc(n))\n#&gt; # A tibble: 6,264 × 2\n#&gt;   word      n\n#&gt;   &lt;chr&gt; &lt;int&gt;\n#&gt; 1 the    2330\n#&gt; 2 to     1413\n#&gt; 3 and    1245\n#&gt; 4 in     1190\n#&gt; 5 i      1151\n#&gt; # ℹ 6,259 more rows\n\nIt is not surprising that these are the top words, which are not informative. The tidytext package has a database of these commonly used words, referred to as stop words, in text analysis:\n\nhead(stop_words)\n#&gt; # A tibble: 6 × 2\n#&gt;   word  lexicon\n#&gt;   &lt;chr&gt; &lt;chr&gt;  \n#&gt; 1 a     SMART  \n#&gt; 2 a's   SMART  \n#&gt; 3 able  SMART  \n#&gt; 4 about SMART  \n#&gt; 5 above SMART  \n#&gt; # ℹ 1 more row\n\nIf we filter out rows representing stop words with filter(!word %in% stop_words$word):\n\ntweet_words &lt;- campaign_tweets |&gt; \n  mutate(text = str_remove_all(text, links_to_pics))  |&gt;\n  unnest_tokens(word, text) |&gt;\n  filter(!word %in% stop_words$word ) \n\nwe end up with a much more informative set of top 10 tweeted words:\n\ntweet_words |&gt; \n  count(word) |&gt;\n  top_n(10, n) |&gt;\n  mutate(word = reorder(word, n)) |&gt;\n  arrange(desc(n))\n#&gt; # A tibble: 10 × 2\n#&gt;   word                      n\n#&gt;   &lt;fct&gt;                 &lt;int&gt;\n#&gt; 1 trump2016               415\n#&gt; 2 hillary                 407\n#&gt; 3 people                  304\n#&gt; 4 makeamericagreatagain   298\n#&gt; 5 america                 255\n#&gt; # ℹ 5 more rows\n\nSome exploration of the resulting words (not shown here) reveals a couple of unwanted characteristics in our tokens. First, some of our tokens are just numbers (years, for example). We want to remove these and we can find them using the regex ^\\d+$. Second, some of our tokens come from a quote and they start with '. We want to remove the ' when it is at the start of a word so we will just str_replace. We add these two lines to the code above to generate our final table:\n\ntweet_words &lt;- campaign_tweets |&gt; \n  mutate(text = str_remove_all(text, links_to_pics))  |&gt;\n  unnest_tokens(word, text) |&gt;\n  filter(!word %in% stop_words$word &\n           !str_detect(word, \"^\\\\d+$\")) |&gt;\n  mutate(word = str_replace(word, \"^'\", \"\"))\n\nNow that we have all our words in a table, along with information about what device was used to compose the tweet they came from, we can start exploring which words are more common when comparing Android to iPhone.\nFor each word, we want to know if it is more likely to come from an Android tweet or an iPhone tweet. We therefore compute, for each word, what proportion of all words it represent for Android and iPhone, respectively.\n\nandroid_vs_iphone &lt;- tweet_words |&gt;\n  count(word, source) |&gt;\n  pivot_wider(names_from = \"source\", values_from = \"n\", values_fill = 0) |&gt;\n  mutate(p_a = Android / sum(Android), p_i = iPhone / sum(iPhone),\n         percent_diff = (p_a - p_i) / ((p_a + p_i)/2) * 100)\n\nFor words appearing at least 100 times in total, here are the highest percent differences for Android\n\nandroid_vs_iphone |&gt; filter(Android + iPhone &gt;= 100) |&gt;\n  arrange(desc(percent_diff))\n#&gt; # A tibble: 30 × 6\n#&gt;   word        Android iPhone     p_a     p_i percent_diff\n#&gt;   &lt;chr&gt;         &lt;int&gt;  &lt;int&gt;   &lt;dbl&gt;   &lt;dbl&gt;        &lt;dbl&gt;\n#&gt; 1 bad             104     26 0.00645 0.00188        110. \n#&gt; 2 crooked         156     49 0.00968 0.00354         92.9\n#&gt; 3 cnn             116     37 0.00720 0.00267         91.7\n#&gt; 4 ted              86     28 0.00533 0.00202         90.1\n#&gt; 5 interviewed      76     25 0.00471 0.00180         89.3\n#&gt; # ℹ 25 more rows\n\nand the top for iPhone:\n\nandroid_vs_iphone |&gt; filter(Android + iPhone &gt;= 100) |&gt; \n  arrange(percent_diff)\n#&gt; # A tibble: 30 × 6\n#&gt;   word                  Android iPhone       p_a     p_i percent_diff\n#&gt;   &lt;chr&gt;                   &lt;int&gt;  &lt;int&gt;     &lt;dbl&gt;   &lt;dbl&gt;        &lt;dbl&gt;\n#&gt; 1 makeamericagreatagain       0    298 0         0.0215        -200  \n#&gt; 2 join                        1    157 0.0000620 0.0113        -198. \n#&gt; 3 trump2016                   3    412 0.000186  0.0297        -198. \n#&gt; 4 tomorrow                   24    101 0.00149   0.00729       -132. \n#&gt; 5 vote                       46     67 0.00285   0.00484        -51.6\n#&gt; # ℹ 25 more rows\n\nWe already see somewhat of a pattern in the types of words that are being tweeted more from one device versus the other. However, we are not interested in specific words but rather in the tone. Vaziri’s assertion is that the Android tweets are more hyperbolic. So how can we check this with data? Hyperbolic is a hard sentiment to extract from words as it relies on interpreting phrases. However, words can be associated to more basic sentiment such as anger, fear, joy, and surprise. In the next section, we demonstrate basic sentiment analysis."
  },
  {
    "objectID": "wrangling/text-analysis.html#sentiment-analysis",
    "href": "wrangling/text-analysis.html#sentiment-analysis",
    "title": "\n17  Text analysis\n",
    "section": "\n17.3 Sentiment analysis",
    "text": "17.3 Sentiment analysis\nIn sentiment analysis, we assign a word to one or more “sentiments”. Although this approach will miss context-dependent sentiments, such as sarcasm, when performed on large numbers of words, summaries can provide insights.\nThe first step in sentiment analysis is to assign a sentiment to each word. As we demonstrate, the tidytext package includes several maps or lexicons. We textdata package includes several of these lexicons.\nThe bing lexicon divides words into positive and negative sentiments. We can see this using the tidytext function get_sentiments:\n\nget_sentiments(\"bing\")\n\nThe AFINN lexicon assigns a score between -5 and 5, with -5 the most negative and 5 the most positive. Note that this lexicon needs to be downloaded the first time you call the function get_sentiment:\n\nget_sentiments(\"afinn\")\n\nThe loughran and nrc lexicons provide several different sentiments. Note that these also have to be downloaded the first time you use them.\n\nget_sentiments(\"loughran\") |&gt; count(sentiment)\n#&gt; # A tibble: 6 × 2\n#&gt;   sentiment        n\n#&gt;   &lt;chr&gt;        &lt;int&gt;\n#&gt; 1 constraining   184\n#&gt; 2 litigious      904\n#&gt; 3 negative      2355\n#&gt; 4 positive       354\n#&gt; 5 superfluous     56\n#&gt; # ℹ 1 more row\n\n\nget_sentiments(\"nrc\") |&gt; count(sentiment)\n#&gt; # A tibble: 10 × 2\n#&gt;   sentiment        n\n#&gt;   &lt;chr&gt;        &lt;int&gt;\n#&gt; 1 anger         1245\n#&gt; 2 anticipation   837\n#&gt; 3 disgust       1056\n#&gt; 4 fear          1474\n#&gt; 5 joy            687\n#&gt; # ℹ 5 more rows\n\nFor our analysis, we are interested in exploring the different sentiments of each tweet so we will use the nrc lexicon:\n\nnrc &lt;- get_sentiments(\"nrc\") |&gt;\n  select(word, sentiment)\n\nWe can combine the words and sentiments using inner_join, which will only keep words associated with a sentiment. Here are 10 random words extracted from the tweets:\n\ntweet_words |&gt; inner_join(nrc, by = \"word\", relationship = \"many-to-many\") |&gt; \n  select(source, word, sentiment) |&gt; \n  sample_n(5)\n#&gt; # A tibble: 5 × 3\n#&gt;   source  word     sentiment   \n#&gt;   &lt;chr&gt;   &lt;chr&gt;    &lt;chr&gt;       \n#&gt; 1 Android enjoy    joy         \n#&gt; 2 iPhone  terrific sadness     \n#&gt; 3 iPhone  tactics  trust       \n#&gt; 4 Android clue     anticipation\n#&gt; 5 iPhone  change   fear\n\nNow we are ready to perform a quantitative analysis comparing Android and iPhone by comparing the sentiments of the tweets posted from each device. Here we could perform a tweet-by-tweet analysis, assigning a sentiment to each tweet. However, this will be challenging since each tweet will have several sentiments attached to it, one for each word appearing in the lexicon. For illustrative purposes, we will perform a much simpler analysis: we will count and compare the frequencies of each sentiment appearing in each device.\n\nsentiment_counts &lt;- tweet_words |&gt;\n  left_join(nrc, by = \"word\", relationship = \"many-to-many\") |&gt;\n  count(source, sentiment) |&gt;\n  pivot_wider(names_from = \"source\", values_from = \"n\") |&gt;\n  mutate(sentiment = replace_na(sentiment, replace = \"none\"))\nsentiment_counts\n#&gt; # A tibble: 11 × 3\n#&gt;   sentiment    Android iPhone\n#&gt;   &lt;chr&gt;          &lt;int&gt;  &lt;int&gt;\n#&gt; 1 anger            962    527\n#&gt; 2 anticipation     917    707\n#&gt; 3 disgust          639    314\n#&gt; 4 fear             799    486\n#&gt; 5 joy              695    536\n#&gt; # ℹ 6 more rows\n\nFor each sentiment, we can compute the percent difference in proportion for Android compared to iPhone:\n\nsentiment_counts |&gt;\n  mutate(p_a = Android / sum(Android) , \n         p_i = iPhone / sum(iPhone), \n         percent_diff = (p_a - p_i) / ((p_a + p_i)/2) * 100) |&gt;\n  arrange(desc(percent_diff))\n#&gt; # A tibble: 11 × 6\n#&gt;   sentiment Android iPhone    p_a    p_i percent_diff\n#&gt;   &lt;chr&gt;       &lt;int&gt;  &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt;        &lt;dbl&gt;\n#&gt; 1 disgust       639    314 0.0290 0.0178         48.1\n#&gt; 2 anger         962    527 0.0437 0.0298         37.8\n#&gt; 3 negative     1657    931 0.0753 0.0527         35.3\n#&gt; 4 sadness       901    514 0.0409 0.0291         33.8\n#&gt; 5 fear          799    486 0.0363 0.0275         27.6\n#&gt; # ℹ 6 more rows\n\nSo we do see some differences and the order is interesting: the largest three sentiments are disgust, anger, and negative!\nIf we are interested in exploring which specific words are driving these differences, we can refer back to our android_iphone_or object:\n\nandroid_vs_iphone |&gt; inner_join(nrc, by = \"word\") |&gt;\n  filter(sentiment == \"disgust\") |&gt;\n  arrange(desc(percent_diff))\n#&gt; # A tibble: 157 × 7\n#&gt;   word      Android iPhone       p_a   p_i percent_diff sentiment\n#&gt;   &lt;chr&gt;       &lt;int&gt;  &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;        &lt;dbl&gt; &lt;chr&gt;    \n#&gt; 1 abuse           1      0 0.0000620     0          200 disgust  \n#&gt; 2 angry          10      0 0.000620      0          200 disgust  \n#&gt; 3 arrogant        2      0 0.000124      0          200 disgust  \n#&gt; 4 attacking       5      0 0.000310      0          200 disgust  \n#&gt; 5 belittle        2      0 0.000124      0          200 disgust  \n#&gt; # ℹ 152 more rows\n\nand we can make a graph:\n\n\n\n\n\n\n\n\nThis is just a simple example of the many analyses one can perform with tidytext. To learn more, we again recommend the Tidy Text Mining book4."
  },
  {
    "objectID": "wrangling/text-analysis.html#exercises",
    "href": "wrangling/text-analysis.html#exercises",
    "title": "\n17  Text analysis\n",
    "section": "\n17.4 Exercises",
    "text": "17.4 Exercises\nProject Gutenberg is a digital archive of public domain books. The R package gutenbergr facilitates the importation of these texts into R.\nYou can install and load by typing:\n\ninstall.packages(\"gutenbergr\")\nlibrary(gutenbergr)\n\nYou can see the books that are available like this:\n\ngutenberg_metadata\n\n1. Use str_detect to find the ID of the novel Pride and Prejudice.\n2. We notice that there are several versions. The gutenberg_works() function filters this table to remove replicates and include only English language works. Read the help file and use this function to find the ID for Pride and Prejudice.\n3. Use the gutenberg_download function to download the text for Pride and Prejudice. Save it to an object called book.\n4. Use the tidytext package to create a tidy table with all the words in the text. Save the table in an object called words\n5. We will later make a plot of sentiment versus location in the book. For this, it will be useful to add a column with the word number to the table.\n6. Remove the stop words and numbers from the words object. Hint: use the anti_join.\n7. Now use the AFINN lexicon to assign a sentiment value to each word.\n8. Make a plot of sentiment score versus location in the book and add a smoother.\n9. Assume there are 300 words per page. Convert the locations to pages and then compute the average sentiment in each page. Plot that average score by page. Add a smoother that appears to go through data."
  },
  {
    "objectID": "wrangling/text-analysis.html#footnotes",
    "href": "wrangling/text-analysis.html#footnotes",
    "title": "\n17  Text analysis\n",
    "section": "",
    "text": "https://twitter.com/tvaziri/status/762005541388378112/photo/1↩︎\nhttp://varianceexplained.org/r/trump-tweets/↩︎\nhttps://www.tidytextmining.com/↩︎\nhttps://www.tidytextmining.com/↩︎"
  },
  {
    "objectID": "productivity/intro-productivity.html#footnotes",
    "href": "productivity/intro-productivity.html#footnotes",
    "title": "Productivity Tools",
    "section": "",
    "text": "http://github.com↩︎\nhttps://www.rstudio.com/↩︎"
  },
  {
    "objectID": "productivity/installing-r-and-rstudio.html#installing-r",
    "href": "productivity/installing-r-and-rstudio.html#installing-r",
    "title": "18  Installing R and RStudio",
    "section": "18.1 Installing R",
    "text": "18.1 Installing R\nRStudio is an interactive desktop environment, but it is not R, nor does it include R when you download and install it. Therefore, to use RStudio, we first need to install R.\n\nYou can download R from the Comprehensive R Archive Network (CRAN)1. Search for CRAN on your browser:\n\n\n\n\n\n\n\nOnce on the CRAN page, select the version for your operating system: Linux, Mac OS X, or Windows.\n\n\n\n\n\n\nHere we show screenshots for Windows, but the process is similar for the other platforms. When they differ, we will also show screenshots for Mac OS X.\n\nOnce at the CRAN download page, you will have several choices. You want to install the base subdirectory. This installs the basic packages you need to get started. We will later learn how to install other needed packages from within R, rather than from this webpage.\n\n\n\n\n\n\n\nClick on the link for the latest version to start the download.\n\n\n\n\n\n\n\nIf you are using Chrome, at the bottom of your browser you should see a tab that shows you the progress of the download. Once the installer file downloads, you can click on that tab to start the installation process. Other browsers may be different, so you will have to find where they store downloaded files and click on them to get the process started.\n\n\n\n\n\n\nIf using Safari on a Mac, you can access the download through the download button.\n\n\n\n\n\n\nYou can now click through different choices to finish the installation. We recommend you select all the default choices.\n\n\n\n\n\n\nSelect the default even when you get an ominous warning.\n\n\n\n\n\nWhen selecting the language, consider that it will be easier to follow this book if you select English.\n\n\n\n\n\nContinue to select all the defaults:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOn the Mac it looks different, but you are also accepting the defaults:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCongratulations! You have installed R."
  },
  {
    "objectID": "productivity/installing-r-and-rstudio.html#installing-rstudio",
    "href": "productivity/installing-r-and-rstudio.html#installing-rstudio",
    "title": "18  Installing R and RStudio",
    "section": "18.2 Installing RStudio",
    "text": "18.2 Installing RStudio\n\nYou can start by searching for RStudio on your browser:\n\n\n\n\n\n\n\nYou should find the RStudio website as shown above. Once there, click on Download RStudio.\n\n\n\n\n\n\n\nThis will give you several options. For what we do in this book, it is more than enough to use the free Desktop version:\n\n\n\n\n\n\n\nOnce you select this option, it will take you to a page in which the operating system options are provided. Click the link showing your operating system.\n\n\n\n\n\n\n\nOnce the installation file is downloaded, click on the downloaded file to start the installation process:\n\n\n\n\n\n\n\nWe recommend clicking yes on all the defaults.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOn the Mac, there are fewer clicks. You basically drag and drop the RStudio icon into the Applications folder icon here:\n\n\n\n\n\nCongratulations! You have installed RStudio. You can now get started as you do on any other program in your computer. On Windows, you can open RStudio from the Start menu. If RStudio does not appear, you can search for it:\n\n\n\n\n\nOn the Mac, it will be in the Applications folder:\n\n\n\n\n\n\n\n\n\n\nPro tip for the Mac: To avoid using the mouse to open RStudio, hit command+spacebar to open Spotlight Search and type RStudio into that search bar, then hit enter."
  },
  {
    "objectID": "productivity/installing-r-and-rstudio.html#footnotes",
    "href": "productivity/installing-r-and-rstudio.html#footnotes",
    "title": "18  Installing R and RStudio",
    "section": "",
    "text": "https://cran.r-project.org/↩︎"
  },
  {
    "objectID": "productivity/installing-git.html#sec-terminal-on-mac",
    "href": "productivity/installing-git.html#sec-terminal-on-mac",
    "title": "19  Installing Git",
    "section": "19.1 Accessing the terminal on a Mac",
    "text": "19.1 Accessing the terminal on a Mac\nIn Chapter Chapter 20 we will show how the terminal is our window into the Unix world. On a Mac you can access a terminal by opening the application in the Utilities folder:\n\n\n\n\n\n\n\n\n\n\nYou can also use the Spotlight feature on the Mac by typing command-spacebar, then type Terminal.\nYet another way to access the terminal is from RStudio. In the Console pane you should see a Terminal tab. If you click on this tab you will open a terminal window."
  },
  {
    "objectID": "productivity/installing-git.html#installing-git-on-the-mac",
    "href": "productivity/installing-git.html#installing-git-on-the-mac",
    "title": "19  Installing Git",
    "section": "19.2 Installing Git on the Mac",
    "text": "19.2 Installing Git on the Mac\n\nThe instructions in this subsection are not for Windows users.\n\n\nStart by opening a terminal as described in the previous section.\nOnce you start the terminal, you will see a console like this:\n\n\n\nYou might have Git installed already. One way to check is by asking for the version by typing:\n\ngit --version\nIf you get a version number back, it is already installed. If not, you will get the following message:\n\nand you will be asked if you want to install it. You should click Install:\n\n\nThis will take you through the installation process:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOnce installed, you can check for the version again and it should show you something like this:\n\n\nCongratulations. You have installed Git on your Mac."
  },
  {
    "objectID": "productivity/installing-git.html#installing-git-and-git-bash-on-windows",
    "href": "productivity/installing-git.html#installing-git-and-git-bash-on-windows",
    "title": "19  Installing Git",
    "section": "19.3 Installing Git and Git Bash on Windows",
    "text": "19.3 Installing Git and Git Bash on Windows\n\n\n\n\n\n\nThe instructions in this section are not for Mac users.\n\n\n\nThere are several pieces of software that will permit you to perform Unix commands on Windows. We will be using Git Bash as it interfaces with RStudio and it is automatically installed when we install Git for Windows.\n\nStart by searching for Git for Windows on your browser and clicking on the link from git-scm.com.\n\n\n\nThis will take you to the Download Git page from which you can download the most recent maintained build:\n\n\n\nYou can then accept to run the installer and agree to the license:\n\n\n\n\n\n\n\n\n\n\n\n\nIn one of the installation steps, you will be asked to pick the default editor for Git. Unless you are already a vi or vim user, we recommend against selecting vim which might be the default. If you do not recognize an editor you are familiar with among the options given, we recommend that you select nano as your default editor for Git since it is the easiest to learn:\n\n\n\nThe next installation decision is actually an important one. This installation process installs Git Bash. We recommend that you select Git and optional Unix tools from the Windows Command Prompt as this will permit you to learn Unix from within RStudio. However, if you do this, some commands that run on your Windows command line will stop working. If you do not use your Windows command line, then this should not be a problem. Also, most, if not all, of these Windows command lines have a Unix equivalent that you will be able to use now.\n\n\n\nYou can now continue selecting the default options.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou have now installed Git on Windows."
  },
  {
    "objectID": "productivity/installing-git.html#sec-terminal-on-windows",
    "href": "productivity/installing-git.html#sec-terminal-on-windows",
    "title": "19  Installing Git",
    "section": "19.4 Accessing the terminal on Windows",
    "text": "19.4 Accessing the terminal on Windows\nNow that Git Bash is installed, we can access the terminal either through RStudio or by opening Git Bash directly.\nTo access the terminal through RStudio, we need to change a preference so that Git Bash becomes the default Unix shell in RStudio. In RStudio, go to preferences (under the File pull down menu), then select Terminal, then select Git Bash:\n\nTo check that you in fact are using Git Bash in RStudio, you can open a New Terminal in RStudio:\n\nIt should look something like this:\n\nOften we want access to the terminal, but do not need RStudio. You can do this by running the Git Bash program directly:"
  },
  {
    "objectID": "productivity/unix.html#naming-convention",
    "href": "productivity/unix.html#naming-convention",
    "title": "20  Organizing with Unix",
    "section": "\n20.1 Naming convention",
    "text": "20.1 Naming convention\nBefore you start organizing projects with Unix you want to pick a name convention that you will use to systematically name your files and directories. This will help you find files and know what is in them.\nIn general you want to name your files in a way that is related to their contents and specifies how they relate to other files. The Smithsonian Data Management Best Practices4 has “five precepts of file naming and organization” and they are:\n\n\n\nHave a distinctive, human-readable name that gives an indication of the content.\nFollow a consistent pattern that is machine-friendly.\nOrganize files into directories (when necessary) that follow a consistent pattern.\nAvoid repetition of semantic elements among file and directory names.\nHave a file extension that matches the file format (no changing extensions!)\n\n\n\nFor specific recommendations we highly recommend you follow The Tidyverse Style Guide5."
  },
  {
    "objectID": "productivity/unix.html#sec-the-terminal",
    "href": "productivity/unix.html#sec-the-terminal",
    "title": "20  Organizing with Unix",
    "section": "\n20.2 The terminal",
    "text": "20.2 The terminal\nInstead of clicking, dragging, and dropping to organize our files and folders, we will be typing Unix commands into the terminal. The way we do this is similar to how we type commands into the R console, but instead of generating plots and statistical summaries, we will be organizing files on our system.\nYou will need access to a terminal6. Once you have a terminal open, you can start typing commands. You should see a blinking cursor at the spot where what you type will show up. This position is called the command line. Once you type something and hit enter on Windows or return on the Mac, Unix will try to execute this command. If you want to try out an example, type this command:\necho \"hello world\"\nThe command echo is similar to cat in R. Executing this line should print out hello world, then return back to the command line.\nNotice that you can’t use the mouse to move around in the terminal. You have to use the keyboard. To go back to a command you previously typed, you can use the up arrow.\nNote that above we included a chunk of code showing Unix commands in the same way we have previously shown R commands. We will make sure to distinguish when the command is meant for R and when it is meant for Unix."
  },
  {
    "objectID": "productivity/unix.html#sec-filesystem",
    "href": "productivity/unix.html#sec-filesystem",
    "title": "20  Organizing with Unix",
    "section": "\n20.3 The filesystem",
    "text": "20.3 The filesystem\nWe refer to all the files, folders, and programs on your computer as the filesystem. Keep in mind that folders and programs are also files, but this is a technicality we rarely think about and ignore in this book. We will focus on files and folders for now and discuss programs, or executables, in a later section.\n\n20.3.1 Directories and subdirectories\nThe first concept you need to grasp to become a Unix user is how your filesystem is organized. You should think of it as a series of nested folders, each containing files, folders, and executables.\nHere is a visual representation of the structure we are describing:\n\nIn Unix, we refer to folders as directories. Directories that are inside other directories are often referred to as subdirectories. So, for example, in the figure above, the directory docs has two subdirectories: reports and resumes, and docs is a subdirectory of home.\n\n20.3.2 The home directory\nThe home directory is where all your stuff is kept, as opposed to the system files that come with your computer, which are kept elsewhere. In the figure above, the directory called home represents your home directory, but that is rarely the name used. On your system, the name of your home directory is likely the same as your username on that system. Below are an example on Windows and Mac showing a home directory, in this case, named rafa:\n\n\n\n\n\n\n\n\n\n\nNow, look back at the figure showing a filesystem. Suppose you are using a point-and-click system and you want to remove the file cv.tex. Imagine that on your screen you can see the home directory. To erase this file, you would double click on the home directory, then docs, then resumes, and then drag cv.tex to the trash. Here you are experiencing the hierarchical nature of the system: cv.tex is a file inside the resumes directory, which is a subdirectory inside the docs directory, which is a subdirectory of the home directory.\nNow suppose you can’t see your home directory on your screen. You would somehow need to make it appear on your screen. One way to do this is to navigate from what is called the root directory all the way to your home directory. Any filesystem will have what is called a root directory, which is the directory that contains all directories. The home directory shown in the figure above will usually be two or more levels from the root. On Windows, you will have a structure like this:\n\nwhile on the Mac, it will be like this:\n\n\n\n\n\n\n\nOn Windows, the typical R installation will make your Documents directory your home directory in R. This will likely be different from your home directory in Git Bash. Generally, when we discuss home directories, we refer to the Unix home directory which for Windows, in this book, is the Git Bash Unix directory.\n\n\n\n\n20.3.3 Working directory\nThe concept of a current location is part of the point-and-click experience: at any given moment we are in a folder and see the content of that folder. As you search for a file, as we did above, you are experiencing the concept of a current location: once you double click on a directory, you change locations and are now in that folder, as opposed to the folder you were in before.\nIn Unix, we don’t have the same visual cues, but the concept of a current location is indispensable. We refer to this as the working directory. Each terminal window you have open has a working directory associated with it.\nHow do we know what is our working directory? To answer this, we learn our first Unix command: pwd, which stands for print working directory. This command returns the working directory.\nOpen a terminal and type:\npwd\nWe do not show the result of running this command because it will be quite different on your system compared to others. If you open a terminal and type pwd as your first command, you should see something like /Users/yourusername on a Mac or something like /c/Users/yourusername on Windows. The character string returned by calling pwd represents your working directory. When we first open a terminal, it will start in our home directory so in this case the working directory is the home directory.\nNotice that the forward slashes / in the strings above separate directories. So, for example, the location /c/Users/rafa implies that our working directory is called rafa and it is a subdirectory of Users, which is a subdirectory of c, which is a subdirectory of the root directory. The root directory is therefore represented by just a forward slash: /.\n\n20.3.4 Paths\nWe refer to the string returned by pwd as the full path of the working directory. The name comes from the fact that this string spells out the path you need to follow to get to the directory in question from the root directory. Every directory has a full path. Later, we will learn about relative paths, which tell us how to get to a directory from the working directory.\nIn Unix, we use the shorthand ~ as a nickname for your home directory. So, for example, if docs is a directory in your home directory, the full path for docs can be written like this ~/docs.\nMost terminals will show the path to your working directory right on the command line. If you are using default settings and open a terminal on the Mac, you will see that right at the command line you have something like computername:~ username with ~ representing your working directory, which in this example is the home directory ~. The same is true for the Git Bash terminal where you will see something like username@computername MINGW64 ~, with the working directory at the end. When we change directories, we will see this change on both Macs and Windows."
  },
  {
    "objectID": "productivity/unix.html#unix-commands",
    "href": "productivity/unix.html#unix-commands",
    "title": "20  Organizing with Unix",
    "section": "\n20.4 Unix commands",
    "text": "20.4 Unix commands\nWe will now learn a series of Unix commands that will permit us to prepare a directory for a data science project. We also provide examples of commands that, if you type into your terminal, will return an error. This is because we are assuming the filesystem in the earlier diagram. Your filesystem is different. In the next section, we will provide examples that you can type in.\n\n20.4.1 ls: Listing directory content\nIn a point-and-click system, we know what is in a directory because we see it. In the terminal, we do not see the icons. Instead, we use the command ls to list the directory content.\nTo see the content of your home directory, open a terminal and type:\nls\nWe will see more examples soon.\n\n20.4.2 mkdir and rmdir: make and remove a directory\nWhen we are preparing for a data science project, we will need to create directories. In Unix, we can do this with the command mkdir, which stands for make directory.\nBecause you will soon be working on several projects, we highly recommend creating a directory called projects in your home directory.\nYou can try this particular example on your system. Open a terminal and type:\nmkdir projects\nIf you do this correctly, nothing will happen: no news is good news. If the directory already exists, you will get an error message and the existing directory will remain untouched.\nTo confirm that you created these directories, you can list the directories:\nls\nYou should see the directories we just created listed. Perhaps you can also see many other directories that come pre-installed on your computer.\nFor illustrative purposes, let’s make a few more directories. You can list more than one directory name like this:\nmkdir docs teaching\nYou can check to see if the three directories were created:\nls\nIf you made a mistake and need to remove the directory, you can use the command rmdir to remove it.\nmkdir junk\nrmdir junk\nThis will remove the directory as long as it is empty. If it is not empty, you will get an error message and the directory will remain untouched. To remove directories that are not empty, we will learn about the command rm later.\n\n20.4.3 cd: navigating the filesystem by changing directories\nNext we want to create directories inside directories that we have already created. We also want to avoid pointing and clicking our way through the filesystem. We explain how to do this in Unix, using the command line.\nSuppose we open a terminal and our working directory is our home directory. We want to change our working directory to projects. We do this using the cd command, which stands for change directory:\ncd projects\nTo check that the working directory changed, we can use a command we previously learned to see our location:\npwd\nOur working directory should now be ~/projects. Note that on your computer the home directory ~ will be spelled out to something like /c/Users/yourusername.\n\n\n\n\n\n\nIn Unix you can auto-complete by hitting tab. This means that we can type cd d then hit tab. Unix will either auto-complete if docs is the only directory/file starting with d or show you the options. Try it out! Using Unix without auto-complete can make it unbearable.\n\n\n\nWhen using cd, we can either type a full path, which will start with / or ~, or a relative path. In the example above, in which we typed cd projects, we used a relative path. If the path you type does not start with / or ~, Unix will assume you are typing a relative path, meaning that it will look for the directory in your current working directory. So something like this will give you an error:\ncd Users\nbecause there is no Users directory in your working directory.\nNow suppose we want to move back to the directory in which projects is a subdirectory, referred to as the parent directory. We could use the full path of the parent directory, but Unix provides a shortcut for this: the parent directory of the working directory is represented with two dots: .. so to move back we simply type:\ncd ..\nYou should now be back in your home directory which you can confirm using pwd.\nBecause we can use full paths with cd, the following command:\ncd ~\nwill always take us back to the home directory, no matter where we are in the filesystem.\nThe working directory also has a nickname, which is a single . so if you type\ncd .\nyou will not move. Although this particular use of . is not useful, this nickname does come in handy sometimes. The reasons are not relevant for this section, but you should still be aware of this fact.\nIn summary, we have learned that when using cd we either stay put, move to a new directory using the desired directory name, or move back to the parent directory using ...\nWhen typing directory names, we can concatenate directories with the forward-slashes. So if we want a command that takes us to the projects directory no matter where we are in the filesystem, we can type:\ncd ~/projects\nwhich is equivalent to writing the entire path out. For example, in Windows we would write something like\ncd /c/Users/yourusername/projects\nThe last two commands are equivalent and in both cases we are typing the full path.\nWhen typing out the path of the directory we want, either full or relative, we can concatenate directories with the forward-slashes. We already saw that we can move to the projects directory regardless of where we are by typing the full path like this:\ncd ~/projects\nWe can also concatenate directory names for relative paths. For instance, if we want to move back to the parent directory of the parent directory of the working directory, we can type:\ncd ../..\nHere are a couple of final tips related to the cd command. First, you can go back to whatever directory you just left by typing:\ncd -\nThis can be useful if you type a very long path and then realize you want to go back to where you were, and that too has a very long path.\nSecond, if you just type:\ncd\nyou will be returned to your home directory."
  },
  {
    "objectID": "productivity/unix.html#examples",
    "href": "productivity/unix.html#examples",
    "title": "20  Organizing with Unix",
    "section": "\n20.5 Examples",
    "text": "20.5 Examples\nLet’s explore some examples of using cd. To help visualize, we will show the graphical representation of our filesystem vertically:\n\nSuppose our working directory is ~/projects and we want to move to figs in project-1.\nHere it is convenient to use relative paths:\ncd project-1/figs\nNow suppose our working directory is ~/projects and we want to move to reports in docs, how can we do this?\nOne way is to use relative paths:\ncd ../docs/reports\nAnother is to use the full path:\ncd ~/docs/reports\nIf you are trying this out on your system, remember to use auto-complete.\nLet’s examine one more example. Suppose we are in ~/projects/project-1/figs and want to change to ~/projects/project-2. Again, there are two ways.\nWith relative paths:\ncd ../../proejct-2\nand with full paths:\ncd ~/projects/project-2"
  },
  {
    "objectID": "productivity/unix.html#more-unix-commands",
    "href": "productivity/unix.html#more-unix-commands",
    "title": "20  Organizing with Unix",
    "section": "\n20.6 More Unix commands",
    "text": "20.6 More Unix commands\n\n20.6.1 mv: moving files\nIn a point-and-click system, we move files from one directory to another by dragging and dropping. In Unix, we use the mv command.\n\n\n\n\n\n\nmv will not ask “are you sure?” if your move results in overwriting a file.\n\n\n\nNow that you know how to use full and relative paths, using mv is relatively straightforward. The general form is:\nmv path-to-file path-to-destination-directory\nFor example, if we want to move the file cv.tex from resumes to reports, you could use the full paths like this:\nmv ~/docs/resumes/cv.tex ~/docs/reports/\nYou can also use relative paths. So you could do this:\ncd ~/docs/resumes\nmv cv.tex ../reports/\nor this:\ncd ~/docs/reports/\nmv ../resumes/cv.tex ./\nNotice that in the last one we used the working directory shortcut . to give a relative path as the destination directory.\nWe can also use mv to change the name of a file. To do this, instead of the second argument being the destination directory, it also includes a filename. So, for example, to change the name from cv.tex to resume.tex, we simply type:\ncd ~/docs/resumes\nmv cv.tex resume.tex\nWe can also combine the move and a rename. For example:\ncd ~/docs/resumes\nmv cv.tex ../reports/resume.tex\nAnd we can move entire directories. To move the resumes directory into reports, we do as follows:\nmv ~/docs/resumes ~/docs/reports/\nIt is important to add the last / to make it clear you do not want to rename the resumes directory to reports, but rather move it into the reports directory.\n\n20.6.2 cp: copying files\nThe command cp behaves similar to mv except instead of moving, we copy the file, meaning that the original file stays untouched.\nSo in all the mv examples above, you can switch mv to cp and they will copy instead of move with one exception: we can’t copy entire directories without learning about arguments, which we do later.\n\n20.6.3 rm: removing files\nIn point-and-click systems, we remove files by dragging and dropping them into the trash or using a special click on the mouse. In Unix, we use the rm command.\n\n\n\n\n\n\nUnlike throwing files into the trash, rm is permanent. Be careful!\n\n\n\nThe general way it works is as follows:\nrm filename\nYou can actually list files as well like this:\nrm filename-1 filename-2 filename-3\nYou can use full or relative paths. To remove directories, you will have to learn about arguments, which we do later.\n\n20.6.4 less: looking at a file\nOften you want to quickly look at the content of a file. If this file is a text file, the quickest way to do is by using the command less. To look a the file cv.tex, you do this:\ncd ~/docs/resumes\nless cv.tex \nTo exit the viewer, you type q. If the files are long, you can use the arrow keys to move up and down. There are many other keyboard commands you can use within less to, for example, search or jump pages.\nIf you are wondering why the command is called less, it is because the original was called more, as in “show me more of this file”. The second version was called less because of the saying “less is more”."
  },
  {
    "objectID": "productivity/unix.html#sec-prep-project",
    "href": "productivity/unix.html#sec-prep-project",
    "title": "20  Organizing with Unix",
    "section": "\n20.7 Preparing for a data analysis project",
    "text": "20.7 Preparing for a data analysis project\nWe are now ready to prepare a directory for a project. We will use the US murders project7 as an example.\nYou should start by creating a directory where you will keep all your projects. We recommend a directory called projects in your home directory. To do this you would type:\ncd ~\nmkdir projects\nOur project relates to gun violence murders so we will call the directory for our project murders. It will be a subdirectory in our projects directories. In the murders directory, we will create two subdirectories to hold the raw data and intermediate data. We will call these data and rda, respectively.\nOpen a terminal and make sure you are in the home directory:\ncd ~\nNow run the following commands to create the directory structure we want. At the end, we use ls and pwd to confirm we have generated the correct directories in the correct working directory:\ncd projects\nmkdir murders\ncd murders\nmkdir data rdas \nls\npwd\nNote that the full path of our murders dataset is ~/projects/murders.\nSo if we open a new terminal and want to navigate into that directory we type:\ncd projects/murders\nIn Section Section 22.3 we will describe how we can use RStudio to organize a data analysis project, once these directories have been created."
  },
  {
    "objectID": "productivity/unix.html#advanced-unix",
    "href": "productivity/unix.html#advanced-unix",
    "title": "20  Organizing with Unix",
    "section": "\n20.8 Advanced Unix",
    "text": "20.8 Advanced Unix\nMost Unix implementations include a large number of powerful tools and utilities. We have just learned the very basics here. We recommend that you use Unix as your main file management tool. It will take time to become comfortable with it, but as you struggle, you will find yourself learning just by looking up solutions on the internet. In this section, we superficially cover slightly more advanced topics. The main purpose of the section is to make you aware of what is available rather than explain everything in detail.\n\n20.8.1 Arguments\nMost Unix commands can be run with arguments. Arguments are typically defined by using a dash - or two dashes -- (depending on the command) followed by a letter or a word. An example of an argument is the -r behind rm. The r stands for recursive and the result is that files and directories are removed recursively, which means that if you type:\nrm -r directory-name\nall files, subdirectories, files in subdirectories, subdirectories in subdirectories, and so on, will be removed. This is equivalent to throwing a folder in the trash, except you can’t recover it. Once you remove it, it is deleted for good. Often, when you are removing directories, you will encounter files that are protected. In such cases, you can use the argument -f which stands for force.\nYou can also combine arguments. For instance, to remove a directory regardless of protected files, you type:\nrm -rf directory-name\nRemember that once you remove there is no going back, so use this command very carefully.\nA command that is often called with argument is ls. Here are some examples:\nls -a \nThe a stands for all. This argument makes ls show you all files in the directory, including hidden files. In Unix, all files starting with a . are hidden. Many applications create hidden directories to store important information without getting in the way of your work. An example is git (which we cover in depth later). Once you initialize a directory as a git directory with git init, a hidden directory called .git is created. Another hidden file is the .gitignore file.\nAnother example of using an argument is:\nls -l \nThe l stands for long and the result is that more information about the files is shown.\nIt is often useful to see files in chronological order. For that we use:\nls -t \nand to reverse the order of how files are shown you can use:\nls -r \nWe can combine all these arguments to show more information for all files in reverse chronological order:\nls -lart \nEach command has a different set of arguments. In the next section, we learn how to find out what they each do.\n\n20.8.2 Getting help\nAs you may have noticed, Unix uses an extreme version of abbreviations. This makes it very efficient, but hard to guess how to call commands. To make up for this weakness, Unix includes complete help files or man pages (man is short for manual). In most systems, you can type man followed by the command name to get help. So for ls, we would type:\nman ls\nThis command is not available in some of the compact implementations of Unix, such as Git Bash. An alternative way to get help that works on Git Bash is to type the command followed by --help. So for ls, it would be as follows:\nls --help\n\n20.8.3 Pipes\nThe help pages are typically long and if you type the commands above to see the help, it scrolls all the way to the end. It would be useful if we could save the help to a file and then use less to see it. The pipe, written like this |, does something similar. It pipes the results of a command to the command after the pipe. This is similar to the pipe |&gt; that we use in R. To get more help we thus can type:\nman ls | less\nor in Git Bash:\nls --help | less \nThis is also useful when listing files with many files. We can type:\nls -lart | less \n\n20.8.4 Wild cards\nSome of the most powerful aspects of Unix are the wild cards. Suppose we want to remove all the temporary html files produced while trouble shooting for a project. Imagine there are dozens of files. It would be quite painful to remove them one by one. In Unix, we can actually write an expression that means all the files that end in .html. To do this we type wild card: *. As discussed in the data wrangling part of this book, this character means any number of any combination of characters. Specifically, to list all html files, we would type:\nls *.html\nTo remove all html files in a directory, we would type:\n\nrm *.html\n\nThe other useful wild card is the ? symbol. This means any single character. So if all the files we want to erase have the form file-001.html with the numbers going from 1 to 999, we can type:\nrm file-???.html\nThis will only remove files with that format.\nWe can combine wild cards. For example, to remove all files with the name file-001 regardless of suffix, we can type:\nrm file-001.* \n\n\n\n\n\n\nCombining rm with the * wild card can be dangerous. There are combinations of these commands that will erase your entire filesystem without asking “are you sure?”. So make sure you understand how it works before using this wild card with the rm command.\n\n\n\n\n20.8.5 Environment variables\nUnix has settings that affect your command line environment. These are called environment variables. The home directory is one of them. We can actually change some of these. In Unix, variables are distinguished from other entities by adding a $ in front. The home directory is stored in $HOME.\nEarlier we saw that echo is the Unix command for print. So we can see our home directory by typing:\necho $HOME \nYou can see them all by typing:\nenv\nYou can change some of these environment variables. But their names vary across different shells. We describe shells in the next section.\n\n20.8.6 Shells\nMuch of what we use in this chapter is part of what is called the Unix shell. There are actually different shells, but the differences are almost unnoticeable. They are also important, although we do not cover those here. You can see what shell you are using by typing:\necho $SHELL\nThe most common one is bash.\nOnce you know the shell, you can change environmental variables. In Bash Shell, we do it using export variable value. To change the path, described in more detail soon, you would type:\nexport PATH = /usr/bin/\n\n\n\n\n\n\nDon’t actually run this command though!\n\n\n\nThere is a program that is run before each terminal starts where you can edit variables so they change whenever you call the terminal. This changes in different implementations, but if using bash, you can create a file called .bashrc, .bash_profile,.bash_login, or .profile. You might already have one.\n\n20.8.7 Executables\nIn Unix, all programs are files. They are called executables. So ls, mv and git are all files. But where are these program files? You can find out using the command which:\n\nwhich git\n#&gt; /usr/bin/git\n\nThat directory is probably full of program files. The directory /usr/bin usually holds many program files. If you type:\nls /usr/bin\nin your terminal, you will see several executable files.\nThere are other directories that usually hold program files. The Application directory in the Mac or Program Files directory in Windows are examples.\nWhen you type ls, Unix knows to run a program which is an executable that is stored in some other directory. So how does Unix know where to find it? This information is included in the environmental variable $PATH. If you type:\necho $PATH\nyou will see a list of directories separated by :. The directory /usr/bin is probably one of the first ones on the list.\nUnix looks for program files in those directories in that order. Although we don’t teach it here, you can actually create executables yourself. However, if you put it in your working directory and this directory is not on the path, you can’t run it just by typing the command. You get around this by typing the full path. So if your command is called my-ls, you can type:\n./my-ls\nOnce you have mastered the basics of Unix, you should consider learning to write your own executables as they can help alleviate repetitive work.\n\n20.8.8 Permissions and file types\nIf you type:\nls -l\nAt the beginning, you will see a series of symbols like this -rw-r--r--. This string indicates the type of file: regular file -, directory d, or executable x. This string also indicates the permission of the file: is it readable? writable? executable? Can other users on the system read the file? Can other users on the system edit the file? Can other users execute if the file is executable? This is more advanced than what we cover here, but you can learn much more in a Unix reference book.\n\n20.8.9 Commands you should learn\nThere are many commands that we do not teach in this book, but we want to make you aware of them and what they do. They are:\n\nopen/start - On the Mac open filename tries to figure out the right application of the filename and open it with that application. This is a very useful command. On Git Bash, you can try start filename. Try opening an R or Rmd file with open or start: it should open them with RStudio.\nnano - A bare-bones text editor.\ntar - archive files and subdirectories of a directory into one file.\nssh - connect to another computer.\nfind - find files by filename on your system.\ngrep - search for patterns in a file.\nawk/sed - These are two very powerful commands that permit you to find specific strings in files and change them.\nln - create a symbolic link. We do not recommend its use, but you should be familiar with it.\n\n20.8.10 File manipulation in R\nWe can also perform file management from within R. The key functions to learn about can be seen by looking at the help file for ?files. Another useful function is unlink.\nAlthough not generally recommended, you can run Unix commands in R using system."
  },
  {
    "objectID": "productivity/unix.html#footnotes",
    "href": "productivity/unix.html#footnotes",
    "title": "20  Organizing with Unix",
    "section": "",
    "text": "https://www.quora.com/Which-are-the-best-Unix-Linux-reference-books↩︎\nhttps://gumroad.com/l/bite-size-linux↩︎\nhttps://jvns.ca/blog/2018/08/05/new-zine–bite-size-command-line/↩︎\nhttps://library.si.edu/sites/default/files/tutorial/pdf/filenamingorganizing20180227.pdf↩︎\nhttps://style.tidyverse.org/↩︎\nhttps://rafalab.github.io/dsbook/accessing-the-terminal-and-installing-git.html↩︎\nhttps://github.com/rairizarry/murders↩︎"
  },
  {
    "objectID": "productivity/git.html#why-use-git-and-github",
    "href": "productivity/git.html#why-use-git-and-github",
    "title": "21  Git and GitHub",
    "section": "21.1 Why use Git and GitHub?",
    "text": "21.1 Why use Git and GitHub?\nThree primary reasons to use Git and GitHub are:\n\nVersion Control: Git allows you to track changes in your code, revert to previous file versions, and work on multiple branches simultaneously. Once changes are finalized, different branches can be merged.\nCollaboration: GitHub offers a central storage solution for projects and lets you add collaborators. These collaborators can make changes, keeping all versions synchronized. Moreover, the pull request feature on GitHub enables others to suggest modifications to your code, which you can then approve or reject.\nSharing: Beyond its powerful version control and collaboration tools, Git and GitHub serve as a platform to easily share your code with others.\n\nWe primarily emphasize the sharing capabilities of Git here. For a deeper dive into its other functionality, please refer to the provided links above. One major advantage of hosting code on GitHub is the ease with which you can showcase it to potential employers seeking samples of your work. Given that numerous companies and organizations employ version control systems like Git for project collaboration, they may find it commendable that you possess some knowledge of the tool."
  },
  {
    "objectID": "productivity/git.html#sec-git-overview",
    "href": "productivity/git.html#sec-git-overview",
    "title": "21  Git and GitHub",
    "section": "21.2 Overview of Git",
    "text": "21.2 Overview of Git\nTo effectively permit version control and collaboration with Git we need to understand the concept of a repository, often simply called a repo. A repo is a digital storage space where you can save, edit, and track versions of files for a specific project. Think of it as a project folder combined with a detailed logbook. It holds all the files and directories related to the project and also records of every change made, who made it, and when. This allows multiple people to collaborate on a project without overwriting contributions from others. You can also easily revert to previous versions if needed.\nNote that Git permits the creation of different branches within a repository. This permits working on files in parallel which is particularly useful for testing ideas that involve big changes before incorporating with a stable version. In this book we provide only examples with just one branch. To learn more about how to define and use multiple branches please consult the resources provided above.\nA common practice involves hosting central main branch on a GitHub repository that all collaborators can access remotely. The main branch is considered the stable official version. Each collaborator also maintains a local repository on their computer, allowing them to edit and test changes before committing them to the main repository.\nWe’re going to explore how Git works by following these step:\n1. First, you’ll learn how to make changes on your computer in what’s called the working directory.\n2. Once you’re happy with your changes, you’ll move them to the staging area. Think of this as preparing or packing your changes.\n3. From there, you’ll save these changes to your local repo, this is like your personal save point on your computer and will generate a new version of the repository in the log.\n4. After saving locally, you’ll then send, or push, these changes to the main storage space where everyone can see them. In our examples, this main storage space is hosted on GitHub, and Git calls it the upstream repo.\n\nNow, to work with this strategy, you’ll need an account on GitHub. In the next two sections, we’ll guide you on how to set up an account and create repos on GitHub."
  },
  {
    "objectID": "productivity/git.html#github-accounts",
    "href": "productivity/git.html#github-accounts",
    "title": "21  Git and GitHub",
    "section": "21.3 GitHub accounts",
    "text": "21.3 GitHub accounts\nBasic GitHub accounts are free. To create one, go to GitHub where you will see a box in which you can sign up.\nYou want to pick a name carefully. It should be short, easy to remember and to spell, somehow related to your name, and professional. This last one is important since you might be sending potential employers a link to your GitHub account. Your initials and last name are usually a good choice."
  },
  {
    "objectID": "productivity/git.html#sec-github-repos",
    "href": "productivity/git.html#sec-github-repos",
    "title": "21  Git and GitHub",
    "section": "21.4 GitHub repositories",
    "text": "21.4 GitHub repositories\nOnce you have an account, you are now ready to create a GitHub repository that will serve as the main or upstream repo for a project. Collaborators you add to this project, will be able to manage a local repository on their computer and push changes. Git will help you keep all the different copies synced.\nTo create a repo, first log in to your account by clicking the Sign In button on https://github.com. You might already be signed in, in which case the Sign In button will not show up. If signing in, you will have to enter your username and password. We recommend you set up your browser to remember this to avoid typing it in each time.\nOnce on your account, you can click on Repositories and then click on New to create a new repo. You will be prompted for a name:\n\n\n\n\n\n\n\n\n\n\nWhen naming your project, pick a descriptive name that clearly tells what the project is about. Keep in mind that as you work on more projects, you’ll accumulate many repositories. As an illustration, we will use the name homework-0.\nYou will also be prompted to decide whether your repo should be public or private. To decide, know that this is the difference:\n\nPublic repositories: Anyone on the internet can see these. Although only collaborators can make changes.\nPrivate repositories: Only people you grant access to can view them.\n\nWhile there are other settings to consider, we typically stick with the default options provided by GitHub.\nAfter creating your repo, GitHub will show you steps to link your local repo (the one on your computer) to the new one you’ve set up on GitHub. They’ll provide some code that you can directly copy and paste into your terminal. We will break down that code so you’ll know exactly what each command does."
  },
  {
    "objectID": "productivity/git.html#sec-connecting-git-github",
    "href": "productivity/git.html#sec-connecting-git-github",
    "title": "21  Git and GitHub",
    "section": "21.5 Connecting Git and GitHub",
    "text": "21.5 Connecting Git and GitHub\nWhen accessing GitHub you need credentials to verify your identity. There are two ways to connect: HTTPS or SSH, each requiring different credentials. We recommend using HTTPS, which uses a Personal Access Token (PAT). Note that your GitHub website password isn’t your access token.\nGitHub provides a detailed guide on obtaining an access token. You can find the guide here. To generate a token:\n\nCarefully follow the instructions in the provided link.\nWhen setting permissions for the token, choose non-expiring and select the repo option in the scopes section.\n\nOnce you complete these steps, GitHub will display your token—a lengthy string of characters. You should then:\n\nImmediately copy this token to your clipboard. Remember, this is the only time GitHub will show it to you.\nFor security, save this token in a password manager. This ensures you can access it if needed later on.\n\nIn some of the procedures outlined below, you’ll be prompted to enter your password. Instead, paste the token you’ve copied. After this, password prompts should no longer appear. If you ever need the token again, retrieve it from your password manager.\nFor a much more detailed explanation, including how to use SSH instead of HTTPS, please consult Happy Git and GitHub for the useR.\nThe next step is to let Git know who we are. This will make it easier to connect with GitHub. To to this type the following two commands in our terminal window:\ngit config --global user.name \"Your Name\"\ngit config --global user.mail \"your@email.com\"\nThis will change the Git configuration in way that anytime you use Git, it will know this information. Note that you need to use the email account that you used to open your GitHub account."
  },
  {
    "objectID": "productivity/git.html#sec-init",
    "href": "productivity/git.html#sec-init",
    "title": "21  Git and GitHub",
    "section": "21.6 Initial setup",
    "text": "21.6 Initial setup\nIn a terminal move to the directory you want to store the local repository. We recommend naming the directory the same as the GitHub repo. In our example we would use:\nmkdir homework-0\ncd homework-0\nWe then initialize the directory as a Git repository, starting the version control process.\ngit init\n\n\n\n\n\n\nmain verus master\n\n\n\nGitHub now uses main as the default branch name. In the past, both Git and GitHub used master as the default. As a result, many older repositories or older versions of Git might still use master as their primary branch.\nTo ensure your local branch aligns with the GitHub repository’s branch name:\n\nVisit the GitHub repository page.\nCheck the dropdown menu on the left that lists branches. This will display the default branch name.\n\nTo verify your local branch name, use:\ngit branch\nIf you see a branch name other than main but want it to be main, rename it with:\ngit branch -M main\nThe -M stands for move. Note that this is different from changing branches, it is renaming the current branch.\n\n\nTo link your local repository to its counterpart on GitHub, you’ll need the GitHub repository’s URL. To find this, go to the repository’s webpage at https://github.com/rairizarry/homework-0. Click the green Code button to quickly copy the URL, which in our example is https://github.com/rairizarry/homework-0.git.\n\nOnce you have this you can type\ngit remote add origin https://github.com/rairizarry/homework-0.git\nTo understand this command note that git remote add adds a new remote reference. A remote in Git refers to another place where your code repository is stored, usually on the internet or another network. origin is the conventional name given to the remote repository or the central repository that other people will treat as an main project source. It’s essentially a shorthand alias for the repository’s URL. You could technically name it anything you want, but origin is the convention most use. Finally, https://github.com/rairizarry/homework-0.git is the URL of the remote repository. It tells Git where the repository is hosted. Together, these commands set up a new local Git repository and link it to a remote repository on GitHub."
  },
  {
    "objectID": "productivity/git.html#git-basics",
    "href": "productivity/git.html#git-basics",
    "title": "21  Git and GitHub",
    "section": "21.7 Git basics",
    "text": "21.7 Git basics\nNow that you have initialized a directory to store your local repository, we can learn how to move files from our working directory all the way to the upstream repo.\n\n21.7.1 The working directory\n\nThe working directory is the same as your Unix working directory. In our example, if we create a file in the homework-0 directory, it is considered to be in the working directory. Git can tell you how the files in the working directory relate to the versions of the files in other areas with the command\ngit status\nBecause we have not done anything yet, you should receive a message such as\nOn branch main\n\nNo commits yet\n\nnothing to commit (create/copy files and use \"git add\" to track)\nIf we add a file, say code.R, you will see a message like:\nUntracked files:\n  (use \"git add &lt;file&gt;...\" to include in what will be committed)\n    code.R\n\n\n21.7.2 add\n\nNow we are going to make changes to these files. Eventually, we want these new versions of the files to be tracked and synced with the upstream repo. But we don’t want to keep track of every little change: we don’t want to sync until we are sure these versions are final enough to share as a new version. For this reason, edits in the staging area are not kept by the version control system.\nTo demonstrate, we add code.R to the staging area:\ngit add code.R\nRunning git status now shows\nChanges to be committed:\n  (use \"git rm --cached &lt;file&gt;...\" to unstage)\n    new file:   code.R\nNote that it is not a problem to have other files in the working directory that are not in the staging area. For example, if we create files test-1.R and test-2.R, git status reminds us these are not staged:\nOn branch main\n\nNo commits yet\n\nChanges to be committed:\n  (use \"git rm --cached &lt;file&gt;...\" to unstage)\n    new file:   code.R\n\nUntracked files:\n  (use \"git add &lt;file&gt;...\" to include in what will be committed)\n    test-1.R\n    test-2.R\n\n\n21.7.3 commit\n\nIf we are now ready to make a first version of our repository, which only includes code.R, we can use the following command:\ngit commit -m \"Adding a new file.\" \nNote that commit requires us to add a messge. Making these informative will help us remember why this change was made. After running commit we will receive a message letting us know it was committed:\n[main (root-commit) 1735c25] adding a new file\n 1 file changed, 0 insertions(+), 0 deletions(-)\n create mode 100644 code.R\nNote that if we edit code.R, it changes only in the working directory. git status shows us\nChanges not staged for commit:\n (use \"git add &lt;file&gt;...\" to update what will be committed)\n  (use \"git restore &lt;file&gt;...\" to discard changes in working directory)\n    modified:   code.R\nTo add the edited file to the local repo, we need to stage the edited file and commit the changes\ngit add code.R\ngit commit -m \"Added some lines of code.\"\nwhich gives us a message letting us know a change was made:\n[main 8843673] added some lines of code\n 1 file changed, 1 insertion(+)\nNote that we can achieve the same results with just one line by following the commit command with the files we want committed\ngit commit -m \"Added some lines of code.\" code.R\nThis is convenient when the number of files that change is small and we can list them at the end.\nTo see version control in action note what happens when we type\ngit log code.R\nwe get a list of the version that have been stored in our log:\ncommit 88436739dcbd57d8ad27a23663d30fd2c06034ca (HEAD -&gt; main)\nAuthor: Rafael A Irizarry \nDate:   Sun Sep 3 15:32:03 2023 -0400\n\n    Added some lines of code.\n\ncommit 1735c25c675d23790df1f9cdb3a215a13c8ae5d6\nAuthor: Rafael A Irizarry \nDate:   Sun Sep 3 15:27:19 2023 -0400\n\n    Adding a new file.\n\n\n21.7.4 push\n\nOnce we are ready to sync our local repo with the upstream GitHub repo, we can use\ngit push -u origin main\n\n\n\n\n\n\nIf this is your first time pushing to your GitHub account, you will be asked for a password and you have to enter the personal access token we described in Section 21.5. You should only need to do this once.\n\n\n\nThe -u flag, short for --set-upstream will make Git remember that in this repository you want to push to the main branch in the remote repo origin defined in the initialization. This is beneficial because the next time you want to push or pull from this branch, you can simply use\ngit push\nIf you need a reminder of where you are pushing to you can type\ngit remove -v\nThe v stands for verbose. In our example we will get\norigin  https://github.com/username/homework-0.git (fetch)\norigin  https://github.com/username/homework-0.git (push)\nWe describe fetch next. If you don’t get anything back it means you have not defined your remote as we did in Section Section 21.6.\n\n\n21.7.5 fetch and merge\n\n\nIf this is a collaborative project, the upstream repo may change and become different than your version. To update your local repository to be like the upstream repo, we use the command fetch:\ngit fetch\nAnd then to make these copies to our working directory, we use the command:\ngit merge\n\n\n21.7.6 pull\n We very often just want to fetch and merge without checking. For this, we use:\ngit pull\n\n\n21.7.7 clone\nYou can easily download all the code and version control logs from an existing public repository using git clone. When you clone you are essentially making a complete copy of the entire directory. For example, you can download all the code used to create this book using:\ngit clone https://github.com/rafalab/dsbook-part-1.git\nYou can see a simple example with the murders directory created in the Unix chapter by cloning this repository:\ngit clone https://github.com/rairizarry/murders.git\nIf you use git clone, you do not need to initialize as the branch and remote will already be defined. Now, to push changes you need to be added as a collaborator. Otherwise you will have to follow the more complex process of a pull request, which we don’t cover here."
  },
  {
    "objectID": "productivity/git.html#gitignore",
    "href": "productivity/git.html#gitignore",
    "title": "21  Git and GitHub",
    "section": "21.8 .gitignore",
    "text": "21.8 .gitignore\nWhen we use git status we obtain information about all files in our local repo. But we don’t necessarily need to add all the files in our local repo to the Git repo, only the ones we want to keep track of or the ones we want to share. If our work is producing files of a certain type that we do not want to keep track of, we can add the suffix that defines these files to the .gitignore file. More details on using .gitignore are included here: https://git-scm.com/docs/gitignore. These files will stop appearing in your RStudio Git pane."
  },
  {
    "objectID": "productivity/git.html#sec-rstudio-git",
    "href": "productivity/git.html#sec-rstudio-git",
    "title": "21  Git and GitHub",
    "section": "21.9 Git in RStudio",
    "text": "21.9 Git in RStudio\nWhile command line Git is a powerful and flexible tool, it can be somewhat daunting when we are getting started. RStudio provides a graphical interface that facilitates the use of Git in the context of a data analysis project.\nTo do this, we start a project but, instead of New Directory, or Existing Directory, we select Version Control and then we will select Git as our version control system:\n\n\n\n\n\n\n\n\n\n\nThe repository URL is the link you used as origin or to clone. In Section Section 21.4, we used https://github.com/username/homework-0.git as an example. In the project directory name, you need to put the name of the folder that was generated, which in our example will be the name of the repo homework-0. This will create a folder called homework-0 on your local system. Note you will need to remove the folder if it already exists or chose a different name. Once you do this, the project is created and it is aware of the connection to a GitHub repo. You will see on the top right corner the name and type of project as well as a new tab on the upper right pane titled Git.\n\n\n\n\n\n\n\n\n\n\nIf you select this tab, it will show you the files on your project with some icons that give you information about these files and their relationship to the repo. In the example below, we already added a file to the folder, called code.R which you can see in the editing pane.\n\nWe now need to pay attention to the Git pane. It is important to know that your local files and the GitHub repo will not be synced automatically. As described in Section Section 21.2, you have to sync using git push when you are ready. We show how you can do this through RStudio rather than the terminal below.\nBefore we start working on a collaborative project, usually the first thing we do is pull in the changes from the remote repo, in our case the one on GitHub. However, for the example shown here, since we are starting with an empty repo and we are the only ones making changes, we don’t need to start by pulling.\nIn RStudio, the status of the file as it relates to the remote and local repos are represented in the status symbols with colors. A yellow square means that Git knows nothing about this file. To sync with the GitHub repo, we need to add the file, then commit the change to our local Git repo, then push the change to the GitHub repo. Right now, the file is just on our computer. To add the file using RStudio, we click the Stage box. You will see that the status icon now changes to a green A.\n\nNow we are ready to commit the file to our local repo. In RStudio, we can use the Commit button. This will open a new dialog window. With Git, whenever we commit a change, we are required to enter a comment describing the changes being committed.\n\nIn this case, we will simply describe that we are adding a new script. In this dialog box, RStudio also gives you a summary of what you are changing to the GitHub repo. In this case, because it is a new file, the entire file is highlighted as green, which highlights the changes.\nOnce we hit the commit button, we should see a message from Git with a summary of the changes that were committed. Now we are ready to push these changes to the GitHub repo. We can do this by clicking on the Push button on the top right corner:\n\n\n\n\n\n\n\n\n\n\nWe now see a message from Git letting us know that the push has succeeded. In the pop-up window we no longer see the code.R file. This is because no new changes have been performed since we last pushed. We can exit this pop-up window now and continue working on our code.\n\n\n\n\n\n\n\n\n\n\nIf we now visit our repo on the web, we will see that it matches our local copy.\n\nCongratulations, you have successfully shared code on a GitHub repository!\n\n\n\n\n\n\nFor the example shown here, we only be added code.R. But, in general, for an RStudio project, we recommend adding a README.md file and both the .gitignore and .Rproj files."
  },
  {
    "objectID": "productivity/reproducible-projects.html#rstudio-projects",
    "href": "productivity/reproducible-projects.html#rstudio-projects",
    "title": "22  Reproducible projects",
    "section": "\n22.1 RStudio projects",
    "text": "22.1 RStudio projects\nRStudio provides a way to keep all the components of a data analysis project organized into one folder and to keep track of information about this project, such as the Git status of files, in one file. In Section Section 21.9 we demonstrate how RStudio facilitates the use of Git and GitHub through RStudio projects. In this section we quickly demonstrate how to start a new a project and some recommendations on how to keep these organized. RStudio projects also permit you to have several RStudio sessions open and keep track of which is which.\nTo start a project, click on File and then New Project. Often we have already created a folder to save the work, as we did in Section Section 20.7 and we select Existing Directory. Here we show an example in which we have not yet created a folder and select the New Directory option.\n\n\n\n\n\n\n\n\n\n\nThen, for a data analysis project, you usually select the New Project option:\n\nNow you will have to decide on the location of the folder that will be associated with your project, as well as the name of the folder. When choosing a folder name, just like with file names, make sure it is a meaningful name that will help you remember what the project is about. As with files, we recommend using lower case letters, no spaces, and hyphens to separate words. We will call the folder for this project my-first-project. This will then generate a Rproj file called my-first-project.Rproj in the folder associated with the project. We will see how this is useful a few lines below.\n\nYou will be given options on where this folder should be on your filesystem. In this example, we will place it in our home folder, but this is generally not good practice. As we described in Section 20.7 in the Unix chapter, you want to organize your filesystem following a hierarchical approach and with a folder called projects where you keep a folder for each project.\n\nWhen you start using RStudio with a project, you will see the project name in the upper right corner. This will remind you what project this particular RStudio session belongs to. When you open an RStudio session with no project, it will say Project: (None).\nWhen working on a project, all files will be saved and searched for in the folder associated with the project. Below, we show an example of a script that we wrote and saved with the name code.R. Because we used a meaningful name for the project, we can be a bit less informative when we name the files. Although we do not do it here, you can have several scripts open at once. You simply need to click File, then New File and pick the type of file you want to edit.\n\nOne of the main advantages of using Projects is that after closing RStudio, if we wish to continue where we left off on the project, we simply double click or open the file saved when we first created the RStudio project. In this case, the file is called my-first-project.Rproj. If we open this file, RStudio will start up and open the scripts we were editing.\n\n\n\n\n\n\n\n\n\n\nAnother advantage is that if you click on two or more different Rproj files, you start new RStudio and R sessions for each."
  },
  {
    "objectID": "productivity/reproducible-projects.html#markdown",
    "href": "productivity/reproducible-projects.html#markdown",
    "title": "22  Reproducible projects",
    "section": "\n22.2 Markdown",
    "text": "22.2 Markdown\nMarkdown is a format for literate programming documents that is widely used to generate html pages or pdf documents. Literate programming weaves instructions, documentation, and detailed comments in between machine executable code, producing a document that describes the program that is best for human understanding (Knuth 1984). You can learn more about markdown here: https://www.markdowntutorial.com/.\nUnlike a word processor, such as Microsoft Word, where what you see is what you get, with markdown, you need to compile the document into the final report. The markdown document looks different than the final product. This approach seems like a disadvantage at first, but it can save you time in the long run. For example, instead of producing plots and inserting them one by one into the word processing document, the plots are automatically added when the document is compiled. If you need to change the plots, you just recompile the document after editing the code that produces the plot.\nIn R, we can produce literate programming documents using Quarto or R Markdown. We recommend using Quarto because it is a newer and more flexible version of R markdown that permits the use of languages other than R. Because R markdown preceded Quarto by several years, many public and educational literate programming documents are written in R markdown. However, because the format is similar and both use the knitr package, described in Section Section 22.2.4, to execute the R code, most existing R markdown files can be rendered with Quarto, without modification.\nIn RStudio, you can start either a Quarto or R markdown document by clicking on File, New File, then Quarto Document or R Markdown, respectively. You will then be asked to enter a title and author for your document. We are going to prepare a report on gun murders so we will give it an appropriate name. You can also decide what format you would like the final report to be in: HTML, PDF, or Microsoft Word. Later, we can easily change this, but here we select html as it is the preferred format for debugging purposes:\n\n\n\n\n\n\n\n\n\n\nThis will generate a template file:\n\nAs a convention, we use qmd and Rmd suffixes for Quarto and R markdown files, respectively.\nOnce you gain experience with markdown, you will be able to do this without the template and can simply start from a blank template.\nIn the template, you will see several things to note.\n\n22.2.1 The header\nAt the top you see:\n---\ntitle: \"Report on Gun Murders\"\nauthor: \"Rafael Irizarry\"\nformat: html\n---\nThe things between the --- is the YAML header. YAML is a widely used language mainly used for providing configuration data. With Quarto and R markdown it is mainly used to define options for the document. You can define many other things in the header than what is included in the template. We don’t discuss those here, but much information is available from the quarto guide. The one parameter that we will highlight is format. By changing this to, say, pdf, we can control the type of output that is produced when we compile. The title and author parameters are automatically filled because we filled in the blanks in the RStudio diaglog box that pops up when starting a new document.\n\n22.2.2 R code chunks\nIn various places in the document, we see something like this:\n```{r}\n1 + 1\n```\nThese are the code chunks. When you compile the document, the R code inside the chunk, in this case 1+1, will be evaluated and the result included in that position in the final document.\nTo add your own R chunks, you can type the characters above quickly with the key binding command-option-I on the Mac and Ctrl-Alt-I on Windows.\nThis applies to plots as well; the plot will be placed in that position. We can write something like this:\n```{r}\nplot(1)\n```\nBy default, the code will show up as well. To avoid having the code show up, you can use an argument, which are annotated with #| To avoid showing code in the final document, you can use the argument echo: FALSE. For example:\n```{r}\n#| echo: false\n\n1+1\n```\nWe recommend getting into the habit of adding a label to the R code chunks. This will be very useful when debugging, among other situations. You do this by adding a descriptive word like this:\n```{r}\n#| label: one-plus-one\n\n1+1\n```\n\n22.2.3 Global execution options\nIf you want to apply an option globally, you can include in the header, under execute. For example adding the following line to the header make code not shou up, by default:\nexecute:\n  echo: false\nWe will not cover more details here, but as you become more experienced with R Markdown, you will learn the advantages of setting global options for the compilation process.\n\n22.2.4 knitR\nWe use the knitR package to compile Quarto or R markdown documents. The specific function used to compile is the knit function, which takes a filename as input. RStudio provides a button that makes it easier to compile the document. For the screenshot below, we have edited the document so that a report on gun murders is produced. You can see the file here: https://raw.githubusercontent.com/rairizarry/murders/master/report.qmd. You can now click on the Render button:\n\nNote that the first time you click on the Render button, a dialog box may appear asking you to install packages you need. Once you have installed the packages, clicking Render will compile your Quarto file and the resulting document will pop up.\nThis particular example produces an html document which you can see in your working directory. To view it, open a terminal and list the files. You can open the file in a browser and use this to present your analysis. You can also produce a PDF or Microsoft document by changing: format: html to format: pdf or format: docx. Note this is one difference between Quarto and R markdown. With R markdown we use output: html_document, output: pdf_document, or output: word_document.\nWe can also produce documents that render on GitHub using format: gfm, which stands for GitHub flavored markdown. This will produce a markdown file, with suffix md, that renders nicely on GitHub. Because we have uploaded these files to GitHub, you can click on the md file and you will see the report as a webpage:\n\nThis is a convenient way to share your reports.\n\n22.2.5 Learning more\nThere is a lot more you can do with R markdown. We highly recommend you continue learning as you gain more experience writing reports in R. There are many free resources on the internet including:\n\nThe Quarto Guide: https://quarto.org/docs/guide/\n\nRStudio’s tutorial: https://quarto.org/docs/get-started/hello/rstudio.html\n\nThe knitR book: https://yihui.name/knitr/"
  },
  {
    "objectID": "productivity/reproducible-projects.html#sec-organizing",
    "href": "productivity/reproducible-projects.html#sec-organizing",
    "title": "22  Reproducible projects",
    "section": "\n22.3 Organizing a data science project",
    "text": "22.3 Organizing a data science project\nIn this section we put it all together to create the US murders project and share it on GitHub.\n\n22.3.1 Create directories in Unix\nIn Section Section 20.7 we demonstrated how to use Unix to prepare for a data science project using an example. Here we continue this example and show how to use RStudio. In Section Section 20.7 we created the following directories using Unix:\n\ncd ~\ncd projects\nmkdir murders\ncd murders\nmkdir data rdas \n\n\n22.3.2 Create an RStudio project\nIn the next section we will use create an RStudio project. In RStudio we go to File and then New Project… and when given the options we pick Existing Directory. We then write the full path of the murders directory created above.\n\n\n\n\n\n\n\n\n\n\nOnce you do this, you will see the rdas and data directories you created in the RStudio Files tab.\n\nKeep in mind that when we are in this project, our default working directory will be ~/projects/murders. You can confirm this by typing getwd() into your R session. This is important because it will help us organize the code when we need to write file paths.\n\n\n\n\n\n\nTry to always use relative paths in code for data analysis projects. These should be relative to the default working directory. The problem with using full paths, including using the home directory ~ as part of your path, is that your code is unlikely to work on file systems other than yours since the directory structures will be different.\n\n\n\n\n22.3.3 Edit some R scripts\nLet’s now write a script that downloads a file into the data directory. We will call this file download-data.R.\nThe content of this file will be:\n\nurl &lt;- \"https://raw.githubusercontent.com/rafalab/dslabs/master/inst/\nextdata/murders.csv\"\ndest_file &lt;- \"data/murders.csv\"\ndownload.file(url, destfile = dest_file)\n\nNotice that we are using the relative path data/murders.csv.\nRun this code in R and you will see that a file is added to the data directory.\nNow we are ready to write a script to read this data and prepare a table that we can use for analysis. Call the file wrangle-data.R. The content of this file will be:\n\nlibrary(tidyverse)\nmurders &lt;- read_csv(\"data/murders.csv\")\nmurders &lt;-murders |&gt; mutate(region = factor(region),\n                             rate = total / population * 10^5)\nsave(murders, file = \"rdas/murders.rda\")\n\nAgain note that we use relative paths exclusively.\n\n22.3.4 Saving processed data\nIn this file, we introduce an R command we have not seen: save. The save command in R saves objects into what is called an rda file: rda is short for R data. We recommend using the .rda suffix on files saving R objects. You will see that .RData is also used.\nIf you run this code above, the processed data object will be saved in a file in the rda directory. You can then restore the object using load. Although not the case here, this approach is often practical because generating the data object we use for final analyses and plots can be a complex and time-consuming process. So we run this process once and save the file. But we still want to be able to generate the entire analysis from the raw data.\nWhile save let’s you save several objects that then get loaded with the same names used when saving, the function saveRDS lets you save one object, without the name. To bring it back you use the readRDS function. An example of how this is used is you save it in one session:\n\nsaveRDS(murders, file = \"rdas/murders.rda\")\n\nThen read it in in another session, using whatever object name you want:\n\ndat &lt;- readRDS(\"rdas/murders.rda\")\n\n\n22.3.5 The main analysis file\nNow we are ready to write the analysis file. Let’s call it analysis.R. The content should be the following:\n\nlibrary(tidyverse)\nload(\"rdas/murders.rda\")\n\nmurders |&gt; mutate(abb = reorder(abb, rate)) |&gt;\n  ggplot(aes(abb, rate)) +\n  geom_bar(width = 0.5, stat = \"identity\", color = \"black\") +\n  coord_flip()\n\nIf you run this analysis, you will see that it generates a plot.\n\n22.3.6 Other directories\nNow suppose we want to save the generated plot for use in a report or presentation. We can do this with the ggplot command ggsave. But where do we put the graph? We should be systematically organized so we will save plots to a directory called figs. Start by creating a directory by typing the following in the terminal:\n\nmkdir figs\n\nand then you can add the line:\n\nggsave(\"figs/barplot.png\")\n\nto your R script. If you run the script now, a png file will be saved into the figs directory. If we wanted to copy that file to some other directory where we are developing a presentation, we can avoid using the mouse by using the cp command in our terminal.\n\n22.3.7 The README file\nYou now have a self-contained analysis in one directory. One final recommendation is to create a README.txt file describing what each of these files does for the benefit of others reading your code, including your future self. This would not be a script but just some notes. One of the options provided when opening a new file in RStudio is a text file. You can save something like this into the text file:\nWe analyze US gun murder data collected by the FBI.\n\ndownload-data.R - Downloads csv file to data directory\n\nwrangle-data.R - Creates a derived dataset and saves as R object in rdas\ndirectory\n\nanalysis.R - A plot is generated and saved in the figs directory.\n\n22.3.8 Initializing a Git directory\nIn Section Section 21.6 we demonstrated how to initialize a Git directory and connect it to the upstream repository on GitHub, which we already created in that section.\nWe can do this in the Unix terminal:\n\ncd ~/projects/murders\ngit init\ngit add README.txt\ngit commit -m \"First commit. Adding README.txt file just to get started\"\ngit remote add origin `https://github.com/rairizarry/murders.git`\ngit push -u origin remote\n\n\n22.3.9 Add, commit, and push files using RStudio\nWe can continue adding and committing each file, but it might be easier to use RStudio. To do this, start the project by opening the Rproj file. The git icons should appear and you can add, commit and push using these.\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can now go to GitHub and confirm that our files are there. You can see a version of this project, organized with Unix directories, on GitHub1. You can download a copy to your computer by using the git clone command on your terminal. This command will create a directory called murders in your working directory, so be careful where you call it from.\n\ngit clone https://github.com/rairizarry/murders.git"
  },
  {
    "objectID": "productivity/reproducible-projects.html#footnotes",
    "href": "productivity/reproducible-projects.html#footnotes",
    "title": "22  Reproducible projects",
    "section": "",
    "text": "https://github.com/rairizarry/murders↩︎"
  }
]